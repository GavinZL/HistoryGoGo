<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Scrapy çˆ¬è™«æ¡†æ¶æŠ€æœ¯æŒ‡å— - HistoryGogoé¡¹ç›®</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.8;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 12px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
            overflow: hidden;
        }

        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 60px 40px;
            text-align: center;
        }

        header h1 {
            font-size: 3em;
            margin-bottom: 10px;
            font-weight: 700;
        }

        header p {
            font-size: 1.3em;
            opacity: 0.95;
        }

        nav {
            background: #f8f9fa;
            padding: 20px 40px;
            border-bottom: 3px solid #667eea;
            position: sticky;
            top: 0;
            z-index: 100;
        }

        nav ul {
            list-style: none;
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
        }

        nav a {
            color: #667eea;
            text-decoration: none;
            font-weight: 600;
            transition: color 0.3s;
            padding: 5px 10px;
            border-radius: 4px;
        }

        nav a:hover {
            background: #667eea;
            color: white;
        }

        main {
            padding: 40px;
        }

        section {
            margin-bottom: 60px;
        }

        h2 {
            font-size: 2.2em;
            color: #667eea;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 3px solid #667eea;
        }

        h3 {
            font-size: 1.6em;
            color: #764ba2;
            margin: 30px 0 15px;
        }

        h4 {
            font-size: 1.3em;
            color: #555;
            margin: 20px 0 10px;
        }

        p {
            margin-bottom: 15px;
            text-align: justify;
        }

        ul, ol {
            margin: 15px 0 15px 30px;
        }

        li {
            margin-bottom: 10px;
        }

        code {
            background: #f4f4f4;
            padding: 2px 8px;
            border-radius: 4px;
            font-family: "Monaco", "Menlo", monospace;
            font-size: 0.9em;
            color: #d63384;
        }

        pre {
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 20px 0;
            font-family: "Monaco", "Menlo", monospace;
            font-size: 0.9em;
            line-height: 1.6;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }

        pre code {
            background: transparent;
            color: inherit;
            padding: 0;
        }

        .info-box {
            background: #e7f3ff;
            border-left: 5px solid #2196F3;
            padding: 20px;
            margin: 20px 0;
            border-radius: 4px;
        }

        .warning-box {
            background: #fff3cd;
            border-left: 5px solid #ffc107;
            padding: 20px;
            margin: 20px 0;
            border-radius: 4px;
        }

        .success-box {
            background: #d4edda;
            border-left: 5px solid #28a745;
            padding: 20px;
            margin: 20px 0;
            border-radius: 4px;
        }

        .danger-box {
            background: #f8d7da;
            border-left: 5px solid #dc3545;
            padding: 20px;
            margin: 20px 0;
            border-radius: 4px;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }

        th, td {
            padding: 12px;
            text-align: left;
            border: 1px solid #ddd;
        }

        th {
            background: #667eea;
            color: white;
            font-weight: 600;
        }

        tr:nth-child(even) {
            background: #f8f9fa;
        }

        .diagram {
            background: #f8f9fa;
            border: 2px solid #667eea;
            border-radius: 8px;
            padding: 30px;
            margin: 30px 0;
            text-align: center;
        }

        .flow-chart {
            display: flex;
            justify-content: space-around;
            align-items: center;
            flex-wrap: wrap;
            gap: 20px;
        }

        .flow-item {
            background: white;
            border: 2px solid #667eea;
            border-radius: 8px;
            padding: 15px 25px;
            font-weight: 600;
            color: #667eea;
            min-width: 120px;
            text-align: center;
        }

        .flow-arrow::after {
            content: "â†’";
            margin: 0 10px;
            font-size: 1.5em;
            color: #764ba2;
        }

        footer {
            background: #2d2d2d;
            color: white;
            padding: 30px 40px;
            text-align: center;
        }

        .badge {
            display: inline-block;
            background: #667eea;
            color: white;
            padding: 5px 15px;
            border-radius: 20px;
            font-size: 0.85em;
            margin: 5px;
            font-weight: 600;
        }

        .highlight {
            background: linear-gradient(120deg, #f6d365 0%, #fda085 100%);
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
        }

        @media (max-width: 768px) {
            header h1 {
                font-size: 2em;
            }

            main {
                padding: 20px;
            }

            nav ul {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>ğŸ•·ï¸ Scrapy çˆ¬è™«æ¡†æ¶æŠ€æœ¯æŒ‡å—</h1>
            <p>HistoryGogoé¡¹ç›®å®æˆ˜è§£æ - ä»å…¥é—¨åˆ°ç²¾é€š</p>
        </header>

        <nav>
            <ul>
                <li><a href="#intro">æ¡†æ¶ç®€ä»‹</a></li>
                <li><a href="#architecture">æ¶æ„è®¾è®¡</a></li>
                <li><a href="#config">é…ç½®è¯¦è§£</a></li>
                <li><a href="#workflow">è¿è¡Œæµç¨‹</a></li>
                <li><a href="#spider">Spiderå¼€å‘</a></li>
                <li><a href="#pipeline">Pipelineç®¡é“</a></li>
                <li><a href="#middleware">ä¸­é—´ä»¶</a></li>
                <li><a href="#bestpractices">æœ€ä½³å®è·µ</a></li>
            </ul>
        </nav>

        <main>
            <!-- æ¡†æ¶ç®€ä»‹ -->
            <section id="intro">
                <h2>ğŸ“– ä¸€ã€Scrapy æ¡†æ¶ç®€ä»‹</h2>
                
                <h3>1.1 ä»€ä¹ˆæ˜¯ Scrapyï¼Ÿ</h3>
                <p>
                    <strong>Scrapy</strong> æ˜¯ä¸€ä¸ªç”¨ Python ç¼–å†™çš„å¼€æºç½‘ç»œçˆ¬è™«æ¡†æ¶ï¼Œä¸“ä¸ºé«˜æ•ˆã€å¯æ‰©å±•çš„æ•°æ®æŠ“å–è€Œè®¾è®¡ã€‚
                    å®ƒæä¾›äº†å®Œæ•´çš„çˆ¬è™«å¼€å‘å·¥å…·é“¾ï¼Œä»è¯·æ±‚è°ƒåº¦ã€æ•°æ®è§£æåˆ°æŒä¹…åŒ–å­˜å‚¨ï¼Œä¸€åº”ä¿±å…¨ã€‚
                </p>

                <div class="info-box">
                    <h4>ğŸ¯ æ ¸å¿ƒç‰¹æ€§</h4>
                    <ul>
                        <li><strong>é«˜æ€§èƒ½å¼‚æ­¥å¼•æ“</strong>ï¼šåŸºäº Twisted å¼‚æ­¥ç½‘ç»œåº“ï¼Œæ”¯æŒé«˜å¹¶å‘è¯·æ±‚</li>
                        <li><strong>çµæ´»çš„æ¶æ„</strong>ï¼šç»„ä»¶åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•å’Œå®šåˆ¶</li>
                        <li><strong>å¼ºå¤§çš„é€‰æ‹©å™¨</strong>ï¼šæ”¯æŒ XPath å’Œ CSS é€‰æ‹©å™¨è¿›è¡Œæ•°æ®æå–</li>
                        <li><strong>å†…ç½®ä¸­é—´ä»¶</strong>ï¼šè¯·æ±‚/å“åº”å¤„ç†ã€Cookieç®¡ç†ã€é‡è¯•æœºåˆ¶ç­‰</li>
                        <li><strong>æ•°æ®ç®¡é“</strong>ï¼šPipeline æœºåˆ¶ç”¨äºæ•°æ®æ¸…æ´—å’ŒæŒä¹…åŒ–</li>
                        <li><strong>è‡ªåŠ¨åŒ–è°ƒåº¦</strong>ï¼šæ™ºèƒ½è¯·æ±‚è°ƒåº¦å’Œå»é‡æœºåˆ¶</li>
                    </ul>
                </div>

                <h3>1.2 ä¸ºä»€ä¹ˆé€‰æ‹© Scrapyï¼Ÿ</h3>
                <table>
                    <tr>
                        <th>å¯¹æ¯”ç»´åº¦</th>
                        <th>Scrapy</th>
                        <th>Requests + BeautifulSoup</th>
                    </tr>
                    <tr>
                        <td>æ€§èƒ½</td>
                        <td>âœ… å¼‚æ­¥é«˜å¹¶å‘</td>
                        <td>âŒ åŒæ­¥å•çº¿ç¨‹</td>
                    </tr>
                    <tr>
                        <td>æ¶æ„</td>
                        <td>âœ… å®Œæ•´æ¡†æ¶</td>
                        <td>âš ï¸ éœ€è‡ªè¡Œæ­å»º</td>
                    </tr>
                    <tr>
                        <td>æ•°æ®ç®¡é“</td>
                        <td>âœ… å†…ç½® Pipeline</td>
                        <td>âŒ éœ€æ‰‹åŠ¨å®ç°</td>
                    </tr>
                    <tr>
                        <td>ä¸­é—´ä»¶</td>
                        <td>âœ… ä¸°å¯Œçš„ä¸­é—´ä»¶</td>
                        <td>âŒ éœ€è‡ªè¡Œå¼€å‘</td>
                    </tr>
                    <tr>
                        <td>è°ƒåº¦ç®¡ç†</td>
                        <td>âœ… è‡ªåŠ¨åŒ–è°ƒåº¦</td>
                        <td>âŒ æ‰‹åŠ¨ç®¡ç†</td>
                    </tr>
                    <tr>
                        <td>å­¦ä¹ æ›²çº¿</td>
                        <td>âš ï¸ ä¸­ç­‰</td>
                        <td>âœ… ç®€å•</td>
                    </tr>
                </table>

                <h3>1.3 åœ¨ HistoryGogo é¡¹ç›®ä¸­çš„åº”ç”¨</h3>
                <p>
                    HistoryGogo é¡¹ç›®ä½¿ç”¨ Scrapy ä»ç™¾åº¦ç™¾ç§‘å’Œç»´åŸºç™¾ç§‘æŠ“å–æ˜æœå†å²æ•°æ®ï¼ŒåŒ…æ‹¬ï¼š
                </p>
                <ul>
                    <li><span class="badge">çš‡å¸ä¿¡æ¯</span> 16ä½æ˜æœçš‡å¸çš„è¯¦ç»†èµ„æ–™</li>
                    <li><span class="badge">å†å²äº‹ä»¶</span> å†›äº‹ã€æ”¿æ²»ã€æ–‡åŒ–ç­‰é‡å¤§äº‹ä»¶</li>
                    <li><span class="badge">å†å²äººç‰©</span> å¤§è‡£ã€å°†é¢†ã€æ–‡äººç­‰ç›¸å…³äººç‰©</li>
                    <li><span class="badge">ä½œå“æ–‡çŒ®</span> æ–‡å­¦ä½œå“ã€è‰ºæœ¯åˆ›ä½œç­‰</li>
                </ul>
            </section>

            <!-- æ¶æ„è®¾è®¡ -->
            <section id="architecture">
                <h2>ğŸ—ï¸ äºŒã€æ¶æ„è®¾è®¡</h2>

                <h3>2.1 Scrapy æ ¸å¿ƒç»„ä»¶</h3>
                
                <div class="diagram">
                    <h4>Scrapy æ¶æ„å›¾</h4>
                    <div class="flow-chart">
                        <div class="flow-item">Engine<br>(å¼•æ“)</div>
                        <span class="flow-arrow"></span>
                        <div class="flow-item">Scheduler<br>(è°ƒåº¦å™¨)</div>
                        <span class="flow-arrow"></span>
                        <div class="flow-item">Downloader<br>(ä¸‹è½½å™¨)</div>
                        <span class="flow-arrow"></span>
                        <div class="flow-item">Spider<br>(çˆ¬è™«)</div>
                        <span class="flow-arrow"></span>
                        <div class="flow-item">Pipeline<br>(ç®¡é“)</div>
                    </div>
                </div>

                <h4>ç»„ä»¶è¯¦è§£ï¼š</h4>
                
                <table>
                    <tr>
                        <th>ç»„ä»¶</th>
                        <th>èŒè´£</th>
                        <th>åœ¨é¡¹ç›®ä¸­çš„åº”ç”¨</th>
                    </tr>
                    <tr>
                        <td><strong>Engineï¼ˆå¼•æ“ï¼‰</strong></td>
                        <td>æ ¸å¿ƒæ§åˆ¶å™¨ï¼Œåè°ƒå„ç»„ä»¶å·¥ä½œ</td>
                        <td>è‡ªåŠ¨ç®¡ç†æ•´ä¸ªçˆ¬å–æµç¨‹</td>
                    </tr>
                    <tr>
                        <td><strong>Schedulerï¼ˆè°ƒåº¦å™¨ï¼‰</strong></td>
                        <td>æ¥æ”¶è¯·æ±‚å¹¶æ’é˜Ÿï¼Œå»é‡</td>
                        <td>è‡ªåŠ¨å¤„ç†çš‡å¸ã€äº‹ä»¶ã€äººç‰©çš„URLè°ƒåº¦</td>
                    </tr>
                    <tr>
                        <td><strong>Downloaderï¼ˆä¸‹è½½å™¨ï¼‰</strong></td>
                        <td>ä¸‹è½½ç½‘é¡µå†…å®¹</td>
                        <td>ä»ç™¾åº¦ç™¾ç§‘è·å–HTMLé¡µé¢</td>
                    </tr>
                    <tr>
                        <td><strong>Spiderï¼ˆçˆ¬è™«ï¼‰</strong></td>
                        <td>è§£æå“åº”ï¼Œæå–æ•°æ®å’Œæ–°è¯·æ±‚</td>
                        <td>BaiduBaikeSpider è§£æå†å²æ•°æ®</td>
                    </tr>
                    <tr>
                        <td><strong>Pipelineï¼ˆç®¡é“ï¼‰</strong></td>
                        <td>å¤„ç†Spiderè¿”å›çš„æ•°æ®</td>
                        <td>æ•°æ®æ¸…æ´—ã€éªŒè¯ã€å­˜å‚¨åˆ°SQLite/Neo4j</td>
                    </tr>
                    <tr>
                        <td><strong>Middlewareï¼ˆä¸­é—´ä»¶ï¼‰</strong></td>
                        <td>å¤„ç†è¯·æ±‚å’Œå“åº”çš„é’©å­</td>
                        <td>éšæœºUser-Agentã€é‡è¯•ç­–ç•¥ã€æ•°æ®åˆå¹¶</td>
                    </tr>
                </table>

                <h3>2.2 HistoryGogo é¡¹ç›®ç»“æ„</h3>
                
<pre><code>crawler/
â”œâ”€â”€ config/                 # é…ç½®æ¨¡å—
â”‚   â”œâ”€â”€ settings.py         # Scrapyå…¨å±€é…ç½®
â”‚   â””â”€â”€ ming_data.py        # æ˜æœåŸºç¡€æ•°æ®
â”œâ”€â”€ models/                 # æ•°æ®æ¨¡å‹
â”‚   â””â”€â”€ entities.py         # å®ä½“å®šä¹‰ï¼ˆEmperorã€Eventã€Personç­‰ï¼‰
â”œâ”€â”€ spiders/                # çˆ¬è™«æ¨¡å—
â”‚   â”œâ”€â”€ baidu_baike_spider.py    # ç™¾åº¦ç™¾ç§‘çˆ¬è™«
â”‚   â””â”€â”€ wikipedia_spider.py      # ç»´åŸºç™¾ç§‘çˆ¬è™«
â”œâ”€â”€ pipelines/              # æ•°æ®ç®¡é“
â”‚   â”œâ”€â”€ data_cleaning.py    # æ•°æ®æ¸…æ´—
â”‚   â”œâ”€â”€ data_validation.py  # æ•°æ®éªŒè¯
â”‚   â”œâ”€â”€ sqlite_pipeline.py  # SQLiteå­˜å‚¨
â”‚   â””â”€â”€ neo4j_pipeline.py   # Neo4jå­˜å‚¨
â”œâ”€â”€ middlewares.py          # ä¸­é—´ä»¶
â””â”€â”€ utils/                  # å·¥å…·å‡½æ•°
    â””â”€â”€ date_utils.py       # æ—¥æœŸè§£æå·¥å…·
</code></pre>
            </section>

            <!-- é…ç½®è¯¦è§£ -->
            <section id="config">
                <h2>âš™ï¸ ä¸‰ã€é…ç½®è¯¦è§£</h2>

                <h3>3.1 åŸºç¡€é…ç½®ï¼ˆsettings.pyï¼‰</h3>

                <h4>ğŸ”§ é¡¹ç›®è¯†åˆ«é…ç½®</h4>
<pre><code># Botåç§° - ç”¨äºæ—¥å¿—å’Œè¯†åˆ«
BOT_NAME = 'historygogo_crawler'

# Spideræ¨¡å—è·¯å¾„
SPIDER_MODULES = ['crawler.spiders']
NEWSPIDER_MODULE = 'crawler.spiders'
</code></pre>

                <h4>ğŸŒ ç½‘ç»œè¯·æ±‚é…ç½®</h4>
<pre><code># éµå®ˆrobots.txtè§„åˆ™ï¼ˆæ¨èå¼€å¯ï¼‰
ROBOTSTXT_OBEY = True

# å¹¶å‘è¯·æ±‚æ•°ï¼ˆæ ¹æ®ç›®æ ‡ç½‘ç«™è°ƒæ•´ï¼‰
CONCURRENT_REQUESTS = 8

# ä¸‹è½½å»¶è¿Ÿï¼ˆç§’ï¼‰- é¿å…è¢«å°
DOWNLOAD_DELAY = 3

# ä¸‹è½½å»¶è¿ŸéšæœºåŒ–ï¼ˆé˜²æ­¢è¢«è¯†åˆ«ä¸ºçˆ¬è™«ï¼‰
RANDOMIZE_DOWNLOAD_DELAY = True

# è¯·æ±‚è¶…æ—¶æ—¶é—´
DOWNLOAD_TIMEOUT = 30

# é‡è¯•æ¬¡æ•°
RETRY_TIMES = 3
</code></pre>

                <div class="warning-box">
                    <h4>âš ï¸ åçˆ¬è™«æ³¨æ„äº‹é¡¹</h4>
                    <ul>
                        <li><strong>åˆç†è®¾ç½®å»¶è¿Ÿ</strong>ï¼šDOWNLOAD_DELAY è‡³å°‘ 2-3 ç§’ï¼Œé¿å…ç»™æœåŠ¡å™¨é€ æˆå‹åŠ›</li>
                        <li><strong>å°Šé‡ robots.txt</strong>ï¼šå§‹ç»ˆå¼€å¯ ROBOTSTXT_OBEY</li>
                        <li><strong>æ§åˆ¶å¹¶å‘</strong>ï¼šCONCURRENT_REQUESTS ä¸è¦è®¾ç½®è¿‡å¤§ï¼ˆå»ºè®® 4-16ï¼‰</li>
                        <li><strong>ä½¿ç”¨ç¼“å­˜</strong>ï¼šé¿å…é‡å¤è¯·æ±‚åŒä¸€URL</li>
                    </ul>
                </div>

                <h4>ğŸ‘¤ User-Agent é…ç½®</h4>
<pre><code># é»˜è®¤ User-Agent
USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) ...'

# è¯·æ±‚å¤´é…ç½®
DEFAULT_REQUEST_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
}
</code></pre>

                <h4>ğŸ’¾ HTTPç¼“å­˜é…ç½®</h4>
<pre><code># å¯ç”¨HTTPç¼“å­˜ï¼ˆå¤§å¹…æå‡å¼€å‘æ•ˆç‡ï¼‰
HTTPCACHE_ENABLED = True
HTTPCACHE_EXPIRATION_SECS = 86400  # 1å¤©
HTTPCACHE_DIR = 'crawler/data/httpcache'

# å¿½ç•¥è¿™äº›çŠ¶æ€ç çš„ç¼“å­˜
HTTPCACHE_IGNORE_HTTP_CODES = [500, 502, 503, 504, 408, 429]
</code></pre>

                <div class="success-box">
                    <h4>âœ… ç¼“å­˜çš„å¥½å¤„</h4>
                    <ul>
                        <li>å¼€å‘è°ƒè¯•æ—¶æ— éœ€é‡å¤è¯·æ±‚ç½‘é¡µ</li>
                        <li>å‡å°‘å¯¹ç›®æ ‡ç½‘ç«™çš„è®¿é—®å‹åŠ›</li>
                        <li>åŠ å¿«æµ‹è¯•é€Ÿåº¦</li>
                    </ul>
                </div>

                <h3>3.2 ç®¡é“é…ç½®</h3>

<pre><code>ITEM_PIPELINES = {
    'crawler.pipelines.data_cleaning.DataCleaningPipeline': 100,
    'crawler.pipelines.data_validation.DataValidationPipeline': 200,
    'crawler.pipelines.sqlite_pipeline.SQLitePipeline': 300,
    'crawler.pipelines.neo4j_pipeline.Neo4jPipeline': 400,
}
</code></pre>

                <p>
                    <strong>æ•°å­—è¡¨ç¤ºä¼˜å…ˆçº§</strong>ï¼Œæ•°å€¼è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ã€‚æ•°æ®æŒ‰é¡ºåºç»è¿‡ï¼š
                    æ•°æ®æ¸…æ´— â†’ æ•°æ®éªŒè¯ â†’ SQLiteå­˜å‚¨ â†’ Neo4jå­˜å‚¨
                </p>

                <h3>3.3 ä¸­é—´ä»¶é…ç½®</h3>

<pre><code>DOWNLOADER_MIDDLEWARES = {
    'crawler.middlewares.RandomUserAgentMiddleware': 400,
    'crawler.middlewares.RetryMiddleware': 500,
}
</code></pre>

                <h3>3.4 æ•°æ®åº“é…ç½®</h3>

<pre><code># SQLiteæ•°æ®åº“è·¯å¾„
SQLITE_DB_PATH = 'server/database/historygogo.db'

# Neo4jæ•°æ®åº“é…ç½®
NEO4J_URI = 'bolt://localhost:7687'
NEO4J_USER = 'neo4j'
NEO4J_PASSWORD = 'your_password'
</code></pre>

                <h3>3.5 è‡ªå®šä¹‰é…ç½®</h3>

<pre><code># çˆ¬å–æ¨¡å¼ï¼š'test' æµ‹è¯•æ¨¡å¼ æˆ– 'full' å…¨é‡çˆ¬å–
CRAWL_MODE = 'test'

# æµ‹è¯•æ¨¡å¼ä¸‹çˆ¬å–çš„çš‡å¸æ•°é‡
TEST_EMPEROR_COUNT = 3

# éªŒè¯æŠ¥å‘Šè·¯å¾„
VALIDATION_REPORT_PATH = 'crawler/data/reports/validation_report.json'
</code></pre>
            </section>

            <!-- è¿è¡Œæµç¨‹ -->
            <section id="workflow">
                <h2>ğŸ”„ å››ã€è¿è¡Œæµç¨‹</h2>

                <h3>4.1 Scrapy æ‰§è¡Œæµç¨‹</h3>

                <div class="diagram">
                    <h4>è¯¦ç»†æ‰§è¡Œæµç¨‹</h4>
                    <ol style="text-align: left; max-width: 800px; margin: 20px auto;">
                        <li><strong>Engine</strong> ä» <strong>Spider</strong> è·å–åˆå§‹è¯·æ±‚ï¼ˆstart_requestsï¼‰</li>
                        <li><strong>Engine</strong> å°†è¯·æ±‚äº¤ç»™ <strong>Scheduler</strong> è°ƒåº¦</li>
                        <li><strong>Engine</strong> ä» <strong>Scheduler</strong> è·å–ä¸‹ä¸€ä¸ªè¯·æ±‚</li>
                        <li><strong>Engine</strong> é€šè¿‡ä¸‹è½½ä¸­é—´ä»¶å°†è¯·æ±‚å‘é€ç»™ <strong>Downloader</strong></li>
                        <li><strong>Downloader</strong> ä¸‹è½½é¡µé¢ï¼Œé€šè¿‡ä¸‹è½½ä¸­é—´ä»¶è¿”å›å“åº”ç»™ <strong>Engine</strong></li>
                        <li><strong>Engine</strong> å°†å“åº”é€šè¿‡çˆ¬è™«ä¸­é—´ä»¶å‘é€ç»™ <strong>Spider</strong> å¤„ç†</li>
                        <li><strong>Spider</strong> å¤„ç†å“åº”ï¼Œè¿”å› Item å’Œæ–°çš„ Request</li>
                        <li><strong>Engine</strong> å°† Item å‘é€ç»™ <strong>Pipeline</strong> å¤„ç†</li>
                        <li><strong>Engine</strong> å°†æ–°çš„ Request å‘é€ç»™ <strong>Scheduler</strong></li>
                        <li>é‡å¤æ­¥éª¤ 3-9ï¼Œç›´åˆ° <strong>Scheduler</strong> ä¸­æ²¡æœ‰æ›´å¤šè¯·æ±‚</li>
                    </ol>
                </div>

                <h3>4.2 HistoryGogo çˆ¬å–æµç¨‹</h3>

                <h4>ç¬¬ä¸€æ­¥ï¼šå¯åŠ¨çˆ¬è™«</h4>
<pre><code># æµ‹è¯•æ¨¡å¼ï¼ˆåªçˆ¬å–å‰3ä½çš‡å¸ï¼‰
python run_crawler.py --mode test --spider baidu_baike

# å…¨é‡çˆ¬å–æ¨¡å¼
python run_crawler.py --mode full --spider baidu_baike
</code></pre>

                <h4>ç¬¬äºŒæ­¥ï¼šç”Ÿæˆåˆå§‹è¯·æ±‚</h4>
<pre><code>def start_requests(self):
    """ç”Ÿæˆèµ·å§‹è¯·æ±‚"""
    # æ ¹æ®çˆ¬å–æ¨¡å¼å†³å®šçˆ¬å–å¤šå°‘ä½çš‡å¸
    emperors_to_crawl = MING_EMPERORS
    if self.crawl_mode == 'test':
        emperors_to_crawl = MING_EMPERORS[:self.test_emperor_count]
    
    # ä¸ºæ¯ä½çš‡å¸ç”Ÿæˆè¯·æ±‚
    for emperor_info in emperors_to_crawl:
        url = f"https://baike.baidu.com/item/{emperor_info['name']}"
        yield scrapy.Request(
            url=url,
            callback=self.parse_emperor,
            meta={'emperor_info': emperor_info}
        )
</code></pre>

                <h4>ç¬¬ä¸‰æ­¥ï¼šè§£æçš‡å¸é¡µé¢</h4>
<pre><code>def parse_emperor(self, response):
    """è§£æçš‡å¸é¡µé¢"""
    # 1. æå–çš‡å¸åŸºæœ¬ä¿¡æ¯
    emperor_data = self._extract_emperor_data(soup, emperor_info)
    
    # 2. åˆ›å»º Emperor å®ä½“å¹¶yieldï¼ˆè¿›å…¥Pipelineï¼‰
    emperor = self._create_emperor_entity(emperor_data, emperor_info)
    yield emperor
    
    # 3. æå–è¯¥çš‡å¸ç›¸å…³çš„äº‹ä»¶é“¾æ¥
    event_links = self._extract_event_links(soup)
    for event_name in event_links[:10]:
        url = self._build_baidu_url(event_name)
        yield scrapy.Request(
            url=url,
            callback=self.parse_event,
            meta={'emperor_id': emperor_id}
        )
    
    # 4. æå–ç›¸å…³äººç‰©é“¾æ¥
    person_links = self._extract_person_links(soup)
    for person_name in person_links[:20]:
        url = self._build_baidu_url(person_name)
        yield scrapy.Request(
            url=url,
            callback=self.parse_person,
            meta={'emperor_id': emperor_id}
        )
</code></pre>

                <h4>ç¬¬å››æ­¥ï¼šæ•°æ®è¿›å…¥Pipelineå¤„ç†</h4>
                <div class="flow-chart" style="justify-content: flex-start;">
                    <div class="flow-item">æ•°æ®æ¸…æ´—<br>å»é™¤ç©ºç™½ã€è§„èŒƒåŒ–</div>
                    <span class="flow-arrow"></span>
                    <div class="flow-item">æ•°æ®éªŒè¯<br>æ£€æŸ¥å¿…å¡«å­—æ®µ</div>
                    <span class="flow-arrow"></span>
                    <div class="flow-item">SQLiteå­˜å‚¨<br>å…³ç³»å‹æ•°æ®</div>
                    <span class="flow-arrow"></span>
                    <div class="flow-item">Neo4jå­˜å‚¨<br>å›¾æ•°æ®åº“</div>
                </div>

                <h3>4.3 å¹¶å‘ä¸å¼‚æ­¥</h3>

                <div class="info-box">
                    <h4>ğŸš€ Scrapy çš„å¼‚æ­¥ä¼˜åŠ¿</h4>
                    <p>
                        Scrapy åŸºäº <strong>Twisted</strong> å¼‚æ­¥æ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†å¤šä¸ªè¯·æ±‚è€Œä¸é˜»å¡ã€‚
                        å½“ä¸€ä¸ªè¯·æ±‚åœ¨ç­‰å¾…ç½‘ç»œå“åº”æ—¶ï¼Œå¼•æ“å¯ä»¥åŒæ—¶å¤„ç†å…¶ä»–è¯·æ±‚ã€‚
                    </p>
                    <p>
                        <strong>ä¸¾ä¾‹</strong>ï¼šè®¾ç½® <code>CONCURRENT_REQUESTS = 8</code>ï¼Œ
                        Scrapy å¯ä»¥åŒæ—¶å‘é€8ä¸ªè¯·æ±‚ã€‚å¦‚æœæ¯ä¸ªè¯·æ±‚éœ€è¦2ç§’å“åº”ï¼Œç†è®ºä¸Š
                        <span class="highlight">æ¯2ç§’å¯ä»¥å®Œæˆ8ä¸ªè¯·æ±‚</span>ï¼Œ
                        è€ŒåŒæ­¥æ–¹å¼éœ€è¦16ç§’ã€‚
                    </p>
                </div>
            </section>

            <!-- Spiderå¼€å‘ -->
            <section id="spider">
                <h2>ğŸ•·ï¸ äº”ã€Spider å¼€å‘</h2>

                <h3>5.1 Spider åŸºç¡€ç»“æ„</h3>

<pre><code>import scrapy

class BaiduBaikeSpider(scrapy.Spider):
    """ç™¾åº¦ç™¾ç§‘çˆ¬è™«"""
    
    # Spider å”¯ä¸€æ ‡è¯†
    name = 'baidu_baike'
    
    # å…è®¸çˆ¬å–çš„åŸŸå
    allowed_domains = ['baike.baidu.com']
    
    # è‡ªå®šä¹‰é…ç½®ï¼ˆè¦†ç›–settings.pyï¼‰
    custom_settings = {
        'DOWNLOAD_DELAY': 3,
        'RANDOMIZE_DOWNLOAD_DELAY': True,
        'CONCURRENT_REQUESTS': 4,
    }
    
    def __init__(self, crawl_mode='test', *args, **kwargs):
        """åˆå§‹åŒ–"""
        super().__init__(*args, **kwargs)
        self.crawl_mode = crawl_mode
    
    def start_requests(self):
        """ç”Ÿæˆåˆå§‹è¯·æ±‚"""
        for emperor_info in MING_EMPERORS:
            yield scrapy.Request(
                url=f"https://baike.baidu.com/item/{emperor_info['name']}",
                callback=self.parse
            )
    
    def parse(self, response):
        """é»˜è®¤è§£ææ–¹æ³•"""
        # æå–æ•°æ®
        # yield Item æˆ–æ–°çš„ Request
        pass
</code></pre>

                <h3>5.2 æ•°æ®æå–</h3>

                <h4>ä½¿ç”¨ BeautifulSoup è§£æ</h4>
<pre><code>from bs4 import BeautifulSoup

def parse_emperor(self, response):
    soup = BeautifulSoup(response.text, 'lxml')
    
    # æå–æ ‡é¢˜
    title = soup.select_one('.lemmaWgt-lemmaTitle-title h1').get_text()
    
    # æå–ä¿¡æ¯æ¡†
    info_box = soup.select_one('.basic-info')
    if info_box:
        birth_elem = info_box.find('dt', text=re.compile('å‡ºç”Ÿæ—¥æœŸ'))
        if birth_elem:
            birth_text = birth_elem.find_next_sibling('dd').get_text(strip=True)
    
    # æå–ç®€ä»‹
    summary = soup.select_one('.lemma-summary')
    if summary:
        paragraphs = summary.find_all('div', class_='para')
        biography = paragraphs[0].get_text()
</code></pre>

                <h4>ä½¿ç”¨ Scrapy é€‰æ‹©å™¨</h4>
<pre><code>def parse(self, response):
    # CSS é€‰æ‹©å™¨
    title = response.css('.lemmaWgt-lemmaTitle-title h1::text').get()
    
    # XPath é€‰æ‹©å™¨
    paragraphs = response.xpath('//div[@class="para"]/text()').getall()
    
    # æå–æ‰€æœ‰é“¾æ¥
    links = response.css('a::attr(href)').getall()
</code></pre>

                <h3>5.3 è¯·æ±‚ä¼ é€’æ•°æ®</h3>

                <p>ä½¿ç”¨ <code>meta</code> åœ¨è¯·æ±‚é—´ä¼ é€’æ•°æ®ï¼š</p>

<pre><code>def parse_emperor(self, response):
    emperor_id = "ming_emperor_001"
    
    # ç”Ÿæˆäº‹ä»¶è¯·æ±‚ï¼Œä¼ é€’ emperor_id
    yield scrapy.Request(
        url="https://baike.baidu.com/item/é–éš¾ä¹‹å½¹",
        callback=self.parse_event,
        meta={'emperor_id': emperor_id}
    )

def parse_event(self, response):
    # è·å–ä¼ é€’çš„æ•°æ®
    emperor_id = response.meta['emperor_id']
    
    event = Event(
        emperor_id=emperor_id,
        title=response.css('h1::text').get()
    )
    yield event
</code></pre>

                <h3>5.4 é”™è¯¯å¤„ç†</h3>

<pre><code>def parse_emperor(self, response):
    try:
        soup = BeautifulSoup(response.text, 'lxml')
        emperor_data = self._extract_emperor_data(soup, emperor_info)
        
        if emperor_data:
            self.logger.info(f"æˆåŠŸçˆ¬å–çš‡å¸: {emperor_data['name']}")
            yield emperor_data
        else:
            self.logger.warning(f"æœªèƒ½æå–åˆ°çš‡å¸æ•°æ®")
    
    except Exception as e:
        self.logger.error(f"è§£æçš‡å¸é¡µé¢å¤±è´¥: {str(e)}")
</code></pre>

                <h3>5.5 é“¾æ¥æå–ç­–ç•¥</h3>

                <h4>æå–äº‹ä»¶é“¾æ¥ï¼ˆåŸºäºå…³é”®è¯ï¼‰</h4>
<pre><code>def _extract_event_links(self, soup):
    """æå–äº‹ä»¶ç›¸å…³é“¾æ¥"""
    events = []
    content = soup.select_one('.main-content')
    
    # äº‹ä»¶å…³é”®è¯
    event_keywords = ['ä¹‹å½¹', 'ä¹‹æˆ˜', 'ä¹‹å˜', 'æ”¿å˜', 'èµ·ä¹‰', 'æ”¹é©']
    
    links = content.find_all('a', href=True)
    for link in links:
        link_text = link.get_text(strip=True)
        if any(keyword in link_text for keyword in event_keywords):
            events.append(link_text)
    
    return list(set(events))[:15]  # å»é‡å¹¶é™åˆ¶æ•°é‡
</code></pre>

                <h4>æå–äººç‰©é“¾æ¥ï¼ˆåŸºäºé•¿åº¦ï¼‰</h4>
<pre><code>def _extract_person_links(self, soup):
    """æå–äººç‰©ç›¸å…³é“¾æ¥"""
    persons = []
    links = soup.find_all('a', href=True)
    
    for link in links:
        link_text = link.get_text(strip=True)
        # äººåé€šå¸¸æ˜¯2-4ä¸ªä¸­æ–‡å­—ç¬¦
        if 2 <= len(link_text) <= 4:
            if all('\u4e00' <= c <= '\u9fff' for c in link_text):
                persons.append(link_text)
    
    return list(set(persons))[:25]
</code></pre>
            </section>

            <!-- Pipelineç®¡é“ -->
            <section id="pipeline">
                <h2>ğŸ“Š å…­ã€Pipeline æ•°æ®ç®¡é“</h2>

                <h3>6.1 Pipeline åŸºç¡€ç»“æ„</h3>

<pre><code>class DataCleaningPipeline:
    """æ•°æ®æ¸…æ´—ç®¡é“"""
    
    def open_spider(self, spider):
        """çˆ¬è™«å¯åŠ¨æ—¶è°ƒç”¨"""
        spider.logger.info("æ•°æ®æ¸…æ´—ç®¡é“å·²å¯åŠ¨")
    
    def close_spider(self, spider):
        """çˆ¬è™«å…³é—­æ—¶è°ƒç”¨"""
        spider.logger.info("æ•°æ®æ¸…æ´—ç®¡é“å·²å…³é—­")
    
    def process_item(self, item, spider):
        """å¤„ç†æ¯ä¸ªItem"""
        # æ¸…æ´—é€»è¾‘
        if isinstance(item, Emperor):
            item.name = item.name.strip()
            if item.biography:
                item.biography = self._clean_text(item.biography)
        
        return item  # å¿…é¡»è¿”å›itemä¼ é€’ç»™ä¸‹ä¸€ä¸ªç®¡é“
</code></pre>

                <h3>6.2 æ•°æ®æ¸…æ´—ç®¡é“</h3>

<pre><code>class DataCleaningPipeline:
    """æ•°æ®æ¸…æ´—ç®¡é“"""
    
    def process_item(self, item, spider):
        """æ¸…æ´—æ•°æ®"""
        if isinstance(item, Emperor):
            # å»é™¤å¤šä½™ç©ºç™½
            item.name = self._clean_text(item.name)
            item.biography = self._clean_text(item.biography)
            
            # å¤„ç†Noneå€¼
            if not item.temple_name:
                item.temple_name = f"{item.name}ï¼ˆåº™å·æœªçŸ¥ï¼‰"
        
        elif isinstance(item, Event):
            # è§„èŒƒåŒ–åœ°ç‚¹åç§°
            if item.location:
                item.location = self._normalize_location(item.location)
        
        return item
    
    def _clean_text(self, text):
        """æ¸…æ´—æ–‡æœ¬"""
        if not text:
            return None
        # å»é™¤å¤šä½™ç©ºç™½
        text = ' '.join(text.split())
        # å»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[\r\n\t]', '', text)
        return text.strip()
</code></pre>

                <h3>6.3 æ•°æ®éªŒè¯ç®¡é“</h3>

<pre><code>class DataValidationPipeline:
    """æ•°æ®éªŒè¯ç®¡é“"""
    
    def __init__(self):
        self.validation_errors = []
    
    def process_item(self, item, spider):
        """éªŒè¯æ•°æ®"""
        errors = []
        
        if isinstance(item, Emperor):
            # å¿…å¡«å­—æ®µæ£€æŸ¥
            if not item.name:
                errors.append("ç¼ºå°‘çš‡å¸å§“å")
            if not item.reign_start:
                errors.append("ç¼ºå°‘åœ¨ä½å¼€å§‹æ—¶é—´")
            
            # é€»è¾‘æ£€æŸ¥
            if item.birth_date and item.death_date:
                if item.birth_date > item.death_date:
                    errors.append("å‡ºç”Ÿæ—¥æœŸæ™šäºå»ä¸–æ—¥æœŸ")
        
        if errors:
            self.validation_errors.append({
                'item_type': type(item).__name__,
                'item_id': getattr(item, f'{type(item).__name__.lower()}_id', 'unknown'),
                'errors': errors
            })
            spider.logger.warning(f"æ•°æ®éªŒè¯å¤±è´¥: {errors}")
        
        return item
</code></pre>

                <h3>6.4 SQLite æŒä¹…åŒ–ç®¡é“</h3>

<pre><code>class SQLitePipeline:
    """SQLiteæŒä¹…åŒ–ç®¡é“"""
    
    def __init__(self, db_path):
        self.db_manager = SQLiteManager(db_path)
        self.stats = {'emperors': 0, 'events': 0, 'persons': 0}
    
    def open_spider(self, spider):
        """æ‰“å¼€æ•°æ®åº“è¿æ¥"""
        self.db_manager.connect()
    
    def close_spider(self, spider):
        """å…³é—­æ•°æ®åº“å¹¶è¾“å‡ºç»Ÿè®¡"""
        spider.logger.info(f"SQLiteæŒä¹…åŒ–ç»Ÿè®¡: {self.stats}")
        self.db_manager.close()
    
    def process_item(self, item, spider):
        """ä¿å­˜æ•°æ®"""
        if isinstance(item, Emperor):
            self._save_emperor(item, spider)
            self.stats['emperors'] += 1
        elif isinstance(item, Event):
            self._save_event(item, spider)
            self.stats['events'] += 1
        
        return item
    
    def _save_emperor(self, emperor, spider):
        """ä¿å­˜çš‡å¸æ•°æ®"""
        sql = """
        INSERT OR REPLACE INTO emperors (
            emperor_id, name, temple_name, birth_date, ...
        ) VALUES (?, ?, ?, ?, ...)
        """
        params = (emperor.emperor_id, emperor.name, ...)
        self.db_manager.execute(sql, params)
</code></pre>

                <h3>6.5 Pipeline é“¾å¼å¤„ç†</h3>

                <div class="info-box">
                    <h4>ğŸ’¡ Pipeline æ‰§è¡Œé¡ºåº</h4>
                    <p>PipelineæŒ‰ä¼˜å…ˆçº§é¡ºåºæ‰§è¡Œï¼Œæ•°æ®ä¾æ¬¡æµè¿‡æ‰€æœ‰ç®¡é“ï¼š</p>
                    <ol>
                        <li><strong>DataCleaningPipeline (100)</strong> - æ¸…æ´—æ•°æ®</li>
                        <li><strong>DataValidationPipeline (200)</strong> - éªŒè¯æ•°æ®</li>
                        <li><strong>SQLitePipeline (300)</strong> - å­˜å‚¨åˆ°SQLite</li>
                        <li><strong>Neo4jPipeline (400)</strong> - å­˜å‚¨åˆ°Neo4j</li>
                    </ol>
                    <p>å¦‚æœæŸä¸ªPipelineè¿”å›Noneï¼Œæ•°æ®æµä¸­æ–­ï¼Œåç»­Pipelineä¸ä¼šæ‰§è¡Œã€‚</p>
                </div>
            </section>

            <!-- ä¸­é—´ä»¶ -->
            <section id="middleware">
                <h2>ğŸ”Œ ä¸ƒã€ä¸­é—´ä»¶å¼€å‘</h2>

                <h3>7.1 ä¸‹è½½ä¸­é—´ä»¶</h3>

                <h4>éšæœº User-Agent ä¸­é—´ä»¶</h4>
<pre><code>from fake_useragent import UserAgent

class RandomUserAgentMiddleware:
    """éšæœºUser-Agentä¸­é—´ä»¶"""
    
    def __init__(self):
        self.ua = UserAgent()
    
    def process_request(self, request, spider):
        """ä¸ºæ¯ä¸ªè¯·æ±‚è®¾ç½®éšæœºUser-Agent"""
        try:
            request.headers['User-Agent'] = self.ua.random
        except Exception:
            # å¦‚æœè·å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤UA
            request.headers['User-Agent'] = 'Mozilla/5.0 ...'
        
        return None  # è¿”å›Noneç»§ç»­å¤„ç†
</code></pre>

                <h4>è‡ªå®šä¹‰é‡è¯•ä¸­é—´ä»¶</h4>
<pre><code>import time
import random

class RetryMiddleware:
    """è‡ªå®šä¹‰é‡è¯•ä¸­é—´ä»¶"""
    
    def __init__(self, max_retry_times=3):
        self.max_retry_times = max_retry_times
    
    def process_response(self, request, response, spider):
        """å¤„ç†å“åº”"""
        # å¦‚æœæ˜¯429ï¼ˆå¤ªå¤šè¯·æ±‚ï¼‰ï¼Œå¢åŠ å»¶è¿Ÿåé‡è¯•
        if response.status == 429:
            retry_times = request.meta.get('retry_times', 0) + 1
            
            if retry_times <= self.max_retry_times:
                spider.logger.warning(
                    f"æ”¶åˆ°429é”™è¯¯ï¼Œç­‰å¾…åé‡è¯• (ç¬¬{retry_times}æ¬¡)"
                )
                time.sleep(random.randint(5, 10))  # ç­‰å¾…5-10ç§’
                
                retryreq = request.copy()
                retryreq.meta['retry_times'] = retry_times
                return retryreq
        
        return response
</code></pre>

                <h3>7.2 çˆ¬è™«ä¸­é—´ä»¶</h3>

                <h4>æ•°æ®åˆå¹¶ä¸­é—´ä»¶ï¼ˆåŒæºäº’è¡¥ï¼‰</h4>
<pre><code>class DataMergeMiddleware:
    """æ•°æ®åˆå¹¶ä¸­é—´ä»¶
    
    åˆå¹¶æ¥è‡ªç™¾åº¦ç™¾ç§‘å’Œç»´åŸºç™¾ç§‘çš„æ•°æ®
    """
    
    def __init__(self):
        self.data_cache = {}
    
    def process_item(self, item, spider):
        """åˆå¹¶æ•°æ®"""
        item_id = self._get_item_id(item)
        
        if item_id in self.data_cache:
            # ç¼“å­˜ä¸­å·²æœ‰è¯¥æ•°æ®ï¼Œæ‰§è¡Œåˆå¹¶
            cached_item = self.data_cache[item_id]
            merged_item = self._merge_items(cached_item, item, spider)
            self.data_cache[item_id] = merged_item
            return merged_item
        else:
            # é¦–æ¬¡é‡åˆ°ï¼ŒåŠ å…¥ç¼“å­˜
            self.data_cache[item_id] = item
            return item
    
    def _merge_items(self, item1, item2, spider):
        """åˆå¹¶ä¸¤ä¸ªæ•°æ®é¡¹
        
        ç­–ç•¥ï¼š
        1. ä¼˜å…ˆä½¿ç”¨ç™¾åº¦ç™¾ç§‘çš„ç»“æ„åŒ–å­—æ®µ
        2. ä½¿ç”¨ç»´åŸºç™¾ç§‘è¡¥å……è¯¦ç»†æè¿°
        3. åˆ—è¡¨å­—æ®µå–å¹¶é›†
        """
        baidu_item = item1 if item1.data_source == 'baidu' else item2
        wiki_item = item2 if item2.data_source == 'wikipedia' else item1
        
        merged = baidu_item
        
        # è¡¥å……ç©ºç¼ºå­—æ®µ
        for field in ['biography', 'description', 'achievements']:
            wiki_value = getattr(wiki_item, field, None)
            merged_value = getattr(merged, field, None)
            
            # å¦‚æœç™¾åº¦æ•°æ®ä¸ºç©ºï¼Œä½¿ç”¨ç»´åŸºæ•°æ®
            if not merged_value and wiki_value:
                setattr(merged, field, wiki_value)
        
        merged.data_source = 'baidu,wikipedia'
        return merged
</code></pre>

                <h3>7.3 ä¸­é—´ä»¶æ‰§è¡Œé¡ºåº</h3>

                <div class="warning-box">
                    <h4>âš ï¸ ä¸­é—´ä»¶ä¼˜å…ˆçº§</h4>
                    <p><strong>ä¸‹è½½ä¸­é—´ä»¶</strong>ï¼ˆDownloader Middlewareï¼‰ï¼š</p>
                    <ul>
                        <li><code>process_request</code>ï¼šæ•°å­—è¶Šå°è¶Šå…ˆæ‰§è¡Œ</li>
                        <li><code>process_response</code>ï¼šæ•°å­—è¶Šå¤§è¶Šå…ˆæ‰§è¡Œï¼ˆåå‘ï¼‰</li>
                    </ul>
                    <p><strong>çˆ¬è™«ä¸­é—´ä»¶</strong>ï¼ˆSpider Middlewareï¼‰ï¼š</p>
                    <ul>
                        <li>æ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜</li>
                    </ul>
                </div>
            </section>

            <!-- æœ€ä½³å®è·µ -->
            <section id="bestpractices">
                <h2>ğŸ’¡ å…«ã€æœ€ä½³å®è·µä¸å¼€å‘æ³¨æ„äº‹é¡¹</h2>

                <h3>8.1 åçˆ¬è™«ç­–ç•¥</h3>

                <div class="danger-box">
                    <h4>ğŸ›¡ï¸ å¸¸è§åçˆ¬æœºåˆ¶åŠåº”å¯¹</h4>
                    <table>
                        <tr>
                            <th>åçˆ¬æœºåˆ¶</th>
                            <th>åº”å¯¹ç­–ç•¥</th>
                        </tr>
                        <tr>
                            <td>User-Agentæ£€æµ‹</td>
                            <td>ä½¿ç”¨RandomUserAgentMiddlewareè½®æ¢UA</td>
                        </tr>
                        <tr>
                            <td>IPé™åˆ¶</td>
                            <td>å¢åŠ DOWNLOAD_DELAYï¼Œä½¿ç”¨ä»£ç†æ± </td>
                        </tr>
                        <tr>
                            <td>é¢‘ç‡é™åˆ¶</td>
                            <td>å¯ç”¨RANDOMIZE_DOWNLOAD_DELAYï¼Œæ§åˆ¶å¹¶å‘</td>
                        </tr>
                        <tr>
                            <td>CookieéªŒè¯</td>
                            <td>å¯ç”¨COOKIES_ENABLEDï¼Œä¿æŒä¼šè¯</td>
                        </tr>
                        <tr>
                            <td>JavaScriptæ¸²æŸ“</td>
                            <td>ä½¿ç”¨Splashæˆ–Seleniumä¸­é—´ä»¶</td>
                        </tr>
                        <tr>
                            <td>éªŒè¯ç </td>
                            <td>OCRè¯†åˆ«æˆ–äººå·¥ä»‹å…¥</td>
                        </tr>
                    </table>
                </div>

                <h3>8.2 æ€§èƒ½ä¼˜åŒ–</h3>

                <h4>âœ… åˆç†é…ç½®å¹¶å‘</h4>
<pre><code># æ ¹æ®ç›®æ ‡ç½‘ç«™è°ƒæ•´
CONCURRENT_REQUESTS = 16  # é»˜è®¤16ï¼Œå¯é€‚å½“å¢åŠ 
CONCURRENT_REQUESTS_PER_DOMAIN = 8  # å•åŸŸåå¹¶å‘
CONCURRENT_REQUESTS_PER_IP = 4  # å•IPå¹¶å‘
</code></pre>

                <h4>âœ… ä½¿ç”¨HTTPç¼“å­˜</h4>
<pre><code>HTTPCACHE_ENABLED = True
HTTPCACHE_EXPIRATION_SECS = 86400  # 1å¤©
</code></pre>

                <h4>âœ… ç¦ç”¨ä¸å¿…è¦çš„ä¸­é—´ä»¶</h4>
<pre><code># å¦‚æœä¸éœ€è¦Cookieï¼Œå¯ä»¥ç¦ç”¨
COOKIES_ENABLED = False

# ç¦ç”¨æŸäº›é»˜è®¤ä¸­é—´ä»¶
DOWNLOADER_MIDDLEWARES = {
    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
}
</code></pre>

                <h3>8.3 æ•°æ®è´¨é‡ä¿è¯</h3>

                <h4>âœ… å¤šæºæ•°æ®äº’è¡¥</h4>
                <div class="success-box">
                    <p>
                        HistoryGogo é‡‡ç”¨<strong>åŒæºçˆ¬å–ç­–ç•¥</strong>ï¼š
                        åŒæ—¶ä»ç™¾åº¦ç™¾ç§‘å’Œç»´åŸºç™¾ç§‘æŠ“å–æ•°æ®ï¼Œé€šè¿‡ DataMergeMiddleware åˆå¹¶ï¼Œ
                        æé«˜æ•°æ®å®Œæ•´æ€§å’Œå‡†ç¡®æ€§ã€‚
                    </p>
                </div>

                <h4>âœ… æ•°æ®éªŒè¯</h4>
<pre><code># å¿…å¡«å­—æ®µæ£€æŸ¥
if not emperor.name:
    raise ValueError("ç¼ºå°‘çš‡å¸å§“å")

# é€»è¾‘éªŒè¯
if emperor.birth_date > emperor.death_date:
    raise ValueError("å‡ºç”Ÿæ—¥æœŸæ™šäºå»ä¸–æ—¥æœŸ")

# æ ¼å¼éªŒè¯
if not re.match(r'^\d{4}-\d{2}-\d{2}$', date_str):
    raise ValueError("æ—¥æœŸæ ¼å¼ä¸æ­£ç¡®")
</code></pre>

                <h4>âœ… å¼‚å¸¸å¤„ç†</h4>
<pre><code>def parse_emperor(self, response):
    try:
        # è§£æé€»è¾‘
        emperor_data = self._extract_emperor_data(soup)
        yield emperor_data
    except Exception as e:
        self.logger.error(f"è§£æå¤±è´¥: {str(e)}")
        # å¯é€‰ï¼šå‘é€é”™è¯¯é€šçŸ¥ã€è®°å½•åˆ°æ•°æ®åº“ç­‰
</code></pre>

                <h3>8.4 æ—¥å¿—ä¸ç›‘æ§</h3>

                <h4>âœ… åˆç†ä½¿ç”¨æ—¥å¿—çº§åˆ«</h4>
<pre><code># settings.py
LOG_LEVEL = 'INFO'  # DEBUG, INFO, WARNING, ERROR, CRITICAL

# spiderä¸­
self.logger.debug("è°ƒè¯•ä¿¡æ¯")
self.logger.info("çˆ¬å–æˆåŠŸ")
self.logger.warning("æ•°æ®ä¸å®Œæ•´")
self.logger.error("è§£æå¤±è´¥")
</code></pre>

                <h4>âœ… è¾“å‡ºç»Ÿè®¡ä¿¡æ¯</h4>
<pre><code>def close_spider(self, spider):
    """çˆ¬è™«å…³é—­æ—¶è¾“å‡ºç»Ÿè®¡"""
    spider.logger.info(
        f"çˆ¬å–ç»Ÿè®¡: "
        f"çš‡å¸={self.stats['emperors']}, "
        f"äº‹ä»¶={self.stats['events']}, "
        f"äººç‰©={self.stats['persons']}"
    )
</code></pre>

                <h3>8.5 æµ‹è¯•ä¸è°ƒè¯•</h3>

                <h4>âœ… Scrapy Shell</h4>
<pre><code># å¯åŠ¨shellè°ƒè¯•
scrapy shell 'https://baike.baidu.com/item/æœ±å…ƒç’‹'

# åœ¨shellä¸­æµ‹è¯•é€‰æ‹©å™¨
>>> response.css('.lemmaWgt-lemmaTitle-title h1::text').get()
'æœ±å…ƒç’‹'

>>> response.xpath('//div[@class="para"]/text()').getall()
</code></pre>

                <h4>âœ… æµ‹è¯•æ¨¡å¼</h4>
<pre><code># ä½¿ç”¨æµ‹è¯•æ¨¡å¼ï¼ˆåªçˆ¬å–å°‘é‡æ•°æ®ï¼‰
python run_crawler.py --mode test --spider baidu_baike

# åœ¨Spiderä¸­å®ç°
if self.crawl_mode == 'test':
    emperors_to_crawl = MING_EMPERORS[:3]  # åªçˆ¬3ä½çš‡å¸
</code></pre>

                <h3>8.6 ä»£ç è§„èŒƒ</h3>

                <div class="info-box">
                    <h4>ğŸ“ ç¼–ç å»ºè®®</h4>
                    <ul>
                        <li><strong>æ¨¡å—åŒ–è®¾è®¡</strong>ï¼šå°†æ•°æ®æå–ã€æ¸…æ´—ã€éªŒè¯é€»è¾‘åˆ†ç¦»</li>
                        <li><strong>ä½¿ç”¨dataclass</strong>ï¼šå®šä¹‰æ¸…æ™°çš„æ•°æ®æ¨¡å‹</li>
                        <li><strong>å¼‚å¸¸å¤„ç†</strong>ï¼šæ¯ä¸ªå…³é”®ç¯èŠ‚éƒ½è¦æ•è·å¼‚å¸¸</li>
                        <li><strong>æ—¥å¿—è®°å½•</strong>ï¼šè®°å½•å…³é”®æ“ä½œå’Œé”™è¯¯ä¿¡æ¯</li>
                        <li><strong>é…ç½®åŒ–</strong>ï¼šé¿å…ç¡¬ç¼–ç ï¼Œä½¿ç”¨settingsé…ç½®</li>
                        <li><strong>æ–‡æ¡£æ³¨é‡Š</strong>ï¼šä¸ºç±»å’Œæ–¹æ³•æ·»åŠ docstring</li>
                    </ul>
                </div>

                <h3>8.7 å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ</h3>

                <table>
                    <tr>
                        <th>é—®é¢˜</th>
                        <th>åŸå› </th>
                        <th>è§£å†³æ–¹æ¡ˆ</th>
                    </tr>
                    <tr>
                        <td>è¿”å›ç©ºResponse</td>
                        <td>robots.txtç¦æ­¢ã€è¢«å°IP</td>
                        <td>æ£€æŸ¥ROBOTSTXT_OBEYï¼Œå¢åŠ å»¶è¿Ÿ</td>
                    </tr>
                    <tr>
                        <td>é€‰æ‹©å™¨æå–å¤±è´¥</td>
                        <td>é¡µé¢ç»“æ„å˜åŒ–ã€JSæ¸²æŸ“</td>
                        <td>æ£€æŸ¥HTMLç»“æ„ï¼Œè€ƒè™‘ä½¿ç”¨Splash</td>
                    </tr>
                    <tr>
                        <td>æ•°æ®é‡å¤</td>
                        <td>URLå»é‡å¤±è´¥</td>
                        <td>æ£€æŸ¥dont_filterå‚æ•°ï¼Œä½¿ç”¨æ•°æ®åº“å”¯ä¸€çº¦æŸ</td>
                    </tr>
                    <tr>
                        <td>å†…å­˜å ç”¨è¿‡é«˜</td>
                        <td>å¹¶å‘è¿‡å¤§ã€ç¼“å­˜è¿‡å¤š</td>
                        <td>é™ä½CONCURRENT_REQUESTSï¼Œé™åˆ¶ç¼“å­˜å¤§å°</td>
                    </tr>
                    <tr>
                        <td>Pipelineæœªæ‰§è¡Œ</td>
                        <td>æœªåœ¨settingsä¸­é…ç½®</td>
                        <td>æ£€æŸ¥ITEM_PIPELINESé…ç½®</td>
                    </tr>
                </table>

                <h3>8.8 é¡¹ç›®å®æˆ˜æŠ€å·§</h3>

                <h4>âœ… å¢é‡çˆ¬å–</h4>
<pre><code># æ£€æŸ¥æ•°æ®æ˜¯å¦å·²å­˜åœ¨
def should_crawl(self, emperor_id):
    """æ£€æŸ¥æ˜¯å¦éœ€è¦çˆ¬å–"""
    exists = self.db_manager.exists('emperors', emperor_id)
    return not exists

def start_requests(self):
    for emperor_info in MING_EMPERORS:
        emperor_id = generate_id("ming_emperor", emperor_info['name'])
        if self.should_crawl(emperor_id):
            yield scrapy.Request(...)
</code></pre>

                <h4>âœ… æ–­ç‚¹ç»­çˆ¬</h4>
<pre><code># å¯ç”¨ä»»åŠ¡æŒä¹…åŒ–
JOBDIR = 'crawler/data/jobs/baidu_baike'

# è¿è¡Œæ—¶Scrapyä¼šè‡ªåŠ¨ä¿å­˜è¿›åº¦
# ä¸­æ–­åå†æ¬¡è¿è¡Œä¼šä»æ–­ç‚¹ç»§ç»­
</code></pre>

                <h4>âœ… ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š</h4>
<pre><code>def close_spider(self, spider):
    """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
    report = {
        'total_items': self.total_items,
        'valid_items': self.valid_items,
        'errors': self.validation_errors,
        'timestamp': datetime.now().isoformat()
    }
    
    with open(self.report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
</code></pre>

                <div class="success-box">
                    <h4>ğŸ“ å­¦ä¹ å»ºè®®</h4>
                    <ol>
                        <li><strong>ä»ç®€å•å¼€å§‹</strong>ï¼šå…ˆçˆ¬å–å•ä¸ªé¡µé¢ï¼Œé€æ­¥å¢åŠ å¤æ‚åº¦</li>
                        <li><strong>ä½¿ç”¨Scrapy Shell</strong>ï¼šè°ƒè¯•é€‰æ‹©å™¨éå¸¸é«˜æ•ˆ</li>
                        <li><strong>é˜…è¯»å®˜æ–¹æ–‡æ¡£</strong>ï¼šScrapyæ–‡æ¡£éå¸¸è¯¦ç»†</li>
                        <li><strong>ç ”ç©¶å®é™…é¡¹ç›®</strong>ï¼šå¦‚HistoryGogoï¼Œå­¦ä¹ æ¶æ„è®¾è®¡</li>
                        <li><strong>éµå®ˆç½‘ç»œç¤¼ä»ª</strong>ï¼šå°Šé‡robots.txtï¼Œæ§åˆ¶çˆ¬å–é¢‘ç‡</li>
                    </ol>
                </div>
            </section>

            <!-- æ€»ç»“ -->
            <section>
                <h2>ğŸ“š æ€»ç»“</h2>
                
                <p>
                    Scrapy æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§ã€è®¾è®¡ä¼˜é›…çš„çˆ¬è™«æ¡†æ¶ï¼Œé€šè¿‡æœ¬æ–‡æ¡£çš„å­¦ä¹ ï¼Œ
                    ä½ åº”è¯¥å·²ç»æŒæ¡äº† Scrapy çš„æ ¸å¿ƒæ¦‚å¿µå’Œå®æˆ˜æŠ€å·§ã€‚
                </p>

                <div class="info-box">
                    <h4>ğŸ¯ å…³é”®è¦ç‚¹å›é¡¾</h4>
                    <ol>
                        <li><strong>æ¶æ„ç†è§£</strong>ï¼šEngineã€Schedulerã€Downloaderã€Spiderã€Pipeline å„å¸å…¶èŒ</li>
                        <li><strong>é…ç½®ä¼˜åŒ–</strong>ï¼šåˆç†è®¾ç½®å»¶è¿Ÿã€å¹¶å‘ã€ç¼“å­˜ç­‰å‚æ•°</li>
                        <li><strong>Spiderå¼€å‘</strong>ï¼šæŒæ¡è¯·æ±‚ç”Ÿæˆã€æ•°æ®æå–ã€é“¾æ¥è·Ÿè¸ª</li>
                        <li><strong>Pipelineå¤„ç†</strong>ï¼šæ•°æ®æ¸…æ´—ã€éªŒè¯ã€æŒä¹…åŒ–çš„æµæ°´çº¿å¤„ç†</li>
                        <li><strong>ä¸­é—´ä»¶åº”ç”¨</strong>ï¼šé€šè¿‡ä¸­é—´ä»¶å®ç°åçˆ¬ã€æ•°æ®åˆå¹¶ç­‰åŠŸèƒ½</li>
                        <li><strong>æœ€ä½³å®è·µ</strong>ï¼šéµå®ˆç½‘ç»œç¤¼ä»ªï¼Œä¿è¯æ•°æ®è´¨é‡ï¼Œä¼˜åŒ–æ€§èƒ½</li>
                    </ol>
                </div>

                <div class="success-box">
                    <h4>ğŸš€ ä¸‹ä¸€æ­¥</h4>
                    <ul>
                        <li>æ·±å…¥å­¦ä¹  Scrapy å®˜æ–¹æ–‡æ¡£ï¼š<a href="https://docs.scrapy.org" target="_blank">https://docs.scrapy.org</a></li>
                        <li>ç ”ç©¶ HistoryGogo é¡¹ç›®æºç ï¼Œç†è§£å®é™…åº”ç”¨</li>
                        <li>å°è¯•å¼€å‘è‡ªå·±çš„çˆ¬è™«é¡¹ç›®</li>
                        <li>æ¢ç´¢åˆ†å¸ƒå¼çˆ¬è™«æ¡†æ¶ Scrapy-Redis</li>
                        <li>å­¦ä¹ ååçˆ¬æŠ€æœ¯ï¼Œå¦‚ä½¿ç”¨ Splashã€Selenium</li>
                    </ul>
                </div>
            </section>
        </main>

        <footer>
            <p>ğŸ“… æ–‡æ¡£ç”Ÿæˆæ—¶é—´ï¼š2024å¹´12æœˆ</p>
            <p>ğŸ›ï¸ HistoryGogo é¡¹ç›® - è®©å†å²æ•°æ®è§¦æ‰‹å¯åŠ</p>
            <p>ğŸ’» Powered by Scrapy Framework</p>
        </footer>
    </div>
</body>
</html>
