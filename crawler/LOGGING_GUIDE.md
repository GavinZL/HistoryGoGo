# 爬虫日志与数据验证指南

## 📝 日志系统说明

### 一、日志级别

项目使用 Python 标准日志级别：

| 级别 | 说明 | 使用场景 |
|------|------|---------|
| **DEBUG** | 调试信息 | 详细的数据提取过程、字段值 |
| **INFO** | 一般信息 | 成功的操作、进度提示 |
| **WARNING** | 警告信息 | 数据不完整、建议检查 |
| **ERROR** | 错误信息 | 解析失败、保存失败 |
| **CRITICAL** | 严重错误 | 系统级错误 |

### 二、日志输出位置

爬虫运行时，日志会输出到两个地方：

1. **控制台输出**：实时显示爬取进度
2. **日志文件**：`crawler/data/logs/爬虫名_模式.log`

示例：
```bash
# 测试模式日志
crawler/data/logs/baidu_baike_test.log

# 全量模式日志
crawler/data/logs/baidu_baike_full.log
```

### 三、日志内容解读

#### 1. 启动阶段

```
================================================================================
🚀 百度百科爬虫启动
   爬取模式: 测试模式
   爬取数量: 前 3 位皇帝
================================================================================

📋 测试模式：只爬取前3位皇帝

================================================================================
开始生成皇帝爬取请求...
================================================================================

📤 [1/3] 请求皇帝: 朱元璋 - https://baike.baidu.com/item/朱元璋
```

**说明**：
- 🚀 表示爬虫启动
- 📋 表示配置信息
- 📤 表示发送请求

#### 2. 数据解析阶段

```
================================================================================
👑 开始解析皇帝: 朱元璋
   URL: https://baike.baidu.com/item/朱元璋
   状态码: 200
================================================================================
📊 正在提取 朱元璋 的详细信息...
✅ 成功提取皇帝数据: 朱元璋
   - 庙号: 明太祖
   - 年号: 洪武
   - 出生: 1328-10-21
   - 去世: 1398-06-24
   - 简介长度: 523 字符
🔍 发现 8 个相关事件链接
   📤 [1/10] 请求事件: 靖难之役
   📤 [2/10] 请求事件: 洪武之治
🔍 发现 15 个相关人物链接
   📤 [1/20] 请求人物: 朱棣
   📤 [2/20] 请求人物: 刘伯温
✅ 皇帝 朱元璋 解析完成
```

**说明**：
- 👑 表示皇帝数据
- 📊 表示数据提取
- ✅ 表示成功
- 🔍 表示链接发现
- ⚠️ 表示警告
- ❌ 表示错误

#### 3. Pipeline 处理阶段

```
🧽 数据清洗管道已启动
✅ 数据验证管道已启动
💾 SQLite管道已连接: server/database/historygogo.db

🧽 清洗皇帝数据: 朱元璋
✅ 成功爬取事件: 靖难之役
   - 类型: military
   - 时间: 1399-08-06
   - 地点: 北平
   - 关联皇帝: 朱元璋
💾 已保存皇帝: 朱元璋
💾 已保存事件: 靖难之役
```

**说明**：
- 🧽 数据清洗
- ✅ 数据验证
- 💾 数据保存

#### 4. 完成阶段

```
================================================================================
🏁 爬虫运行结束
   关闭原因: finished
================================================================================

📊 爬取统计报告:
================================================================================
成功爬取数据：
  - 皇帝: 3 位
  - 事件: 24 个
  - 人物: 45 位
  - 总计: 72 条

请求统计：
  - 发送请求: 72 次
  - 请求失败: 0 次
  - 解析错误: 0 次

成功率：
  - 请求成功率: 100.00%
  - 数据质量率: 100.00%
================================================================================
✅ 爬取任务成功完成！
================================================================================
```

### 四、调整日志级别

#### 在 settings.py 中调整：

```python
# 设置日志级别
LOG_LEVEL = 'INFO'  # 可选: DEBUG, INFO, WARNING, ERROR, CRITICAL
```

**推荐配置**：
- **开发调试**：`LOG_LEVEL = 'DEBUG'` - 查看详细信息
- **正式运行**：`LOG_LEVEL = 'INFO'` - 只看重要信息
- **生产环境**：`LOG_LEVEL = 'WARNING'` - 只看警告和错误

## 🔍 数据验证指南

### 一、自动验证脚本

运行爬虫后，使用验证脚本检查数据：

```bash
python verify_crawl.py
```

### 二、验证内容

验证脚本会检查：

#### 1. 数据库文件
- ✅ 文件是否存在
- ✅ 文件大小

#### 2. 数据数量
- ✅ 朝代数量
- ✅ 皇帝数量
- ✅ 事件数量
- ✅ 人物数量
- ✅ 作品数量

#### 3. 数据质量
- ✅ 皇帝简介完整性
- ✅ 事件描述完整性
- ✅ 人物简介完整性

#### 4. 数据完整性
- ✅ 事件是否关联皇帝
- ✅ 日期逻辑是否正确
- ✅ 外键关系是否完整

### 三、验证报告示例

```
================================================================================
                          🔍 开始验证爬取数据                          
================================================================================

📋 步骤1: 检查数据库文件
--------------------------------------------------------------------------------
✅ 数据库文件存在: server/database/historygogo.db
   文件大小: 2.34 MB

📋 步骤2: 连接数据库
--------------------------------------------------------------------------------
✅ 数据库连接成功

📋 步骤3: 检查数据数量
--------------------------------------------------------------------------------
✅ 朝代: 1 条
✅ 皇帝: 16 条
✅ 事件: 128 条
✅ 人物: 256 条
✅ 作品: 45 条

📊 数据总量: 446 条

📋 步骤4: 检查数据质量
--------------------------------------------------------------------------------
✅ 皇帝有简介: 16/16 (100.0%)
✅ 事件有描述: 120/128 (93.8%)
✅ 人物有简介: 230/256 (89.8%)

📋 步骤5: 检查数据完整性
--------------------------------------------------------------------------------
✅ 所有事件都有关联皇帝
✅ 日期数据正常

📋 步骤6: 生成验证报告
--------------------------------------------------------------------------------
✅ 未发现任何问题

📄 详细报告已保存: crawler/data/reports/crawl_verification_report.json

================================================================================
                             ✅ 验证结果: 成功                             
================================================================================

📊 数据统计摘要:
  - 皇帝: 16 位
  - 事件: 128 个
  - 人物: 256 位
  - 作品: 45 件
  - 总计: 446 条

🎉 恭喜！数据爬取成功，质量良好！
```

### 四、验证报告文件

详细的 JSON 格式报告保存在：
```
crawler/data/reports/crawl_verification_report.json
```

报告内容包括：
```json
{
  "verification_time": "2024-12-14T15:30:00",
  "database_exists": true,
  "database_size": 2453209,
  "data_counts": {
    "dynasties": 1,
    "emperors": 16,
    "events": 128,
    "persons": 256,
    "works": 45
  },
  "data_quality": {
    "emperors_with_biography": 100.0,
    "events_with_description": 93.8,
    "persons_with_biography": 89.8
  },
  "issues": [],
  "overall_status": "SUCCESS"
}
```

## 🛠️ 常见问题排查

### 1. 爬取数据为 0

**原因**：
- 网络连接问题
- 反爬虫限制
- 页面结构变化

**排查**：
1. 检查网络连接
2. 查看日志中的错误信息
3. 检查是否被 429 错误（请求过快）
4. 尝试增加 `DOWNLOAD_DELAY`

### 2. 部分数据缺失

**原因**：
- 源网站数据不完整
- 页面结构差异
- 解析规则不匹配

**排查**：
1. 查看 WARNING 级别日志
2. 检查验证报告中的警告
3. 手动访问失败的 URL

### 3. 数据验证失败

**原因**：
- 数据格式不符合规则
- 日期逻辑错误
- 必填字段缺失

**排查**：
1. 查看验证报告中的 issues
2. 检查 ERROR 级别日志
3. 查看数据清洗和验证管道的输出

## 📊 性能监控

### 查看实时日志

```bash
# 实时查看日志
tail -f crawler/data/logs/baidu_baike_test.log

# 只看错误
tail -f crawler/data/logs/baidu_baike_test.log | grep "ERROR"

# 只看成功的数据
tail -f crawler/data/logs/baidu_baike_test.log | grep "成功爬取"
```

### 统计数据

```bash
# 统计错误数
grep "ERROR" crawler/data/logs/baidu_baike_test.log | wc -l

# 统计成功爬取的皇帝
grep "成功爬取皇帝" crawler/data/logs/baidu_baike_test.log | wc -l
```

## 🎯 最佳实践

1. **先测试后全量**
   ```bash
   # 先测试 3 位皇帝
   python run_crawler.py --mode test
   
   # 验证数据
   python verify_crawl.py
   
   # 确认无误后全量爬取
   python run_crawler.py --mode full
   ```

2. **定期检查日志**
   - 运行中查看控制台输出
   - 运行后查看日志文件
   - 重点关注 WARNING 和 ERROR

3. **及时验证数据**
   - 每次爬取后立即验证
   - 检查验证报告
   - 发现问题及时调整

4. **保存验证报告**
   - 每次验证的报告都会保存
   - 可用于对比不同批次的数据
   - 追踪数据质量变化

## 📞 问题反馈

如果遇到问题：

1. 查看日志文件找到详细错误信息
2. 运行验证脚本查看验证报告
3. 检查数据库中的实际数据
4. 根据错误信息调整配置或代码
