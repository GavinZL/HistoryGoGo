"""
ç™¾åº¦ç™¾ç§‘çˆ¬è™«
ç”¨äºçˆ¬å–æ˜æœçš‡å¸ã€äº‹ä»¶ã€äººç‰©ä¿¡æ¯
"""

import scrapy
from bs4 import BeautifulSoup
from typing import Dict, Any, Optional, List
import re
from datetime import date

from crawler.models.entities import Emperor, Event, Person, EventType, PersonType
from crawler.utils.date_utils import DateParser, clean_text, generate_id
from crawler.config.ming_data import MING_EMPERORS, MING_DYNASTY


class BaiduBaikeSpider(scrapy.Spider):
    """ç™¾åº¦ç™¾ç§‘çˆ¬è™«"""
    
    name = 'baidu_baike'
    allowed_domains = ['baike.baidu.com']
    
    custom_settings = {
        'DOWNLOAD_DELAY': 5,
        'RANDOMIZE_DOWNLOAD_DELAY': True,
        'CONCURRENT_REQUESTS': 4,
    }
    
    def __init__(self, crawl_mode='test', test_emperor_count=1, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.date_parser = DateParser()
        self.emperor_data = {}  # å­˜å‚¨å·²çˆ¬å–çš„çš‡å¸æ•°æ®
        
        # è·å–çˆ¬å–æ¨¡å¼é…ç½®
        self.crawl_mode = crawl_mode
        self.test_emperor_count = int(test_emperor_count)
        
        # çˆ¬å–ç»Ÿè®¡
        self.stats = {
            'emperors': 0,
            'events': 0,
            'persons': 0,
            'requests_made': 0,
            'requests_failed': 0,
            'parse_errors': 0
        }
        
        self.logger.info("=" * 80)
        self.logger.info(f"ğŸš€ ç™¾åº¦ç™¾ç§‘çˆ¬è™«å¯åŠ¨")
        self.logger.info(f"   çˆ¬å–æ¨¡å¼: {'æµ‹è¯•æ¨¡å¼' if crawl_mode == 'test' else 'å…¨é‡æ¨¡å¼'}")
        if crawl_mode == 'test':
            self.logger.info(f"   çˆ¬å–æ•°é‡: å‰ {test_emperor_count} ä½çš‡å¸")
        self.logger.info("=" * 80)
    
    def start_requests(self):
        """ç”Ÿæˆèµ·å§‹è¯·æ±‚"""
        # æ ¹æ®çˆ¬å–æ¨¡å¼å†³å®šçˆ¬å–å¤šå°‘ä½çš‡å¸
        emperors_to_crawl = MING_EMPERORS
        if self.crawl_mode == 'test':
            emperors_to_crawl = MING_EMPERORS[:self.test_emperor_count]
            self.logger.info(f"ğŸ“‹ æµ‹è¯•æ¨¡å¼ï¼šåªçˆ¬å–å‰{self.test_emperor_count}ä½çš‡å¸")
        else:
            self.logger.info(f"ğŸ“‹ å…¨é‡æ¨¡å¼ï¼šçˆ¬å–æ‰€æœ‰{len(MING_EMPERORS)}ä½çš‡å¸")
        
        self.logger.info(f"\n{'='*80}")
        self.logger.info(f"å¼€å§‹ç”Ÿæˆçš‡å¸çˆ¬å–è¯·æ±‚...")
        self.logger.info(f"{'='*80}\n")
        
        # é¦–å…ˆçˆ¬å–çš‡å¸ä¿¡æ¯
        for idx, emperor_info in enumerate(emperors_to_crawl, 1):
            url = self._build_baidu_url(emperor_info['name'])
            self.logger.info(f"ğŸ“¤ [{idx}/{len(emperors_to_crawl)}] è¯·æ±‚çš‡å¸: {emperor_info['name']} - {url}")
            self.stats['requests_made'] += 1
            yield scrapy.Request(
                url=url,
                callback=self.parse_emperor,
                meta={'emperor_info': emperor_info},
                dont_filter=True,
                errback=self.handle_error
            )
    
    def _build_baidu_url(self, keyword: str) -> str:
        """æ„å»ºç™¾åº¦ç™¾ç§‘URL"""
        return f"https://baike.baidu.com/item/{keyword}"
    
    def parse_emperor(self, response):
        """è§£æçš‡å¸é¡µé¢"""
        emperor_info = response.meta['emperor_info']


        self.logger.info(f"ğŸ“¥ æ¥æ”¶åˆ°çš‡å¸é¡µé¢: {emperor_info}")
        emperor_name = emperor_info['name']
        
        self.logger.info(f"\n{'='*80}")
        self.logger.info(f"ğŸ‘‘ å¼€å§‹è§£æçš‡å¸: {emperor_name}")
        self.logger.info(f"   URL: {response.url}")
        self.logger.info(f"   çŠ¶æ€ç : {response.status}")
        self.logger.info(f"{'='*80}")
        
        try:
            soup = BeautifulSoup(response.text, 'lxml')
            
            # æå–çš‡å¸ä¿¡æ¯
            self.logger.info(f"ğŸ“Š æ­£åœ¨æå– {emperor_name} çš„è¯¦ç»†ä¿¡æ¯...")
            emperor_data = self._extract_emperor_data(soup, emperor_info)
            
            if emperor_data:
                self.stats['emperors'] += 1
                self.logger.info(f"âœ… æˆåŠŸæå–çš‡å¸æ•°æ®: {emperor_data['name']}")
                self.logger.info(f"   - åº™å·: {emperor_data.get('temple_name', 'æœªçŸ¥')}")
                self.logger.info(f"   - å¹´å·: {emperor_data.get('reign_title', 'æœªçŸ¥')}")
                self.logger.info(f"   - å‡ºç”Ÿ: {emperor_data.get('birth_date', 'æœªçŸ¥')}")
                self.logger.info(f"   - å»ä¸–: {emperor_data.get('death_date', 'æœªçŸ¥')}")
                self.logger.info(f"   - ç®€ä»‹é•¿åº¦: {len(emperor_data.get('biography', ''))} å­—ç¬¦")
                
                # å­˜å‚¨çš‡å¸æ•°æ®ä¾›åç»­ä½¿ç”¨
                emperor_id = generate_id("ming_emperor", emperor_data['name'], emperor_info['dynasty_order'])
                self.emperor_data[emperor_id] = emperor_data
                
                # åˆ›å»ºEmperorå®ä½“
                emperor = self._create_emperor_entity(emperor_data, emperor_info)
                yield emperor
                
                # æå–è¯¥çš‡å¸æ—¶æœŸçš„é‡å¤§äº‹ä»¶é“¾æ¥
                event_links = self._extract_event_links(soup)
                self.logger.info(f"ğŸ” å‘ç° {len(event_links)} ä¸ªç›¸å…³äº‹ä»¶é“¾æ¥")
                
                event_count = 0
                for event_name in event_links[:10]:  # é™åˆ¶æ¯ä¸ªçš‡å¸æœ€å¤šçˆ¬å–10ä¸ªäº‹ä»¶
                    url = self._build_baidu_url(event_name)
                    event_count += 1
                    self.stats['requests_made'] += 1
                    self.logger.info(f"   ğŸ“¤ [{event_count}/10] è¯·æ±‚äº‹ä»¶: {event_name}")
                    yield scrapy.Request(
                        url=url,
                        callback=self.parse_event,
                        meta={'emperor_id': emperor_id, 'emperor_name': emperor_data['name']},
                        dont_filter=True,
                        errback=self.handle_error
                    )
                
                # æå–ç›¸å…³äººç‰©é“¾æ¥
                person_links = self._extract_person_links(soup)
                self.logger.info(f"ğŸ” å‘ç° {len(person_links)} ä¸ªç›¸å…³äººç‰©é“¾æ¥")
                
                person_count = 0
                for person_name in person_links[:20]:  # é™åˆ¶æ¯ä¸ªçš‡å¸æœ€å¤šçˆ¬å–20ä¸ªäººç‰©
                    url = self._build_baidu_url(person_name)
                    person_count += 1
                    self.stats['requests_made'] += 1
                    self.logger.info(f"   ğŸ“¤ [{person_count}/20] è¯·æ±‚äººç‰©: {person_name}")
                    yield scrapy.Request(
                        url=url,
                        callback=self.parse_person,
                        meta={'emperor_id': emperor_id},
                        dont_filter=True,
                        errback=self.handle_error
                    )
                
                self.logger.info(f"âœ… çš‡å¸ {emperor_name} è§£æå®Œæˆ\n")
            else:
                self.stats['parse_errors'] += 1
                self.logger.warning(f"âš ï¸ æœªèƒ½æå–åˆ° {emperor_name} çš„æœ‰æ•ˆæ•°æ®\n")
        
        except Exception as e:
            self.stats['parse_errors'] += 1
            self.logger.error(f"âŒ è§£æçš‡å¸é¡µé¢å¤±è´¥: {emperor_name}")
            self.logger.error(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")
            self.logger.error(f"   é”™è¯¯ç±»å‹: {type(e).__name__}\n")
    
    def _extract_emperor_data(self, soup: BeautifulSoup, emperor_info: Dict) -> Optional[Dict[str, Any]]:
        """ä»é¡µé¢ä¸­æå–çš‡å¸æ•°æ®
        
        ç™¾åº¦ç™¾ç§‘å·²å‡çº§ä¸ºåŠ¨æ€åŠ è½½ï¼Œæ•°æ®ä»¥JSONå½¢å¼åµŒå…¥åœ¨scriptæ ‡ç­¾ä¸­
        åŒæ—¶ä¿ç•™ä¼ ç»ŸDOMè§£æä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
        """
        data = {
            'name': emperor_info['name'],
            'temple_name': emperor_info.get('temple_name'),
            'reign_title': emperor_info.get('reign_title'),
            'biography': '',
            'achievements': '',
            'portrait_url': None,
            'infobox_data': {},  # å­˜å‚¨infoboxä¸­çš„æ‰€æœ‰ä¿¡æ¯
            'biography_html': ''  # å­˜å‚¨ç”Ÿå¹³HTMLå†…å®¹
        }
        
        try:
            self.logger.info("  ğŸ“‹ å¼€å§‹æå–çš‡å¸è¯¦ç»†ä¿¡æ¯...")
            
            # æ–¹æ³•1: å°è¯•ä»scriptæ ‡ç­¾ä¸­æå–JSONæ•°æ®ï¼ˆç™¾åº¦ç™¾ç§‘æ–°ç‰ˆï¼‰
            self.logger.info("  ğŸ” å°è¯•ä»JSONæå–æ•°æ®...")
            json_data_extracted = self._extract_from_json(soup, data)
            
            # æ–¹æ³•2: ä¼ ç»ŸDOMè§£æï¼ˆä½œä¸ºå¤‡ç”¨ï¼‰
            if not json_data_extracted:
                self.logger.info("  â†’ JSONæå–æœªæˆåŠŸï¼Œä½¿ç”¨ä¼ ç»ŸDOMè§£ææ–¹å¼")
                self._extract_from_dom(soup, data)
            else:
                self.logger.info("  âœ“ æˆåŠŸä»JSONæå–æ•°æ®")
            
            # æ–¹æ³•3: æå–infoboxä¸­çš„<tr>æ ‡ç­¾ä¿¡æ¯
            self.logger.info("  ğŸ” æå–infoboxè¡¨æ ¼æ•°æ®...")
            self._extract_infobox_table(soup, data)
            
            # è®°å½•æå–ç»“æœ
            self.logger.info(f"  ğŸ“Š æå–ç»“æœç»Ÿè®¡:")
            self.logger.info(f"     - å‡ºç”Ÿæ—¥æœŸ: {'âœ“' if data.get('birth_date') else 'âœ—'}")
            self.logger.info(f"     - å»ä¸–æ—¥æœŸ: {'âœ“' if data.get('death_date') else 'âœ—'}")
            self.logger.info(f"     - ç®€ä»‹é•¿åº¦: {len(data.get('biography', ''))} å­—ç¬¦")
            self.logger.info(f"     - æˆå°±é•¿åº¦: {len(data.get('achievements', ''))} å­—ç¬¦")
            self.logger.info(f"     - ç”»åƒURL: {'âœ“' if data.get('portrait_url') else 'âœ—'}")
            self.logger.info(f"     - Infoboxå­—æ®µ: {len(data.get('infobox_data', {}))} é¡¹")
        
        except Exception as e:
            self.logger.error(f"  âŒ æå–çš‡å¸è¯¦ç»†ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
            import traceback
            self.logger.debug(f"  é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        
        return data
    
    def _extract_from_json(self, soup: BeautifulSoup, data: Dict) -> bool:
        """ä»é¡µé¢ä¸­çš„JSONæ•°æ®æå–ä¿¡æ¯ï¼ˆç™¾åº¦ç™¾ç§‘æ–°ç‰ˆï¼‰"""
        try:
            import json
            
            # æŸ¥æ‰¾åŒ…å«lemmaBasicInfoçš„scriptæ ‡ç­¾
            for script in soup.find_all('script'):
                if not script.string:
                    continue
                    
                script_text = script.string
                
                # æŸ¥æ‰¾åŸºç¡€ä¿¡æ¯JSON
                if '"lemmaBasicInfo"' in script_text or '"basicInfo"' in script_text:
                    # æå–å‡ºç”Ÿæ—¥æœŸ
                    birth_match = re.search(r'"dateOfBirth".*?"text":\[\{"tag":"text","text":"([^"]+)"', script_text)
                    if birth_match:
                        birth_text = birth_match.group(1)
                        data['birth_date'] = self.date_parser.parse_chinese_date(birth_text)
                        self.logger.debug(f"    æå–åˆ°å‡ºç”Ÿæ—¥æœŸ: {birth_text}")
                    
                    # æå–é€ä¸–æ—¥æœŸ
                    death_match = re.search(r'"dateOfDeath".*?"text":\[\{"tag":"text","text":"([^"]+)"', script_text)
                    if death_match:
                        death_text = death_match.group(1)
                        data['death_date'] = self.date_parser.parse_chinese_date(death_text)
                        self.logger.debug(f"    æå–åˆ°é€ä¸–æ—¥æœŸ: {death_text}")
                    
                    # æå–ä¸»è¦æˆå°±
                    achievement_match = re.search(r'"majorAchievement".*?"data":\[(.*?)\]\}', script_text)
                    if achievement_match:
                        achievement_json = achievement_match.group(1)
                        # æå–æ‰€æœ‰æˆå°±æ–‡æœ¬
                        achievement_texts = re.findall(r'"text":"([^"]+)"', achievement_json)
                        if achievement_texts:
                            data['achievements'] = 'ï¼›'.join(achievement_texts)
                            self.logger.debug(f"    æå–åˆ°ä¸»è¦æˆå°±: {len(achievement_texts)}é¡¹")
                
                # æŸ¥æ‰¾æè¿°ä¿¡æ¯
                if '"description"' in script_text:
                    desc_match = re.search(r'"description":"([^"]+)"', script_text)
                    if desc_match:
                        description = desc_match.group(1)
                        # å¦‚æœç®€ä»‹ä¸ºç©ºï¼Œä½¿ç”¨æè¿°
                        if not data['biography']:
                            data['biography'] = description
                            self.logger.debug(f"    æå–åˆ°æè¿°: {len(description)}å­—ç¬¦")
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸæå–åˆ°å…³é”®ä¿¡æ¯
            if data.get('birth_date') or data.get('biography'):
                return True
            
            return False
            
        except Exception as e:
            self.logger.debug(f"    JSONæå–å¤±è´¥: {str(e)}")
            return False
    
    def _extract_from_dom(self, soup: BeautifulSoup, data: Dict) -> None:
        """ä»DOMç»“æ„æå–ä¿¡æ¯ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰"""
        try:
            self.logger.debug("    ğŸ” å¼€å§‹DOMè§£æ...")
            
            # æå–åŸºç¡€ä¿¡æ¯æ¡†
            info_box = soup.select_one('.basic-info')
            if info_box:
                self.logger.debug("    âœ“ æ‰¾åˆ°åŸºç¡€ä¿¡æ¯æ¡†")
                
                # æå–å‡ºç”Ÿæ—¥æœŸ
                birth_elem = info_box.find('dt', string=re.compile('å‡ºç”Ÿæ—¥æœŸ|å‡ºç”Ÿæ—¶é—´'))
                if birth_elem and birth_elem.find_next_sibling('dd'):
                    birth_text = birth_elem.find_next_sibling('dd').get_text(strip=True)
                    data['birth_date'] = self.date_parser.parse_chinese_date(birth_text)
                    self.logger.debug(f"    âœ“ æå–å‡ºç”Ÿæ—¥æœŸ: {birth_text}")
                
                # æå–å»ä¸–æ—¥æœŸ
                death_elem = info_box.find('dt', string=re.compile('é€ä¸–æ—¥æœŸ|é€ä¸–æ—¶é—´'))
                if death_elem and death_elem.find_next_sibling('dd'):
                    death_text = death_elem.find_next_sibling('dd').get_text(strip=True)
                    data['death_date'] = self.date_parser.parse_chinese_date(death_text)
                    self.logger.debug(f"    âœ“ æå–å»ä¸–æ—¥æœŸ: {death_text}")
            else:
                self.logger.debug("    âœ— æœªæ‰¾åˆ°åŸºç¡€ä¿¡æ¯æ¡†")
            
            # æå–ç®€ä»‹ - å°è¯•å¤šç§é€‰æ‹©å™¨
            biography_texts = []
            
            # å°è¯•1: lemma-summary
            summary = soup.select_one('.lemma-summary')
            if summary:
                self.logger.debug("    âœ“ æ‰¾åˆ°lemma-summary")
                paragraphs = summary.find_all('div', class_='para')
                for para in paragraphs[:3]:  # æå–å‰3æ®µ
                    text = clean_text(para.get_text())
                    if text:
                        biography_texts.append(text)
                self.logger.debug(f"    âœ“ æå–äº† {len(biography_texts)} æ®µç®€ä»‹")
            
            # å°è¯•2: æŸ¥æ‰¾æ‰€æœ‰æ®µè½
            if not biography_texts:
                self.logger.debug("    â†’ å°è¯•æŸ¥æ‰¾æ‰€æœ‰æ®µè½...")
                all_paras = soup.find_all('div', class_='para')
                for para in all_paras[:5]:  # æå–å‰5æ®µ
                    text = clean_text(para.get_text())
                    if text and len(text) > 50:  # è¿‡æ»¤å¤ªçŸ­çš„æ®µè½
                        biography_texts.append(text)
                self.logger.debug(f"    âœ“ ä»æ‰€æœ‰æ®µè½ä¸­æå–äº† {len(biography_texts)} æ®µ")
            
            if biography_texts:
                data['biography'] = '\n'.join(biography_texts)
                self.logger.debug(f"    âœ“ ç®€ä»‹æ€»é•¿åº¦: {len(data['biography'])} å­—ç¬¦")
            
            # æå–ä¸»è¦æˆå°± - å°è¯•å¤šç§æ–¹å¼
            # æ–¹å¼1: æŸ¥æ‰¾data-title
            achievement_section = soup.find('div', {'data-title': 'ä¸»è¦æˆå°±'})
            if achievement_section:
                data['achievements'] = clean_text(achievement_section.get_text())
                self.logger.debug(f"    âœ“ ä»data-titleæå–æˆå°±: {len(data['achievements'])} å­—ç¬¦")
            
            # æ–¹å¼2: æŸ¥æ‰¾åŒ…å«"ä¸»è¦æˆå°±"çš„æ ‡é¢˜
            if not data['achievements']:
                for heading in soup.find_all(['h2', 'h3']):
                    if 'ä¸»è¦æˆå°±' in heading.get_text():
                        # æå–è¯¥æ ‡é¢˜åçš„å†…å®¹
                        next_elem = heading.find_next_sibling()
                        if next_elem:
                            data['achievements'] = clean_text(next_elem.get_text())
                            self.logger.debug(f"    âœ“ ä»æ ‡é¢˜æå–æˆå°±: {len(data['achievements'])} å­—ç¬¦")
                            break
            
            # æå–ç”»åƒURL
            portrait = soup.select_one('.summary-pic img')
            if portrait and portrait.get('src'):
                data['portrait_url'] = portrait['src']
                self.logger.debug(f"    âœ“ æå–ç”»åƒURL: {data['portrait_url'][:60]}...")
        
        except Exception as e:
            self.logger.error(f"    âŒ DOMæå–å‡ºé”™: {str(e)}")
            import traceback
            self.logger.debug(f"    é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
    
    def _extract_infobox_table(self, soup: BeautifulSoup, data: Dict) -> None:
        """
        ä»infoboxè¡¨æ ¼ä¸­æå–<tr>æ ‡ç­¾ä¿¡æ¯
        ç™¾åº¦ç™¾ç§‘çš„åŸºç¡€ä¿¡æ¯é€šå¸¸åœ¨.basic-infoè¡¨æ ¼ä¸­ï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ª<tr>æ ‡ç­¾
        """
        try:
            self.logger.debug("    ğŸ” å¼€å§‹æå–infoboxè¡¨æ ¼...")
            
            # æŸ¥æ‰¾åŸºç¡€ä¿¡æ¯è¡¨æ ¼
            # ç™¾åº¦ç™¾ç§‘å¯èƒ½ä½¿ç”¨: .basic-info, .basicInfo-block, table.infoboxç­‰
            info_tables = []
            
            # å°è¯•å¤šç§é€‰æ‹©å™¨
            selectors = [
                '.basic-info',
                '.basicInfo-block table',
                'table.infobox',
                '.lemma-table',
            ]
            
            for selector in selectors:
                table = soup.select_one(selector)
                if table:
                    info_tables.append(table)
                    self.logger.debug(f"    âœ“ æ‰¾åˆ°è¡¨æ ¼: {selector}")
                    break
            
            if not info_tables:
                self.logger.debug("    âš  æœªæ‰¾åˆ°infoboxè¡¨æ ¼")
                return
            
            # éå†è¡¨æ ¼è¡Œ
            for table in info_tables:
                rows = table.find_all('tr')
                self.logger.debug(f"    ğŸ“Š æ‰¾åˆ° {len(rows)} è¡Œæ•°æ®")
                
                row_count = 0
                for row in rows:
                    try:
                        # æå–è¡¨å¤´å’Œè¡¨æ•°æ®
                        th = row.find(['th', 'dt'])
                        td = row.find(['td', 'dd'])
                        
                        if not th or not td:
                            continue
                        
                        row_count += 1
                        field_name = clean_text(th.get_text())
                        field_value = clean_text(td.get_text())
                        
                        if not field_name or not field_value:
                            continue
                        
                        # å­˜å‚¨åˆ°infobox_data
                        data['infobox_data'][field_name] = field_value
                        self.logger.debug(f"    ğŸ“Œ [{row_count}] {field_name}: {field_value[:50]}...")
                        
                        # æ ¹æ®å­—æ®µåæå–ç‰¹å®šä¿¡æ¯
                        field_name_lower = field_name.lower()
                        
                        # å‡ºç”Ÿæ—¥æœŸ
                        if any(keyword in field_name for keyword in ['å‡ºç”Ÿæ—¥æœŸ', 'å‡ºç”Ÿæ—¶é—´', 'å‡ºç”Ÿ', 'ç”Ÿäº']):
                            if not data.get('birth_date'):
                                parsed_date = self.date_parser.parse_chinese_date(field_value)
                                if parsed_date:
                                    data['birth_date'] = parsed_date
                                    self.logger.debug(f"    âœ“ ä»è¡¨æ ¼æå–å‡ºç”Ÿæ—¥æœŸ: {field_value} -> {parsed_date}")
                        
                        # å»ä¸–æ—¥æœŸ
                        elif any(keyword in field_name for keyword in ['é€ä¸–æ—¥æœŸ', 'é€ä¸–æ—¶é—´', 'é€ä¸–', 'å’äº', 'å»ä¸–']):
                            if not data.get('death_date'):
                                parsed_date = self.date_parser.parse_chinese_date(field_value)
                                if parsed_date:
                                    data['death_date'] = parsed_date
                                    self.logger.debug(f"    âœ“ ä»è¡¨æ ¼æå–å»ä¸–æ—¥æœŸ: {field_value} -> {parsed_date}")
                        
                        # åœ¨ä½æ—¶é—´
                        elif any(keyword in field_name for keyword in ['åœ¨ä½æ—¶é—´', 'åœ¨ä½', 'ç»Ÿæ²»æ—¶é—´']):
                            data['infobox_data']['reign_period'] = field_value
                            self.logger.debug(f"    âœ“ ä»è¡¨æ ¼æå–åœ¨ä½æ—¶é—´: {field_value}")
                        
                        # åº™å·
                        elif any(keyword in field_name for keyword in ['åº™å·']):
                            if not data.get('temple_name'):
                                data['temple_name'] = field_value
                                data['infobox_data']['temple_name'] = field_value
                                self.logger.debug(f"    âœ“ ä»è¡¨æ ¼æå–åº™å·: {field_value}")
                        
                        # è°¥å·
                        elif any(keyword in field_name for keyword in ['è°¥å·']):
                            data['infobox_data']['posthumous_name'] = field_value
                            self.logger.debug(f"    âœ“ ä»è¡¨æ ¼æå–è°¥å·: {field_value}")
                        
                        # å¹´å·
                        elif any(keyword in field_name for keyword in ['å¹´å·']):
                            if not data.get('reign_title'):
                                data['reign_title'] = field_value
                                data['infobox_data']['era_name'] = field_value
                                self.logger.debug(f"    âœ“ ä»è¡¨æ ¼æå–å¹´å·: {field_value}")
                        
                        # é™µå¯
                        elif any(keyword in field_name for keyword in ['é™µå¢“', 'é™µå¯']):
                            data['infobox_data']['tomb'] = field_value
                            self.logger.debug(f"    âœ“ ä»è¡¨æ ¼æå–é™µå¯: {field_value}")
                        
                        # çš‡å
                        elif any(keyword in field_name for keyword in ['çš‡å']):
                            data['infobox_data']['empress'] = field_value
                            self.logger.debug(f"    âœ“ ä»è¡¨æ ¼æå–çš‡å: {field_value}")
                        
                    except Exception as row_error:
                        self.logger.debug(f"    âš  å¤„ç†è¡Œæ—¶å‡ºé”™: {str(row_error)}")
                        continue
                
                # å°è¯•æå–å›¾ç‰‡URL
                if not data.get('portrait_url'):
                    img = table.find('img')
                    if img and img.get('src'):
                        # å¤„ç†ç›¸å¯¹è·¯å¾„
                        img_url = img['src']
                        if img_url.startswith('//'):
                            img_url = 'https:' + img_url
                        elif img_url.startswith('/'):
                            img_url = 'https://baike.baidu.com' + img_url
                        
                        data['portrait_url'] = img_url
                        data['infobox_data']['portrait_url'] = img_url
                        self.logger.debug(f"    âœ“ ä»è¡¨æ ¼æå–å›¾ç‰‡URL: {img_url[:60]}...")
            
            self.logger.debug(f"    âœ“ Infoboxè¡¨æ ¼æå–å®Œæˆï¼Œå…± {len(data['infobox_data'])} ä¸ªå­—æ®µ")
        
        except Exception as e:
            self.logger.error(f"    âŒ æå–infoboxè¡¨æ ¼æ—¶å‡ºé”™: {str(e)}")
            import traceback
            self.logger.debug(f"    é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
    
    def _create_emperor_entity(self, emperor_data: Dict, emperor_info: Dict) -> Emperor:
        """åˆ›å»ºçš‡å¸å®ä½“"""
        emperor_id = generate_id("ming_emperor", emperor_data['name'], emperor_info['dynasty_order'])
        
        self.logger.info(f"  ğŸ”¨ åˆ›å»ºEmperorå®ä½“: {emperor_id}")
        
        # è§£æåœ¨ä½æ—¶é—´
        reign_years = emperor_info.get('reign_years', '')
        reign_start, reign_end = self._parse_reign_years(reign_years)
        
        emperor = Emperor(
            emperor_id=emperor_id,
            dynasty_id=MING_DYNASTY['dynasty_id'],
            name=emperor_data['name'],
            temple_name=emperor_data.get('temple_name'),
            reign_title=emperor_data.get('reign_title'),
            birth_date=emperor_data.get('birth_date'),
            death_date=emperor_data.get('death_date'),
            reign_start=reign_start,
            reign_end=reign_end,
            dynasty_order=emperor_info['dynasty_order'],
            biography=emperor_data.get('biography'),
            achievements=emperor_data.get('achievements'),
            portrait_url=emperor_data.get('portrait_url'),
            html_content=emperor_data.get('biography_html', ''),
            source_url=f"https://baike.baidu.com/item/{emperor_data['name']}",
            data_source='baidu'
        )
        
        self.logger.info(f"  âœ“ Emperorå®ä½“åˆ›å»ºæˆåŠŸ")
        return emperor
    
    def _parse_reign_years(self, reign_years_str: str) -> tuple:
        """è§£æåœ¨ä½å¹´ä»½"""
        try:
            # æ ¼å¼: "1368-1398" æˆ– "1435-1449, 1457-1464"
            years = reign_years_str.split(',')[0].strip()
            start_year, end_year = years.split('-')
            return (date(int(start_year), 1, 1), date(int(end_year), 12, 31))
        except Exception:
            return (date(1368, 1, 1), None)
    
    def _extract_event_links(self, soup: BeautifulSoup) -> List[str]:
        """æå–äº‹ä»¶ç›¸å…³é“¾æ¥"""
        events = []
        
        # ä»æ­£æ–‡ä¸­æå–äº‹ä»¶é“¾æ¥
        content = soup.select_one('.main-content')
        if content:
            # æŸ¥æ‰¾åŒ…å«ç‰¹å®šå…³é”®è¯çš„é“¾æ¥
            event_keywords = ['ä¹‹å½¹', 'ä¹‹æˆ˜', 'ä¹‹å˜', 'æ”¿å˜', 'èµ·ä¹‰', 'æ”¹é©', 'è¿åŠ¨', 'ä¸‹è¥¿æ´‹', 'æ¡ˆ']
            links = content.find_all('a', href=True)
            
            for link in links:
                link_text = link.get_text(strip=True)
                if any(keyword in link_text for keyword in event_keywords):
                    if link_text and len(link_text) < 20:  # è¿‡æ»¤è¿‡é•¿çš„æ–‡æœ¬
                        events.append(link_text)
        
        return list(set(events))[:15]  # å»é‡å¹¶é™åˆ¶æ•°é‡
    
    def _extract_person_links(self, soup: BeautifulSoup) -> List[str]:
        """æå–äººç‰©ç›¸å…³é“¾æ¥"""
        persons = []
        
        # ä»æ­£æ–‡ä¸­æå–äººç‰©é“¾æ¥
        content = soup.select_one('.main-content')
        if content:
            # æŸ¥æ‰¾äººåé“¾æ¥ï¼ˆé€šå¸¸æ˜¯2-4ä¸ªå­—ï¼‰
            links = content.find_all('a', href=True)
            
            for link in links:
                link_text = link.get_text(strip=True)
                # ç®€å•çš„äººååˆ¤æ–­ï¼š2-4ä¸ªä¸­æ–‡å­—ç¬¦
                if link_text and 2 <= len(link_text) <= 4 and all('\u4e00' <= c <= '\u9fff' for c in link_text):
                    persons.append(link_text)
        
        return list(set(persons))[:25]  # å»é‡å¹¶é™åˆ¶æ•°é‡
    
    def parse_event(self, response):
        """è§£æäº‹ä»¶é¡µé¢"""
        emperor_id = response.meta.get('emperor_id')
        emperor_name = response.meta.get('emperor_name')
        
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"ğŸ“– å¼€å§‹è§£æäº‹ä»¶é¡µé¢")
        self.logger.info(f"   URL: {response.url}")
        self.logger.info(f"   å…³è”çš‡å¸: {emperor_name} ({emperor_id})")
        self.logger.info(f"{'='*60}")
        
        try:
            soup = BeautifulSoup(response.text, 'lxml')
            
            # æå–äº‹ä»¶æ•°æ®
            self.logger.info("ğŸ” å¼€å§‹æå–äº‹ä»¶æ•°æ®...")
            event_data = self._extract_event_data(soup, emperor_id)
            
            if event_data:
                self.stats['events'] += 1
                self.logger.info(f"âœ… æˆåŠŸçˆ¬å–äº‹ä»¶: {event_data.title}")
                self.logger.info(f"   - ç±»å‹: {event_data.event_type.value}")
                self.logger.info(f"   - æ—¶é—´: {event_data.start_date}")
                self.logger.info(f"   - åœ°ç‚¹: {event_data.location or 'æœªçŸ¥'}")
                self.logger.info(f"   - å…³è”çš‡å¸: {emperor_name}")
                yield event_data
            else:
                self.stats['parse_errors'] += 1
                self.logger.warning(f"âš ï¸ äº‹ä»¶æ•°æ®æå–å¤±è´¥: {response.url}")
        
        except Exception as e:
            self.stats['parse_errors'] += 1
            self.logger.error(f"âŒ è§£æäº‹ä»¶é¡µé¢å¤±è´¥: {response.url}")
            self.logger.error(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")
            import traceback
            self.logger.debug(f"   é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
    
    def _extract_event_data(self, soup: BeautifulSoup, emperor_id: str) -> Optional[Dict]:
        """ä»é¡µé¢ä¸­æå–äº‹ä»¶æ•°æ®"""
        try:
            self.logger.debug("  ğŸ” å¼€å§‹æå–äº‹ä»¶è¯¦ç»†ä¿¡æ¯...")
            
            # è·å–æ ‡é¢˜
            title_elem = soup.select_one('.lemmaWgt-lemmaTitle-title h1')
            if not title_elem:
                self.logger.warning("  âœ— æœªæ‰¾åˆ°äº‹ä»¶æ ‡é¢˜")
                return None
            
            title = clean_text(title_elem.get_text())
            self.logger.debug(f"  âœ“ æå–æ ‡é¢˜: {title}")
            
            data = {
                'title': title,
                'event_type': self._determine_event_type(title, soup),
                'start_date': None,
                'end_date': None,
                'location': None,
                'description': '',
                'significance': '',
                'emperor_id': emperor_id
            }
            
            self.logger.debug(f"  âœ“ åˆ¤æ–­äº‹ä»¶ç±»å‹: {data['event_type'].value}")
            
            # æå–åŸºç¡€ä¿¡æ¯æ¡†
            info_box = soup.select_one('.basic-info')
            if info_box:
                self.logger.debug("  âœ“ æ‰¾åˆ°åŸºç¡€ä¿¡æ¯æ¡†")
                
                # æå–æ—¶é—´
                time_elem = info_box.find('dt', string=re.compile('æ—¶é—´|å‘ç”Ÿæ—¶é—´|å¹´ä»£'))
                if time_elem and time_elem.find_next_sibling('dd'):
                    time_text = time_elem.find_next_sibling('dd').get_text(strip=True)
                    data['start_date'] = self.date_parser.parse_chinese_date(time_text)
                    self.logger.debug(f"  âœ“ æå–æ—¶é—´: {time_text} -> {data['start_date']}")
                
                # æå–åœ°ç‚¹
                location_elem = info_box.find('dt', string=re.compile('åœ°ç‚¹|å‘ç”Ÿåœ°ç‚¹'))
                if location_elem and location_elem.find_next_sibling('dd'):
                    data['location'] = clean_text(location_elem.find_next_sibling('dd').get_text())
                    self.logger.debug(f"  âœ“ æå–åœ°ç‚¹: {data['location']}")
            else:
                self.logger.debug("  âœ— æœªæ‰¾åˆ°åŸºç¡€ä¿¡æ¯æ¡†")
            
            # æå–æè¿°
            summary = soup.select_one('.lemma-summary')
            if summary:
                paragraphs = summary.find_all('div', class_='para')
                if paragraphs:
                    data['description'] = clean_text(paragraphs[0].get_text())
                    self.logger.debug(f"  âœ“ æå–æè¿°: {len(data['description'])} å­—ç¬¦")
            
            # åˆ›å»ºEventå®ä½“
            event_id = generate_id("ming_event", title)
            self.logger.debug(f"  âœ“ ç”Ÿæˆevent_id: {event_id}")
            
            event = Event(
                event_id=event_id,
                dynasty_id=MING_DYNASTY['dynasty_id'],
                emperor_id=emperor_id,
                title=data['title'],
                event_type=data['event_type'],
                start_date=data['start_date'] or date(1368, 1, 1),
                end_date=data.get('end_date'),
                location=data.get('location'),
                description=data.get('description'),
                significance=data.get('significance'),
                source_url=f"https://baike.baidu.com/item/{title}",
                data_source='baidu'
            )
            
            self.logger.debug(f"  âœ“ Eventå®ä½“åˆ›å»ºæˆåŠŸ")
            return event
        
        except Exception as e:
            self.logger.error(f"  âŒ æå–äº‹ä»¶æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            import traceback
            self.logger.debug(f"  é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return None
    
    def _determine_event_type(self, title: str, soup: BeautifulSoup) -> EventType:
        """æ ¹æ®æ ‡é¢˜å’Œå†…å®¹åˆ¤æ–­äº‹ä»¶ç±»å‹"""
        if any(keyword in title for keyword in ['ä¹‹æˆ˜', 'ä¹‹å½¹', 'æˆ˜äº‰', 'æˆ˜å½¹']):
            return EventType.MILITARY
        elif any(keyword in title for keyword in ['æ”¿å˜', 'æ”¹é©', 'åºŸé™¤', 'è®¾ç«‹']):
            return EventType.POLITICAL
        elif any(keyword in title for keyword in ['æ–‡åŒ–', 'è¿åŠ¨', 'è‘—ä½œ']):
            return EventType.CULTURAL
        elif any(keyword in title for keyword in ['è´¸æ˜“', 'ä¸‹è¥¿æ´‹', 'é€šå•†']):
            return EventType.DIPLOMATIC
        else:
            return EventType.POLITICAL  # é»˜è®¤ä¸ºæ”¿æ²»äº‹ä»¶
    
    def parse_person(self, response):
        """è§£æäººç‰©é¡µé¢"""
        emperor_id = response.meta.get('emperor_id')
        
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"ğŸ‘¤ å¼€å§‹è§£æäººç‰©é¡µé¢")
        self.logger.info(f"   URL: {response.url}")
        self.logger.info(f"   å…³è”çš‡å¸ ID: {emperor_id}")
        self.logger.info(f"{'='*60}")
        
        try:
            soup = BeautifulSoup(response.text, 'lxml')
            
            # æå–äººç‰©æ•°æ®
            self.logger.info("ğŸ” å¼€å§‹æå–äººç‰©æ•°æ®...")
            person_data = self._extract_person_data(soup, emperor_id)
            
            if person_data:
                self.stats['persons'] += 1
                self.logger.info(f"âœ… æˆåŠŸçˆ¬å–äººç‰©: {person_data.name}")
                self.logger.info(f"   - ç±»å‹: {person_data.person_type.value}")
                self.logger.info(f"   - èŒä½: {person_data.position or 'æœªçŸ¥'}")
                self.logger.info(f"   - ç”Ÿå’: {person_data.birth_date or '?'} - {person_data.death_date or '?'}")
                yield person_data
            else:
                self.stats['parse_errors'] += 1
                self.logger.warning(f"âš ï¸ äººç‰©æ•°æ®æå–å¤±è´¥: {response.url}")
        
        except Exception as e:
            self.stats['parse_errors'] += 1
            self.logger.error(f"âŒ è§£æäººç‰©é¡µé¢å¤±è´¥: {response.url}")
            self.logger.error(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")
            import traceback
            self.logger.debug(f"   é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
    
    def _extract_person_data(self, soup: BeautifulSoup, emperor_id: str) -> Optional[Person]:
        """ä»é¡µé¢ä¸­æå–äººç‰©æ•°æ®"""
        try:
            self.logger.debug("  ğŸ” å¼€å§‹æå–äººç‰©è¯¦ç»†ä¿¡æ¯...")
            
            # è·å–äººå
            title_elem = soup.select_one('.lemmaWgt-lemmaTitle-title h1')
            if not title_elem:
                self.logger.warning("  âœ— æœªæ‰¾åˆ°äººç‰©åç§°")
                return None
            
            name = clean_text(title_elem.get_text())
            self.logger.debug(f"  âœ“ æå–äººå: {name}")
            
            # æå–åŸºç¡€ä¿¡æ¯
            alias_list = []
            birth_date = None
            death_date = None
            position = None
            person_type = PersonType.OTHER
            
            info_box = soup.select_one('.basic-info')
            if info_box:
                self.logger.debug("  âœ“ æ‰¾åˆ°åŸºç¡€ä¿¡æ¯æ¡†")
                
                # æå–åˆ«åã€å­—å·
                alias_elem = info_box.find('dt', string=re.compile('åˆ«å|å­—å·|æœ¬å'))
                if alias_elem and alias_elem.find_next_sibling('dd'):
                    alias_text = alias_elem.find_next_sibling('dd').get_text(strip=True)
                    alias_list = [a.strip() for a in re.split('[ï¼Œã€]', alias_text) if a.strip()]
                    self.logger.debug(f"  âœ“ æå–åˆ«å: {len(alias_list)} ä¸ª")
                
                # æå–å‡ºç”Ÿæ—¥æœŸ
                birth_elem = info_box.find('dt', string=re.compile('å‡ºç”Ÿæ—¥æœŸ|å‡ºç”Ÿæ—¶é—´'))
                if birth_elem and birth_elem.find_next_sibling('dd'):
                    birth_text = birth_elem.find_next_sibling('dd').get_text(strip=True)
                    birth_date = self.date_parser.parse_chinese_date(birth_text)
                    self.logger.debug(f"  âœ“ æå–å‡ºç”Ÿæ—¥æœŸ: {birth_text} -> {birth_date}")
                
                # æå–å»ä¸–æ—¥æœŸ
                death_elem = info_box.find('dt', string=re.compile('é€ä¸–æ—¥æœŸ|é€ä¸–æ—¶é—´'))
                if death_elem and death_elem.find_next_sibling('dd'):
                    death_text = death_elem.find_next_sibling('dd').get_text(strip=True)
                    death_date = self.date_parser.parse_chinese_date(death_text)
                    self.logger.debug(f"  âœ“ æå–å»ä¸–æ—¥æœŸ: {death_text} -> {death_date}")
                
                # æå–èŒä½
                position_elem = info_box.find('dt', string=re.compile('èŒä¸š|ä¸»è¦æˆå°±|èŒåŠ¡'))
                if position_elem and position_elem.find_next_sibling('dd'):
                    position = clean_text(position_elem.find_next_sibling('dd').get_text())
                    # æ ¹æ®èŒä½åˆ¤æ–­äººç‰©ç±»å‹
                    person_type = self._determine_person_type(position, soup)
                    self.logger.debug(f"  âœ“ æå–èŒä½: {position} -> ç±»å‹: {person_type.value}")
            else:
                self.logger.debug("  âœ— æœªæ‰¾åˆ°åŸºç¡€ä¿¡æ¯æ¡†")
            
            # æå–ç”Ÿå¹³
            biography = ''
            summary = soup.select_one('.lemma-summary')
            if summary:
                paragraphs = summary.find_all('div', class_='para')
                if paragraphs:
                    biography = clean_text(paragraphs[0].get_text())
                    self.logger.debug(f"  âœ“ æå–ç”Ÿå¹³: {len(biography)} å­—ç¬¦")
            
            # åˆ›å»ºPersonå®ä½“
            person_id = generate_id("ming_person", name)
            self.logger.debug(f"  âœ“ ç”Ÿæˆperson_id: {person_id}")
            
            person = Person(
                person_id=person_id,
                dynasty_id=MING_DYNASTY['dynasty_id'],
                name=name,
                person_type=person_type,
                alias=alias_list,
                birth_date=birth_date,
                death_date=death_date,
                position=position,
                biography=biography,
                related_emperors=[emperor_id] if emperor_id else [],
                source_url=f"https://baike.baidu.com/item/{name}",
                data_source='baidu'
            )
            
            self.logger.debug(f"  âœ“ Personå®ä½“åˆ›å»ºæˆåŠŸ")
            return person
        
        except Exception as e:
            self.logger.error(f"  âŒ æå–äººç‰©æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            import traceback
            self.logger.debug(f"  é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return None
    
    def _determine_person_type(self, position: str, soup: BeautifulSoup) -> PersonType:
        """æ ¹æ®èŒä½å’Œå†…å®¹åˆ¤æ–­äººç‰©ç±»å‹"""
        if not position:
            return PersonType.OTHER
        
        position_lower = position.lower()
        
        if any(keyword in position for keyword in ['å°†å†›', 'å°†é¢†', 'æ­¦', 'å†›']):
            return PersonType.GENERAL
        elif any(keyword in position for keyword in ['è¯—äºº', 'æ–‡å­¦', 'ä½œå®¶', 'è¯äºº']):
            return PersonType.WRITER
        elif any(keyword in position for keyword in ['ç”»å®¶', 'ä¹¦æ³•', 'è‰ºæœ¯']):
            return PersonType.ARTIST
        elif any(keyword in position for keyword in ['å¤§è‡£', 'å°šä¹¦', 'ä¾éƒ', 'å­¦å£«', 'é˜']):
            return PersonType.OFFICIAL
        elif any(keyword in position for keyword in ['çš‡å­', 'çš‡å', 'å¦ƒ']):
            return PersonType.ROYAL
        elif any(keyword in position for keyword in ['åƒ§', 'é“']):
            return PersonType.MONK
        elif any(keyword in position for keyword in ['æ€æƒ³', 'å“²å­¦']):
            return PersonType.THINKER
        elif any(keyword in position for keyword in ['ç§‘å­¦', 'å¤©æ–‡', 'æ•°å­¦', 'åŒ»']):
            return PersonType.SCIENTIST
        else:
            return PersonType.OTHER
    
    def handle_error(self, failure):
        """å¤„ç†è¯·æ±‚é”™è¯¯"""
        self.stats['requests_failed'] += 1
        self.logger.error(f"âŒ è¯·æ±‚å¤±è´¥: {failure.request.url}")
        self.logger.error(f"   é”™è¯¯ç±»å‹: {failure.type.__name__}")
        self.logger.error(f"   é”™è¯¯ä¿¡æ¯: {failure.getErrorMessage()}")
    
    def closed(self, reason):
        """çˆ¬è™«å…³é—­æ—¶è°ƒç”¨"""
        self.logger.info("\n" + "="*80)
        self.logger.info("ğŸ çˆ¬è™«è¿è¡Œç»“æŸ")
        self.logger.info(f"   å…³é—­åŸå› : {reason}")
        self.logger.info("="*80)
        
        self.logger.info("\nğŸ“Š çˆ¬å–ç»Ÿè®¡æŠ¥å‘Š:")
        self.logger.info(f"{'='*80}")
        self.logger.info(f"æˆåŠŸçˆ¬å–æ•°æ®ï¼š")
        self.logger.info(f"  - çš‡å¸: {self.stats['emperors']} ä½")
        self.logger.info(f"  - äº‹ä»¶: {self.stats['events']} ä¸ª")
        self.logger.info(f"  - äººç‰©: {self.stats['persons']} ä½")
        self.logger.info(f"  - æ€»è®¡: {self.stats['emperors'] + self.stats['events'] + self.stats['persons']} æ¡")
        self.logger.info(f"")
        self.logger.info(f"è¯·æ±‚ç»Ÿè®¡ï¼š")
        self.logger.info(f"  - å‘é€è¯·æ±‚: {self.stats['requests_made']} æ¬¡")
        self.logger.info(f"  - è¯·æ±‚å¤±è´¥: {self.stats['requests_failed']} æ¬¡")
        self.logger.info(f"  - è§£æé”™è¯¯: {self.stats['parse_errors']} æ¬¡")
        self.logger.info(f"")
        
        # è®¡ç®—æˆåŠŸç‡
        total_requests = self.stats['requests_made']
        if total_requests > 0:
            success_rate = ((total_requests - self.stats['requests_failed']) / total_requests) * 100
            self.logger.info(f"æˆåŠŸç‡ï¼š")
            self.logger.info(f"  - è¯·æ±‚æˆåŠŸç‡: {success_rate:.2f}%")
            
            total_items = self.stats['emperors'] + self.stats['events'] + self.stats['persons']
            if total_items > 0:
                data_quality = ((total_items - self.stats['parse_errors']) / total_items) * 100
                self.logger.info(f"  - æ•°æ®è´¨é‡ç‡: {data_quality:.2f}%")
        
        self.logger.info(f"{'='*80}")
        
        # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
        if self.stats['emperors'] > 0:
            self.logger.info("âœ… çˆ¬å–ä»»åŠ¡æˆåŠŸå®Œæˆï¼")
        else:
            self.logger.error("âŒ è­¦å‘Šï¼šæœªèƒ½çˆ¬å–åˆ°ä»»ä½•çš‡å¸æ•°æ®ï¼")
        
        self.logger.info("="*80 + "\n")
