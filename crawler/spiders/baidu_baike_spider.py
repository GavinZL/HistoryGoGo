"""
ç™¾åº¦ç™¾ç§‘çˆ¬è™«
ç”¨äºçˆ¬å–æ˜æœçš‡å¸ã€äº‹ä»¶ã€äººç‰©ä¿¡æ¯
"""

import scrapy
from bs4 import BeautifulSoup
from typing import Dict, Any, Optional, List
import re
from datetime import date

from crawler.models.entities import Emperor, Event, Person, EventType, PersonType
from crawler.utils.date_utils import DateParser, clean_text, generate_id
from crawler.config.ming_data import MING_EMPERORS, MING_DYNASTY


class BaiduBaikeSpider(scrapy.Spider):
    """ç™¾åº¦ç™¾ç§‘çˆ¬è™«"""
    
    name = 'baidu_baike'
    allowed_domains = ['baike.baidu.com']
    
    custom_settings = {
        'DOWNLOAD_DELAY': 5,
        'RANDOMIZE_DOWNLOAD_DELAY': True,
        'CONCURRENT_REQUESTS': 4,
    }
    
    def __init__(self, crawl_mode='test', test_emperor_count=3, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.date_parser = DateParser()
        self.emperor_data = {}  # å­˜å‚¨å·²çˆ¬å–çš„çš‡å¸æ•°æ®
        
        # è·å–çˆ¬å–æ¨¡å¼é…ç½®
        self.crawl_mode = crawl_mode
        self.test_emperor_count = int(test_emperor_count)
        
        # çˆ¬å–ç»Ÿè®¡
        self.stats = {
            'emperors': 0,
            'events': 0,
            'persons': 0,
            'requests_made': 0,
            'requests_failed': 0,
            'parse_errors': 0
        }
        
        self.logger.info("=" * 80)
        self.logger.info(f"ğŸš€ ç™¾åº¦ç™¾ç§‘çˆ¬è™«å¯åŠ¨")
        self.logger.info(f"   çˆ¬å–æ¨¡å¼: {'æµ‹è¯•æ¨¡å¼' if crawl_mode == 'test' else 'å…¨é‡æ¨¡å¼'}")
        if crawl_mode == 'test':
            self.logger.info(f"   çˆ¬å–æ•°é‡: å‰ {test_emperor_count} ä½çš‡å¸")
        self.logger.info("=" * 80)
    
    def start_requests(self):
        """ç”Ÿæˆèµ·å§‹è¯·æ±‚"""
        # æ ¹æ®çˆ¬å–æ¨¡å¼å†³å®šçˆ¬å–å¤šå°‘ä½çš‡å¸
        emperors_to_crawl = MING_EMPERORS
        if self.crawl_mode == 'test':
            emperors_to_crawl = MING_EMPERORS[:self.test_emperor_count]
            self.logger.info(f"ğŸ“‹ æµ‹è¯•æ¨¡å¼ï¼šåªçˆ¬å–å‰{self.test_emperor_count}ä½çš‡å¸")
        else:
            self.logger.info(f"ğŸ“‹ å…¨é‡æ¨¡å¼ï¼šçˆ¬å–æ‰€æœ‰{len(MING_EMPERORS)}ä½çš‡å¸")
        
        self.logger.info(f"\n{'='*80}")
        self.logger.info(f"å¼€å§‹ç”Ÿæˆçš‡å¸çˆ¬å–è¯·æ±‚...")
        self.logger.info(f"{'='*80}\n")
        
        # é¦–å…ˆçˆ¬å–çš‡å¸ä¿¡æ¯
        for idx, emperor_info in enumerate(emperors_to_crawl, 1):
            url = self._build_baidu_url(emperor_info['name'])
            self.logger.info(f"ğŸ“¤ [{idx}/{len(emperors_to_crawl)}] è¯·æ±‚çš‡å¸: {emperor_info['name']} - {url}")
            self.stats['requests_made'] += 1
            yield scrapy.Request(
                url=url,
                callback=self.parse_emperor,
                meta={'emperor_info': emperor_info},
                dont_filter=True,
                errback=self.handle_error
            )
    
    def _build_baidu_url(self, keyword: str) -> str:
        """æ„å»ºç™¾åº¦ç™¾ç§‘URL"""
        return f"https://baike.baidu.com/item/{keyword}"
    
    def parse_emperor(self, response):
        """è§£æçš‡å¸é¡µé¢"""
        emperor_info = response.meta['emperor_info']
        emperor_name = emperor_info['name']
        
        self.logger.info(f"\n{'='*80}")
        self.logger.info(f"ğŸ‘‘ å¼€å§‹è§£æçš‡å¸: {emperor_name}")
        self.logger.info(f"   URL: {response.url}")
        self.logger.info(f"   çŠ¶æ€ç : {response.status}")
        self.logger.info(f"{'='*80}")
        
        try:
            soup = BeautifulSoup(response.text, 'lxml')
            
            # æå–çš‡å¸ä¿¡æ¯
            self.logger.info(f"ğŸ“Š æ­£åœ¨æå– {emperor_name} çš„è¯¦ç»†ä¿¡æ¯...")
            emperor_data = self._extract_emperor_data(soup, emperor_info)
            
            if emperor_data:
                self.stats['emperors'] += 1
                self.logger.info(f"âœ… æˆåŠŸæå–çš‡å¸æ•°æ®: {emperor_data['name']}")
                self.logger.info(f"   - åº™å·: {emperor_data.get('temple_name', 'æœªçŸ¥')}")
                self.logger.info(f"   - å¹´å·: {emperor_data.get('reign_title', 'æœªçŸ¥')}")
                self.logger.info(f"   - å‡ºç”Ÿ: {emperor_data.get('birth_date', 'æœªçŸ¥')}")
                self.logger.info(f"   - å»ä¸–: {emperor_data.get('death_date', 'æœªçŸ¥')}")
                self.logger.info(f"   - ç®€ä»‹é•¿åº¦: {len(emperor_data.get('biography', ''))} å­—ç¬¦")
                
                # å­˜å‚¨çš‡å¸æ•°æ®ä¾›åç»­ä½¿ç”¨
                emperor_id = generate_id("ming_emperor", emperor_data['name'], emperor_info['dynasty_order'])
                self.emperor_data[emperor_id] = emperor_data
                
                # åˆ›å»ºEmperorå®ä½“
                emperor = self._create_emperor_entity(emperor_data, emperor_info)
                yield emperor
                
                # æå–è¯¥çš‡å¸æ—¶æœŸçš„é‡å¤§äº‹ä»¶é“¾æ¥
                event_links = self._extract_event_links(soup)
                self.logger.info(f"ğŸ” å‘ç° {len(event_links)} ä¸ªç›¸å…³äº‹ä»¶é“¾æ¥")
                
                event_count = 0
                for event_name in event_links[:10]:  # é™åˆ¶æ¯ä¸ªçš‡å¸æœ€å¤šçˆ¬å–10ä¸ªäº‹ä»¶
                    url = self._build_baidu_url(event_name)
                    event_count += 1
                    self.stats['requests_made'] += 1
                    self.logger.info(f"   ğŸ“¤ [{event_count}/10] è¯·æ±‚äº‹ä»¶: {event_name}")
                    yield scrapy.Request(
                        url=url,
                        callback=self.parse_event,
                        meta={'emperor_id': emperor_id, 'emperor_name': emperor_data['name']},
                        dont_filter=True,
                        errback=self.handle_error
                    )
                
                # æå–ç›¸å…³äººç‰©é“¾æ¥
                person_links = self._extract_person_links(soup)
                self.logger.info(f"ğŸ” å‘ç° {len(person_links)} ä¸ªç›¸å…³äººç‰©é“¾æ¥")
                
                person_count = 0
                for person_name in person_links[:20]:  # é™åˆ¶æ¯ä¸ªçš‡å¸æœ€å¤šçˆ¬å–20ä¸ªäººç‰©
                    url = self._build_baidu_url(person_name)
                    person_count += 1
                    self.stats['requests_made'] += 1
                    self.logger.info(f"   ğŸ“¤ [{person_count}/20] è¯·æ±‚äººç‰©: {person_name}")
                    yield scrapy.Request(
                        url=url,
                        callback=self.parse_person,
                        meta={'emperor_id': emperor_id},
                        dont_filter=True,
                        errback=self.handle_error
                    )
                
                self.logger.info(f"âœ… çš‡å¸ {emperor_name} è§£æå®Œæˆ\n")
            else:
                self.stats['parse_errors'] += 1
                self.logger.warning(f"âš ï¸ æœªèƒ½æå–åˆ° {emperor_name} çš„æœ‰æ•ˆæ•°æ®\n")
        
        except Exception as e:
            self.stats['parse_errors'] += 1
            self.logger.error(f"âŒ è§£æçš‡å¸é¡µé¢å¤±è´¥: {emperor_name}")
            self.logger.error(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")
            self.logger.error(f"   é”™è¯¯ç±»å‹: {type(e).__name__}\n")
    
    def _extract_emperor_data(self, soup: BeautifulSoup, emperor_info: Dict) -> Optional[Dict[str, Any]]:
        """ä»é¡µé¢ä¸­æå–çš‡å¸æ•°æ®"""
        data = {
            'name': emperor_info['name'],
            'temple_name': emperor_info.get('temple_name'),
            'reign_title': emperor_info.get('reign_title'),
            'biography': '',
            'achievements': '',
            'portrait_url': None
        }
        
        try:
            # æå–åŸºç¡€ä¿¡æ¯æ¡†
            info_box = soup.select_one('.basic-info')
            if info_box:
                # æå–å‡ºç”Ÿæ—¥æœŸ
                birth_elem = info_box.find('dt', text=re.compile('å‡ºç”Ÿæ—¥æœŸ|å‡ºç”Ÿæ—¶é—´'))
                if birth_elem and birth_elem.find_next_sibling('dd'):
                    birth_text = birth_elem.find_next_sibling('dd').get_text(strip=True)
                    data['birth_date'] = self.date_parser.parse_chinese_date(birth_text)
                
                # æå–å»ä¸–æ—¥æœŸ
                death_elem = info_box.find('dt', text=re.compile('é€ä¸–æ—¥æœŸ|é€ä¸–æ—¶é—´'))
                if death_elem and death_elem.find_next_sibling('dd'):
                    death_text = death_elem.find_next_sibling('dd').get_text(strip=True)
                    data['death_date'] = self.date_parser.parse_chinese_date(death_text)
            
            # æå–ç®€ä»‹ï¼ˆç¬¬ä¸€æ®µï¼‰
            summary = soup.select_one('.lemma-summary')
            if summary:
                paragraphs = summary.find_all('div', class_='para')
                if paragraphs:
                    data['biography'] = clean_text(paragraphs[0].get_text())
            
            # æå–ä¸»è¦æˆå°±
            achievement_section = soup.find('div', {'data-title': 'ä¸»è¦æˆå°±'})
            if achievement_section:
                data['achievements'] = clean_text(achievement_section.get_text())
            
            # æå–ç”»åƒURL
            portrait = soup.select_one('.summary-pic img')
            if portrait and portrait.get('src'):
                data['portrait_url'] = portrait['src']
        
        except Exception as e:
            self.logger.warning(f"æå–çš‡å¸è¯¦ç»†ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        
        return data
    
    def _create_emperor_entity(self, emperor_data: Dict, emperor_info: Dict) -> Emperor:
        """åˆ›å»ºçš‡å¸å®ä½“"""
        emperor_id = generate_id("ming_emperor", emperor_data['name'], emperor_info['dynasty_order'])
        
        # è§£æåœ¨ä½æ—¶é—´
        reign_years = emperor_info.get('reign_years', '')
        reign_start, reign_end = self._parse_reign_years(reign_years)
        
        return Emperor(
            emperor_id=emperor_id,
            dynasty_id=MING_DYNASTY['dynasty_id'],
            name=emperor_data['name'],
            temple_name=emperor_data.get('temple_name'),
            reign_title=emperor_data.get('reign_title'),
            birth_date=emperor_data.get('birth_date'),
            death_date=emperor_data.get('death_date'),
            reign_start=reign_start,
            reign_end=reign_end,
            dynasty_order=emperor_info['dynasty_order'],
            biography=emperor_data.get('biography'),
            achievements=emperor_data.get('achievements'),
            portrait_url=emperor_data.get('portrait_url'),
            data_source='baidu'
        )
    
    def _parse_reign_years(self, reign_years_str: str) -> tuple:
        """è§£æåœ¨ä½å¹´ä»½"""
        try:
            # æ ¼å¼: "1368-1398" æˆ– "1435-1449, 1457-1464"
            years = reign_years_str.split(',')[0].strip()
            start_year, end_year = years.split('-')
            return (date(int(start_year), 1, 1), date(int(end_year), 12, 31))
        except Exception:
            return (date(1368, 1, 1), None)
    
    def _extract_event_links(self, soup: BeautifulSoup) -> List[str]:
        """æå–äº‹ä»¶ç›¸å…³é“¾æ¥"""
        events = []
        
        # ä»æ­£æ–‡ä¸­æå–äº‹ä»¶é“¾æ¥
        content = soup.select_one('.main-content')
        if content:
            # æŸ¥æ‰¾åŒ…å«ç‰¹å®šå…³é”®è¯çš„é“¾æ¥
            event_keywords = ['ä¹‹å½¹', 'ä¹‹æˆ˜', 'ä¹‹å˜', 'æ”¿å˜', 'èµ·ä¹‰', 'æ”¹é©', 'è¿åŠ¨', 'ä¸‹è¥¿æ´‹', 'æ¡ˆ']
            links = content.find_all('a', href=True)
            
            for link in links:
                link_text = link.get_text(strip=True)
                if any(keyword in link_text for keyword in event_keywords):
                    if link_text and len(link_text) < 20:  # è¿‡æ»¤è¿‡é•¿çš„æ–‡æœ¬
                        events.append(link_text)
        
        return list(set(events))[:15]  # å»é‡å¹¶é™åˆ¶æ•°é‡
    
    def _extract_person_links(self, soup: BeautifulSoup) -> List[str]:
        """æå–äººç‰©ç›¸å…³é“¾æ¥"""
        persons = []
        
        # ä»æ­£æ–‡ä¸­æå–äººç‰©é“¾æ¥
        content = soup.select_one('.main-content')
        if content:
            # æŸ¥æ‰¾äººåé“¾æ¥ï¼ˆé€šå¸¸æ˜¯2-4ä¸ªå­—ï¼‰
            links = content.find_all('a', href=True)
            
            for link in links:
                link_text = link.get_text(strip=True)
                # ç®€å•çš„äººååˆ¤æ–­ï¼š2-4ä¸ªä¸­æ–‡å­—ç¬¦
                if link_text and 2 <= len(link_text) <= 4 and all('\u4e00' <= c <= '\u9fff' for c in link_text):
                    persons.append(link_text)
        
        return list(set(persons))[:25]  # å»é‡å¹¶é™åˆ¶æ•°é‡
    
    def parse_event(self, response):
        """è§£æäº‹ä»¶é¡µé¢"""
        emperor_id = response.meta.get('emperor_id')
        emperor_name = response.meta.get('emperor_name')
        
        try:
            soup = BeautifulSoup(response.text, 'lxml')
            
            # æå–äº‹ä»¶æ•°æ®
            event_data = self._extract_event_data(soup, emperor_id)
            
            if event_data:
                self.stats['events'] += 1
                self.logger.info(f"âœ… æˆåŠŸçˆ¬å–äº‹ä»¶: {event_data.title}")
                self.logger.info(f"   - ç±»å‹: {event_data.event_type.value}")
                self.logger.info(f"   - æ—¶é—´: {event_data.start_date}")
                self.logger.info(f"   - åœ°ç‚¹: {event_data.location or 'æœªçŸ¥'}")
                self.logger.info(f"   - å…³è”çš‡å¸: {emperor_name}")
                yield event_data
            else:
                self.stats['parse_errors'] += 1
                self.logger.warning(f"âš ï¸ äº‹ä»¶æ•°æ®æå–å¤±è´¥: {response.url}")
        
        except Exception as e:
            self.stats['parse_errors'] += 1
            self.logger.error(f"âŒ è§£æäº‹ä»¶é¡µé¢å¤±è´¥: {response.url}")
            self.logger.error(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")
    
    def _extract_event_data(self, soup: BeautifulSoup, emperor_id: str) -> Optional[Dict]:
        """ä»é¡µé¢ä¸­æå–äº‹ä»¶æ•°æ®"""
        try:
            # è·å–æ ‡é¢˜
            title_elem = soup.select_one('.lemmaWgt-lemmaTitle-title h1')
            if not title_elem:
                return None
            
            title = clean_text(title_elem.get_text())
            
            data = {
                'title': title,
                'event_type': self._determine_event_type(title, soup),
                'start_date': None,
                'end_date': None,
                'location': None,
                'description': '',
                'significance': '',
                'emperor_id': emperor_id
            }
            
            # æå–åŸºç¡€ä¿¡æ¯æ¡†
            info_box = soup.select_one('.basic-info')
            if info_box:
                # æå–æ—¶é—´
                time_elem = info_box.find('dt', text=re.compile('æ—¶é—´|å‘ç”Ÿæ—¶é—´|å¹´ä»£'))
                if time_elem and time_elem.find_next_sibling('dd'):
                    time_text = time_elem.find_next_sibling('dd').get_text(strip=True)
                    data['start_date'] = self.date_parser.parse_chinese_date(time_text)
                
                # æå–åœ°ç‚¹
                location_elem = info_box.find('dt', text=re.compile('åœ°ç‚¹|å‘ç”Ÿåœ°ç‚¹'))
                if location_elem and location_elem.find_next_sibling('dd'):
                    data['location'] = clean_text(location_elem.find_next_sibling('dd').get_text())
            
            # æå–æè¿°
            summary = soup.select_one('.lemma-summary')
            if summary:
                paragraphs = summary.find_all('div', class_='para')
                if paragraphs:
                    data['description'] = clean_text(paragraphs[0].get_text())
            
            # åˆ›å»ºEventå®ä½“
            event_id = generate_id("ming_event", title)
            
            event = Event(
                event_id=event_id,
                dynasty_id=MING_DYNASTY['dynasty_id'],
                emperor_id=emperor_id,
                title=data['title'],
                event_type=data['event_type'],
                start_date=data['start_date'] or date(1368, 1, 1),
                end_date=data.get('end_date'),
                location=data.get('location'),
                description=data.get('description'),
                significance=data.get('significance'),
                data_source='baidu'
            )
            
            return event
        
        except Exception as e:
            self.logger.warning(f"æå–äº‹ä»¶æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return None
    
    def _determine_event_type(self, title: str, soup: BeautifulSoup) -> EventType:
        """æ ¹æ®æ ‡é¢˜å’Œå†…å®¹åˆ¤æ–­äº‹ä»¶ç±»å‹"""
        if any(keyword in title for keyword in ['ä¹‹æˆ˜', 'ä¹‹å½¹', 'æˆ˜äº‰', 'æˆ˜å½¹']):
            return EventType.MILITARY
        elif any(keyword in title for keyword in ['æ”¿å˜', 'æ”¹é©', 'åºŸé™¤', 'è®¾ç«‹']):
            return EventType.POLITICAL
        elif any(keyword in title for keyword in ['æ–‡åŒ–', 'è¿åŠ¨', 'è‘—ä½œ']):
            return EventType.CULTURAL
        elif any(keyword in title for keyword in ['è´¸æ˜“', 'ä¸‹è¥¿æ´‹', 'é€šå•†']):
            return EventType.DIPLOMATIC
        else:
            return EventType.POLITICAL  # é»˜è®¤ä¸ºæ”¿æ²»äº‹ä»¶
    
    def parse_person(self, response):
        """è§£æäººç‰©é¡µé¢"""
        emperor_id = response.meta.get('emperor_id')
        
        try:
            soup = BeautifulSoup(response.text, 'lxml')
            
            # æå–äººç‰©æ•°æ®
            person_data = self._extract_person_data(soup, emperor_id)
            
            if person_data:
                self.stats['persons'] += 1
                self.logger.info(f"âœ… æˆåŠŸçˆ¬å–äººç‰©: {person_data.name}")
                self.logger.info(f"   - ç±»å‹: {person_data.person_type.value}")
                self.logger.info(f"   - èŒä½: {person_data.position or 'æœªçŸ¥'}")
                self.logger.info(f"   - ç”Ÿå’: {person_data.birth_date or '?'} - {person_data.death_date or '?'}")
                yield person_data
            else:
                self.stats['parse_errors'] += 1
                self.logger.warning(f"âš ï¸ äººç‰©æ•°æ®æå–å¤±è´¥: {response.url}")
        
        except Exception as e:
            self.stats['parse_errors'] += 1
            self.logger.error(f"âŒ è§£æäººç‰©é¡µé¢å¤±è´¥: {response.url}")
            self.logger.error(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")
    
    def _extract_person_data(self, soup: BeautifulSoup, emperor_id: str) -> Optional[Person]:
        """ä»é¡µé¢ä¸­æå–äººç‰©æ•°æ®"""
        try:
            # è·å–äººå
            title_elem = soup.select_one('.lemmaWgt-lemmaTitle-title h1')
            if not title_elem:
                return None
            
            name = clean_text(title_elem.get_text())
            
            # æå–åŸºç¡€ä¿¡æ¯
            alias_list = []
            birth_date = None
            death_date = None
            position = None
            person_type = PersonType.OTHER
            
            info_box = soup.select_one('.basic-info')
            if info_box:
                # æå–åˆ«åã€å­—å·
                alias_elem = info_box.find('dt', text=re.compile('åˆ«å|å­—å·|æœ¬å'))
                if alias_elem and alias_elem.find_next_sibling('dd'):
                    alias_text = alias_elem.find_next_sibling('dd').get_text(strip=True)
                    alias_list = [a.strip() for a in re.split('[ï¼Œã€]', alias_text) if a.strip()]
                
                # æå–å‡ºç”Ÿæ—¥æœŸ
                birth_elem = info_box.find('dt', text=re.compile('å‡ºç”Ÿæ—¥æœŸ|å‡ºç”Ÿæ—¶é—´'))
                if birth_elem and birth_elem.find_next_sibling('dd'):
                    birth_text = birth_elem.find_next_sibling('dd').get_text(strip=True)
                    birth_date = self.date_parser.parse_chinese_date(birth_text)
                
                # æå–å»ä¸–æ—¥æœŸ
                death_elem = info_box.find('dt', text=re.compile('é€ä¸–æ—¥æœŸ|é€ä¸–æ—¶é—´'))
                if death_elem and death_elem.find_next_sibling('dd'):
                    death_text = death_elem.find_next_sibling('dd').get_text(strip=True)
                    death_date = self.date_parser.parse_chinese_date(death_text)
                
                # æå–èŒä½
                position_elem = info_box.find('dt', text=re.compile('èŒä¸š|ä¸»è¦æˆå°±|èŒåŠ¡'))
                if position_elem and position_elem.find_next_sibling('dd'):
                    position = clean_text(position_elem.find_next_sibling('dd').get_text())
                    # æ ¹æ®èŒä½åˆ¤æ–­äººç‰©ç±»å‹
                    person_type = self._determine_person_type(position, soup)
            
            # æå–ç”Ÿå¹³
            biography = ''
            summary = soup.select_one('.lemma-summary')
            if summary:
                paragraphs = summary.find_all('div', class_='para')
                if paragraphs:
                    biography = clean_text(paragraphs[0].get_text())
            
            # åˆ›å»ºPersonå®ä½“
            person_id = generate_id("ming_person", name)
            
            person = Person(
                person_id=person_id,
                dynasty_id=MING_DYNASTY['dynasty_id'],
                name=name,
                person_type=person_type,
                alias=alias_list,
                birth_date=birth_date,
                death_date=death_date,
                position=position,
                biography=biography,
                related_emperors=[emperor_id] if emperor_id else [],
                data_source='baidu'
            )
            
            return person
        
        except Exception as e:
            self.logger.warning(f"æå–äººç‰©æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return None
    
    def _determine_person_type(self, position: str, soup: BeautifulSoup) -> PersonType:
        """æ ¹æ®èŒä½å’Œå†…å®¹åˆ¤æ–­äººç‰©ç±»å‹"""
        if not position:
            return PersonType.OTHER
        
        position_lower = position.lower()
        
        if any(keyword in position for keyword in ['å°†å†›', 'å°†é¢†', 'æ­¦', 'å†›']):
            return PersonType.GENERAL
        elif any(keyword in position for keyword in ['è¯—äºº', 'æ–‡å­¦', 'ä½œå®¶', 'è¯äºº']):
            return PersonType.WRITER
        elif any(keyword in position for keyword in ['ç”»å®¶', 'ä¹¦æ³•', 'è‰ºæœ¯']):
            return PersonType.ARTIST
        elif any(keyword in position for keyword in ['å¤§è‡£', 'å°šä¹¦', 'ä¾éƒ', 'å­¦å£«', 'é˜']):
            return PersonType.OFFICIAL
        elif any(keyword in position for keyword in ['çš‡å­', 'çš‡å', 'å¦ƒ']):
            return PersonType.ROYAL
        elif any(keyword in position for keyword in ['åƒ§', 'é“']):
            return PersonType.MONK
        elif any(keyword in position for keyword in ['æ€æƒ³', 'å“²å­¦']):
            return PersonType.THINKER
        elif any(keyword in position for keyword in ['ç§‘å­¦', 'å¤©æ–‡', 'æ•°å­¦', 'åŒ»']):
            return PersonType.SCIENTIST
        else:
            return PersonType.OTHER
    
    def handle_error(self, failure):
        """å¤„ç†è¯·æ±‚é”™è¯¯"""
        self.stats['requests_failed'] += 1
        self.logger.error(f"âŒ è¯·æ±‚å¤±è´¥: {failure.request.url}")
        self.logger.error(f"   é”™è¯¯ç±»å‹: {failure.type.__name__}")
        self.logger.error(f"   é”™è¯¯ä¿¡æ¯: {failure.getErrorMessage()}")
    
    def closed(self, reason):
        """çˆ¬è™«å…³é—­æ—¶è°ƒç”¨"""
        self.logger.info("\n" + "="*80)
        self.logger.info("ğŸ çˆ¬è™«è¿è¡Œç»“æŸ")
        self.logger.info(f"   å…³é—­åŸå› : {reason}")
        self.logger.info("="*80)
        
        self.logger.info("\nğŸ“Š çˆ¬å–ç»Ÿè®¡æŠ¥å‘Š:")
        self.logger.info(f"{'='*80}")
        self.logger.info(f"æˆåŠŸçˆ¬å–æ•°æ®ï¼š")
        self.logger.info(f"  - çš‡å¸: {self.stats['emperors']} ä½")
        self.logger.info(f"  - äº‹ä»¶: {self.stats['events']} ä¸ª")
        self.logger.info(f"  - äººç‰©: {self.stats['persons']} ä½")
        self.logger.info(f"  - æ€»è®¡: {self.stats['emperors'] + self.stats['events'] + self.stats['persons']} æ¡")
        self.logger.info(f"")
        self.logger.info(f"è¯·æ±‚ç»Ÿè®¡ï¼š")
        self.logger.info(f"  - å‘é€è¯·æ±‚: {self.stats['requests_made']} æ¬¡")
        self.logger.info(f"  - è¯·æ±‚å¤±è´¥: {self.stats['requests_failed']} æ¬¡")
        self.logger.info(f"  - è§£æé”™è¯¯: {self.stats['parse_errors']} æ¬¡")
        self.logger.info(f"")
        
        # è®¡ç®—æˆåŠŸç‡
        total_requests = self.stats['requests_made']
        if total_requests > 0:
            success_rate = ((total_requests - self.stats['requests_failed']) / total_requests) * 100
            self.logger.info(f"æˆåŠŸç‡ï¼š")
            self.logger.info(f"  - è¯·æ±‚æˆåŠŸç‡: {success_rate:.2f}%")
            
            total_items = self.stats['emperors'] + self.stats['events'] + self.stats['persons']
            if total_items > 0:
                data_quality = ((total_items - self.stats['parse_errors']) / total_items) * 100
                self.logger.info(f"  - æ•°æ®è´¨é‡ç‡: {data_quality:.2f}%")
        
        self.logger.info(f"{'='*80}")
        
        # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
        if self.stats['emperors'] > 0:
            self.logger.info("âœ… çˆ¬å–ä»»åŠ¡æˆåŠŸå®Œæˆï¼")
        else:
            self.logger.error("âŒ è­¦å‘Šï¼šæœªèƒ½çˆ¬å–åˆ°ä»»ä½•çš‡å¸æ•°æ®ï¼")
        
        self.logger.info("="*80 + "\n")
