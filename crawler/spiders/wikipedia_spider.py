"""
ç»´åŸºç™¾ç§‘çˆ¬è™«
ç”¨äºçˆ¬å–æ˜æœçš‡å¸ã€äº‹ä»¶ã€äººç‰©ä¿¡æ¯ï¼ˆä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼‰
"""

import scrapy
from bs4 import BeautifulSoup
from typing import Dict, Any, Optional, List
import re
from datetime import date

from crawler.models.entities import Emperor, Event, Person, EventType, PersonType
from crawler.utils.date_utils import DateParser, clean_text, generate_id
from crawler.config.ming_data import MING_EMPERORS, MING_DYNASTY


class WikipediaSpider(scrapy.Spider):
    """ç»´åŸºç™¾ç§‘çˆ¬è™«"""
    
    name = 'wikipedia'
    allowed_domains = ['zh.wikipedia.org']
    
    custom_settings = {
        'DOWNLOAD_DELAY': 5,
        'RANDOMIZE_DOWNLOAD_DELAY': True,
        'CONCURRENT_REQUESTS': 4,
    }
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.date_parser = DateParser()
    
    def start_requests(self):
        """ç”Ÿæˆèµ·å§‹è¯·æ±‚"""
        # ä» settings ä¸­è·å–çˆ¬å–æ¨¡å¼é…ç½®
        crawl_mode = self.settings.get('CRAWL_MODE', 'test')
        test_emperor_count = self.settings.get('TEST_EMPEROR_COUNT', 3)
        
        # æ ¹æ®çˆ¬å–æ¨¡å¼å†³å®šçˆ¬å–å¤šå°‘ä½çš‡å¸
        emperors_to_crawl = MING_EMPERORS
        if crawl_mode == 'test':
            emperors_to_crawl = MING_EMPERORS[:test_emperor_count]
            self.logger.info(f"[Wiki] æµ‹è¯•æ¨¡å¼ï¼šåªçˆ¬å–å‰{test_emperor_count}ä½çš‡å¸")
        else:
            self.logger.info(f"[Wiki] å…¨é‡æ¨¡å¼ï¼šçˆ¬å–æ‰€æœ‰{len(MING_EMPERORS)}ä½çš‡å¸")
        
        # çˆ¬å–çš‡å¸ä¿¡æ¯
        for emperor_info in emperors_to_crawl:
            url = self._build_wiki_url(emperor_info['name'])
            yield scrapy.Request(
                url=url,
                callback=self.parse_emperor,
                meta={'emperor_info': emperor_info},
                dont_filter=True
            )
    
    def _build_wiki_url(self, keyword: str) -> str:
        """æ„å»ºç»´åŸºç™¾ç§‘URL"""
        return f"https://zh.wikipedia.org/wiki/{keyword}"
    
    def parse_emperor(self, response):
        """è§£æçš‡å¸é¡µé¢"""
        emperor_info = response.meta['emperor_info']
        
        self.logger.info(f"\n{'='*80}")
        self.logger.info(f"ğŸ‘‘ [ç»´åŸº] å¼€å§‹è§£æçš‡å¸: {emperor_info['name']}")
        self.logger.info(f"   URL: {response.url}")
        self.logger.info(f"   æœä»£é¡ºåº: {emperor_info.get('dynasty_order')}")
        self.logger.info(f"{'='*80}")
        
        try:
            soup = BeautifulSoup(response.text, 'lxml')
            
            # æå–çš‡å¸ä¿¡æ¯
            self.logger.info(f"ğŸ“‹ å¼€å§‹æå– {emperor_info['name']} çš„è¯¦ç»†ä¿¡æ¯...")
            emperor_data = self._extract_emperor_data(soup, emperor_info)
            
            if emperor_data:
                self.logger.info(f"âœ… æˆåŠŸçˆ±å–çš‡å¸: {emperor_data['name']}")
                self.logger.info(f"   - åº™å·: {emperor_data.get('temple_name', 'æœªçŸ¥')}")
                self.logger.info(f"   - å¹´å·: {emperor_data.get('reign_title', 'æœªçŸ¥')}")
                self.logger.info(f"   - å‡ºç”Ÿ: {emperor_data.get('birth_date', 'æœªçŸ¥')}")
                self.logger.info(f"   - å»ä¸–: {emperor_data.get('death_date', 'æœªçŸ¥')}")
                self.logger.info(f"   - ç®€ä»‹é•¿åº¦: {len(emperor_data.get('biography', ''))} å­—ç¬¦")
                self.logger.info(f"   - Infoboxå­—æ®µ: {len(emperor_data.get('infobox_data', {}))} é¡¹")
                
                # åˆ›å»ºEmperorå®ä½“
                emperor = self._create_emperor_entity(emperor_data, emperor_info)
                yield emperor
                
        except Exception as e:
            self.logger.error(f"âŒ [ç»´åŸº] è§£æçš‡å¸é¡µé¢å¤±è´¥: {emperor_info['name']}, é”™è¯¯: {str(e)}")
            import traceback
            self.logger.debug(f"   é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
    
    def _extract_emperor_data(self, soup: BeautifulSoup, emperor_info: Dict) -> Optional[Dict[str, Any]]:
        """ä»ç»´åŸºç™¾ç§‘é¡µé¢ä¸­æå–çš‡å¸æ•°æ®"""
        data = {
            'name': emperor_info['name'],
            'temple_name': emperor_info.get('temple_name'),
            'reign_title': emperor_info.get('reign_title'),
            'biography': '',
            'achievements': '',
            'portrait_url': None,
            'infobox_data': {},  # å­˜å‚¨infoboxä¸­çš„æ‰€æœ‰ä¿¡æ¯
            'biography_html': ''  # å­˜å‚¨ç”Ÿå¹³HTMLå†…å®¹
        }
        
        try:
            self.logger.info("  ğŸ” å¼€å§‹æå–infoboxè¡¨æ ¼æ•°æ®...")
            
            # æå–Infoboxä¿¡æ¯æ¡†ï¼ˆåŸºäº<tr>æ ‡ç­¾è§£æï¼‰
            infobox = soup.find('table', class_='infobox')
            if infobox:
                self.logger.info("  âœ“ æ‰¾åˆ°infoboxè¡¨æ ¼")
                self._extract_infobox_table(infobox, data)
            else:
                self.logger.warning("  âš  æœªæ‰¾åˆ°infoboxè¡¨æ ¼")
            
            # æå–é¦–æ®µç®€ä»‹
            content = soup.find('div', class_='mw-parser-output')
            if content:
                # è·å–ç¬¬ä¸€ä¸ªæ®µè½ï¼ˆé€šå¸¸æ˜¯ç®€ä»‹ï¼‰
                first_para = content.find('p', recursive=False)
                if first_para:
                    # ç§»é™¤å¼•ç”¨æ ‡è®°
                    for sup in first_para.find_all('sup'):
                        sup.decompose()
                    data['biography'] = clean_text(first_para.get_text())
                
                # æå–ç”Ÿå¹³å†…å®¹ï¼ˆä»mw-heading mw-heading2å¼€å§‹åˆ°ä¸‹ä¸€ä¸ªmw-heading2ï¼‰
                data['biography_html'] = self._extract_biography_section(soup)
            
            # å°è¯•æå–"ä¸»è¦æˆå°±"ç›¸å…³å†…å®¹
            # åœ¨ç»´åŸºç™¾ç§‘ä¸­å¯èƒ½åœ¨ä¸åŒçš„ç« èŠ‚
            for heading in soup.find_all(['h2', 'h3']):
                heading_text = heading.get_text()
                if any(keyword in heading_text for keyword in ['æˆå°±', 'è´¡çŒ®', 'æ”¿ç»©']):
                    next_elem = heading.find_next_sibling()
                    if next_elem and next_elem.name in ['p', 'ul']:
                        data['achievements'] = clean_text(next_elem.get_text())
                        break
            
            # è®°å½•æå–ç»“æœ
            self.logger.info(f"  ğŸ“Š æå–ç»“æœç»Ÿè®¡:")
            self.logger.info(f"     - å‡ºç”Ÿæ—¥æœŸ: {'âœ“' if data.get('birth_date') else 'âœ—'}")
            self.logger.info(f"     - å»ä¸–æ—¥æœŸ: {'âœ“' if data.get('death_date') else 'âœ—'}")
            self.logger.info(f"     - ç®€ä»‹é•¿åº¦: {len(data.get('biography', ''))} å­—ç¬¦")
            self.logger.info(f"     - æˆå°±é•¿åº¦: {len(data.get('achievements', ''))} å­—ç¬¦")
            self.logger.info(f"     - ç”»åƒURL: {'âœ“' if data.get('portrait_url') else 'âœ—'}")
            self.logger.info(f"     - Infoboxå­—æ®µ: {len(data.get('infobox_data', {}))} é¡¹")
        
        except Exception as e:
            self.logger.error(f"  âŒ æå–çš‡å¸è¯¦ç»†ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
            import traceback
            self.logger.debug(f"  é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        
        return data
    
    def _extract_infobox_table(self, infobox, data: Dict) -> None:
        """
        ä»infoboxè¡¨æ ¼ä¸­æå–<tr>æ ‡ç­¾ä¿¡æ¯
        ç»´åŸºç™¾ç§‘çš„åŸºç¡€ä¿¡æ¯åœ¨infoboxè¡¨æ ¼ä¸­ï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ª<tr>æ ‡ç­¾
        """
        try:
            self.logger.debug("    ğŸ” å¼€å§‹æå–infoboxè¡¨æ ¼è¡Œ...")
            
            # éå†è¡¨æ ¼è¡Œ
            rows = infobox.find_all('tr')
            self.logger.debug(f"    ğŸ“Š æ‰¾åˆ° {len(rows)} è¡Œæ•°æ®")
            
            row_count = 0
            for row in rows:
                try:
                    # æå–è¡¨å¤´å’Œè¡¨æ•°æ®
                    th = row.find('th')
                    td = row.find('td')
                    
                    if not th or not td:
                        continue
                    
                    row_count += 1
                    field_name = clean_text(th.get_text())
                    field_value = clean_text(td.get_text())
                    
                    if not field_name or not field_value:
                        continue
                    
                    # å­˜å‚¨åˆ°infobox_data
                    data['infobox_data'][field_name] = field_value
                    self.logger.debug(f"    ğŸ“Œ [{row_count}] {field_name}: {field_value[:50]}...")
                    
                    # æ ¹æ®å­—æ®µåæå–ç‰¹å®šä¿¡æ¯
                    # å‡ºç”Ÿæ—¥æœŸ
                    if any(keyword in field_name for keyword in ['å‡ºç”Ÿ', 'èª•ç”Ÿ', 'ç”Ÿäº']):
                        if not data.get('birth_date'):
                            parsed_date = self.date_parser.parse_chinese_date(field_value)
                            if parsed_date:
                                data['birth_date'] = parsed_date
                                self.logger.debug(f"    âœ“ ä»è¡¨æ ¼æå–å‡ºç”Ÿæ—¥æœŸ: {field_value} -> {parsed_date}")
                    
                    # å»ä¸–æ—¥æœŸ
                    elif any(keyword in field_name for keyword in ['é€ä¸–', 'å’äº', 'å»ä¸–']):
                        if not data.get('death_date'):
                            parsed_date = self.date_parser.parse_chinese_date(field_value)
                            if parsed_date:
                                data['death_date'] = parsed_date
                                self.logger.debug(f"    âœ“ ä»è¡¨æ ¼æå–å»ä¸–æ—¥æœŸ: {field_value} -> {parsed_date}")
                    
                    # åœ¨ä½æ—¶é—´/ç»Ÿæ²»
                    elif any(keyword in field_name for keyword in ['ç»Ÿæ²»', 'åœ¨ä½', 'reign']):
                        data['infobox_data']['reign'] = field_value
                        self.logger.debug(f"    âœ“ ä»è¡¨æ ¼æå–åœ¨ä½æ—¶é—´: {field_value}")
                    
                    # åº™å·
                    elif any(keyword in field_name for keyword in ['åº™å·']):
                        if not data.get('temple_name'):
                            data['temple_name'] = field_value
                            self.logger.debug(f"    âœ“ ä»è¡¨æ ¼æå–åº™å·: {field_value}")
                    
                    # è°¥å·
                    elif any(keyword in field_name for keyword in ['è°¥å·']):
                        data['infobox_data']['posthumous_name'] = field_value
                        self.logger.debug(f"    âœ“ ä»è¡¨æ ¼æå–è°¥å·: {field_value}")
                    
                    # å¹´å·
                    elif any(keyword in field_name for keyword in ['å¹´å·', 'å¹´è™Ÿ']):
                        if not data.get('reign_title'):
                            data['reign_title'] = field_value
                            self.logger.debug(f"    âœ“ ä»è¡¨æ ¼æå–å¹´å·: {field_value}")
                    
                    # é™µå¢“
                    elif any(keyword in field_name for keyword in ['é™µå¢“', 'é™µå¯', 'å®‰è‘¬']):
                        data['infobox_data']['tomb'] = field_value
                        self.logger.debug(f"    âœ“ ä»è¡¨æ ¼æå–é™µå¢“: {field_value}")
                    
                    # çš‡å
                    elif any(keyword in field_name for keyword in ['çš‡å']):
                        data['infobox_data']['empress'] = field_value
                        self.logger.debug(f"    âœ“ ä»è¡¨æ ¼æå–çš‡å: {field_value}")
                    
                except Exception as row_error:
                    self.logger.debug(f"    âš  å¤„ç†è¡Œæ—¶å‡ºé”™: {str(row_error)}")
                    continue
            
            # å°è¯•æå–å›¾ç‰‡URL
            if not data.get('portrait_url'):
                img = infobox.find('img')
                if img and img.get('src'):
                    img_url = img['src']
                    if img_url.startswith('//'):
                        img_url = 'https:' + img_url
                    elif img_url.startswith('/'):
                        img_url = 'https://zh.wikipedia.org' + img_url
                    
                    data['portrait_url'] = img_url
                    data['infobox_data']['portrait_url'] = img_url
                    self.logger.debug(f"    âœ“ ä»è¡¨æ ¼æå–å›¾ç‰‡URL: {img_url[:60]}...")
            
            self.logger.debug(f"    âœ“ Infoboxè¡¨æ ¼æå–å®Œæˆï¼Œå…± {len(data['infobox_data'])} ä¸ªå­—æ®µ")
        
        except Exception as e:
            self.logger.error(f"    âŒ æå–infoboxè¡¨æ ¼æ—¶å‡ºé”™: {str(e)}")
            import traceback
            self.logger.debug(f"    é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
    
    def _extract_biography_section(self, soup: BeautifulSoup) -> str:
        """
        æå–ç”Ÿå¹³ç« èŠ‚çš„HTMLå†…å®¹
        æŸ¥æ‰¾æ ‡é¢˜åŒ…å«"ç”Ÿå¹³"ã€"æ—©æœŸ"ç­‰å…³é”®è¯çš„ç« èŠ‚
        æ”¯æŒæ¡Œé¢ç‰ˆå’Œç§»åŠ¨ç‰ˆä¸¤ç§HTMLç»“æ„ï¼š
        - æ¡Œé¢ç‰ˆï¼š<div class="mw-heading mw-heading2"><h2 id="ç”Ÿå¹³">...</h2></div>
        - ç§»åŠ¨ç‰ˆï¼š<div class="mw-heading mw-heading2 section-heading" onclick="..."><h2 id="ç”Ÿå¹³">...</h2></div>
        """
        try:
            self.logger.debug("    ğŸ” å¼€å§‹æå–ç”Ÿå¹³ç« èŠ‚HTML...")
            
            # æŸ¥æ‰¾æ‰€æœ‰ mw-heading2ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªæ ‡é¢˜åŒ…å«ç”Ÿå¹³ç›¸å…³å…³é”®è¯çš„ç« èŠ‚
            all_headings = soup.find_all('div', class_=lambda x: x and 'mw-heading' in x and 'mw-heading2' in x)
            
            if not all_headings:
                self.logger.warning("    âš  æœªæ‰¾åˆ°ä»»ä½•mw-heading2æ ‡é¢˜")
                return ''
            
            # æŸ¥æ‰¾ç”Ÿå¹³ç›¸å…³ç« èŠ‚ï¼ˆæŒ‰ä¼˜å…ˆçº§åŒ¹é…ï¼‰
            biography_keywords = ['ç”Ÿå¹³', 'æ—©æœŸ', 'ç»å†', 'å³ä½', 'ç™»åŸº']
            first_heading = None
            
            for heading in all_headings:
                h2_elem = heading.find('h2')
                if h2_elem:
                    h2_text = h2_elem.get_text()
                    if any(keyword in h2_text for keyword in biography_keywords):
                        first_heading = heading
                        self.logger.debug(f"    âœ“ æ‰¾åˆ°ç”Ÿå¹³ç›¸å…³ç« èŠ‚: {h2_text}")
                        break
            
            # å¦‚æœæ²¡æ‰¾åˆ°å…³é”®è¯åŒ¹é…çš„ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªheading
            if not first_heading:
                first_heading = all_headings[0]
                h2_elem = first_heading.find('h2')
                h2_text = h2_elem.get_text() if h2_elem else 'æœªçŸ¥'
                self.logger.warning(f"    âš  æœªæ‰¾åˆ°ç”Ÿå¹³å…³é”®è¯ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªç« èŠ‚: {h2_text}")
            
            # ç§»åŠ¨ç‰ˆä½¿ç”¨ <section> æ ‡ç­¾åŒ…è£¹å†…å®¹ï¼Œæ¡Œé¢ç‰ˆç›´æ¥è·Ÿåœ¨headingå
            html_parts = []
            html_parts.append(str(first_heading))  # åŒ…å«æ ‡é¢˜æœ¬èº«
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç§»åŠ¨ç‰ˆï¼ˆä¸‹ä¸€ä¸ªå…ƒç´ æ˜¯sectionæ ‡ç­¾ï¼‰
            next_elem = first_heading.find_next_sibling()
            if next_elem and next_elem.name == 'section':
                # ç§»åŠ¨ç‰ˆï¼šæå–sectionå†…çš„å…¨éƒ¨å†…å®¹
                self.logger.debug(f"    âœ“ æ£€æµ‹åˆ°ç§»åŠ¨ç‰ˆHTMLç»“æ„ï¼ˆsectionæ ‡ç­¾ï¼‰")
                html_parts.append(str(next_elem))
                element_count = 1
            else:
                # æ¡Œé¢ç‰ˆï¼šæ”¶é›†è¯¥headingä¹‹åã€ä¸‹ä¸€ä¸ªheading2ä¹‹å‰çš„æ‰€æœ‰å†…å®¹
                self.logger.debug(f"    âœ“ æ£€æµ‹åˆ°æ¡Œé¢ç‰ˆHTMLç»“æ„")
                current_elem = next_elem
                element_count = 0
                
                while current_elem:
                    # æ£€æŸ¥æ˜¯å¦é‡åˆ°ä¸‹ä¸€ä¸ª mw-heading2
                    if current_elem.name == 'div':
                        classes = current_elem.get('class', [])
                        if 'mw-heading' in classes and 'mw-heading2' in classes:
                            self.logger.debug(f"    âœ“ é‡åˆ°ä¸‹ä¸€ä¸ªheading2ï¼Œåœæ­¢é‡‡é›†")
                            break
                    
                    html_parts.append(str(current_elem))
                    element_count += 1
                    current_elem = current_elem.find_next_sibling()
            
            biography_html = '\n'.join(html_parts)
            self.logger.debug(f"    âœ“ ç”Ÿå¹³ç« èŠ‚æå–å®Œæˆ: {element_count} ä¸ªå…ƒç´ , {len(biography_html)} å­—ç¬¦")
            
            return biography_html
        
        except Exception as e:
            self.logger.error(f"    âŒ æå–ç”Ÿå¹³ç« èŠ‚å¤±è´¥: {str(e)}")
            import traceback
            self.logger.debug(f"    é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return ''

    def _create_emperor_entity(self, emperor_data: Dict, emperor_info: Dict) -> Emperor:
        """åˆ›å»ºçš‡å¸å®ä½“"""
        emperor_id = generate_id("ming_emperor", emperor_data['name'], emperor_info['dynasty_order'])
        
        # è§£æåœ¨ä½æ—¶é—´
        reign_years = emperor_info.get('reign_years', '')
        reign_start, reign_end = self._parse_reign_years(reign_years)
        
        return Emperor(
            emperor_id=emperor_id,
            dynasty_id=MING_DYNASTY['dynasty_id'],
            name=emperor_data['name'],
            temple_name=emperor_data.get('temple_name'),
            reign_title=emperor_data.get('reign_title'),
            birth_date=emperor_data.get('birth_date'),
            death_date=emperor_data.get('death_date'),
            reign_start=reign_start,
            reign_end=reign_end,
            dynasty_order=emperor_info['dynasty_order'],
            biography=emperor_data.get('biography'),
            achievements=emperor_data.get('achievements'),
            portrait_url=emperor_data.get('portrait_url'),
            html_content=emperor_data.get('biography_html', ''),
            source_url=f"https://zh.wikipedia.org/wiki/{emperor_data['name']}",
            data_source='wikipedia'
        )
    
    def _parse_reign_years(self, reign_years_str: str) -> tuple:
        """è§£æåœ¨ä½å¹´ä»½"""
        try:
            years = reign_years_str.split(',')[0].strip()
            start_year, end_year = years.split('-')
            return (date(int(start_year), 1, 1), date(int(end_year), 12, 31))
        except Exception:
            return (date(1368, 1, 1), None)
    
    def parse_event(self, response):
        """è§£æäº‹ä»¶é¡µé¢"""
        try:
            soup = BeautifulSoup(response.text, 'lxml')
            event_data = self._extract_event_data(soup, response.meta.get('emperor_id'))
            
            if event_data:
                self.logger.info(f"[Wiki] æˆåŠŸçˆ¬å–äº‹ä»¶: {event_data.title}")
                yield event_data
        
        except Exception as e:
            self.logger.error(f"[Wiki] è§£æäº‹ä»¶é¡µé¢å¤±è´¥: {str(e)}")
    
    def _extract_event_data(self, soup: BeautifulSoup, emperor_id: str) -> Optional[Event]:
        """ä»ç»´åŸºç™¾ç§‘é¡µé¢ä¸­æå–äº‹ä»¶æ•°æ®"""
        try:
            # è·å–æ ‡é¢˜
            title_elem = soup.find('h1', class_='firstHeading')
            if not title_elem:
                return None
            
            title = clean_text(title_elem.get_text())
            
            # æå–Infoboxä¿¡æ¯
            start_date = None
            location = None
            description = ''
            
            infobox = soup.find('table', class_='infobox')
            if infobox:
                # æå–æ—¶é—´
                time_row = infobox.find('th', text=re.compile('æ—¶é—´|æ—¥æœŸ'))
                if time_row and time_row.find_next_sibling('td'):
                    time_text = time_row.find_next_sibling('td').get_text(strip=True)
                    start_date = self.date_parser.parse_chinese_date(time_text)
                
                # æå–åœ°ç‚¹
                location_row = infobox.find('th', text=re.compile('åœ°ç‚¹|åœ°å€'))
                if location_row and location_row.find_next_sibling('td'):
                    location = clean_text(location_row.find_next_sibling('td').get_text())
            
            # æå–æè¿°
            content = soup.find('div', class_='mw-parser-output')
            if content:
                first_para = content.find('p', recursive=False)
                if first_para:
                    for sup in first_para.find_all('sup'):
                        sup.decompose()
                    description = clean_text(first_para.get_text())
            
            # åˆ›å»ºEventå®ä½“
            event_id = generate_id("ming_event", title)
            
            event = Event(
                event_id=event_id,
                dynasty_id=MING_DYNASTY['dynasty_id'],
                emperor_id=emperor_id,
                title=title,
                event_type=self._determine_event_type(title),
                start_date=start_date or date(1368, 1, 1),
                location=location,
                description=description,
                data_source='wikipedia'
            )
            
            return event
        
        except Exception as e:
            self.logger.warning(f"[Wiki] æå–äº‹ä»¶æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return None
    
    def _determine_event_type(self, title: str) -> EventType:
        """æ ¹æ®æ ‡é¢˜åˆ¤æ–­äº‹ä»¶ç±»å‹"""
        if any(keyword in title for keyword in ['ä¹‹æˆ˜', 'ä¹‹å½¹', 'æˆ˜äº‰', 'æˆ˜å½¹']):
            return EventType.MILITARY
        elif any(keyword in title for keyword in ['æ”¿å˜', 'æ”¹é©', 'åºŸé™¤', 'è®¾ç«‹']):
            return EventType.POLITICAL
        elif any(keyword in title for keyword in ['æ–‡åŒ–', 'è¿åŠ¨', 'è‘—ä½œ']):
            return EventType.CULTURAL
        elif any(keyword in title for keyword in ['è´¸æ˜“', 'ä¸‹è¥¿æ´‹', 'é€šå•†']):
            return EventType.DIPLOMATIC
        else:
            return EventType.POLITICAL
    
    def parse_person(self, response):
        """è§£æäººç‰©é¡µé¢"""
        try:
            soup = BeautifulSoup(response.text, 'lxml')
            person_data = self._extract_person_data(soup, response.meta.get('emperor_id'))
            
            if person_data:
                self.logger.info(f"[Wiki] æˆåŠŸçˆ¬å–äººç‰©: {person_data.name}")
                yield person_data
        
        except Exception as e:
            self.logger.error(f"[Wiki] è§£æäººç‰©é¡µé¢å¤±è´¥: {str(e)}")
    
    def _extract_person_data(self, soup: BeautifulSoup, emperor_id: str) -> Optional[Person]:
        """ä»ç»´åŸºç™¾ç§‘é¡µé¢ä¸­æå–äººç‰©æ•°æ®"""
        try:
            # è·å–äººå
            title_elem = soup.find('h1', class_='firstHeading')
            if not title_elem:
                return None
            
            name = clean_text(title_elem.get_text())
            
            # æå–ä¿¡æ¯
            alias_list = []
            birth_date = None
            death_date = None
            position = None
            person_type = PersonType.OTHER
            biography = ''
            infobox_data = {}  # å­˜å‚¨infoboxä¸­çš„æ‰€æœ‰ä¿¡æ¯
            biography_html = ''  # å­˜å‚¨ç”Ÿå¹³HTMLå†…å®¹
            
            infobox = soup.find('table', class_='infobox')
            if infobox:
                # æå–infoboxä¸­çš„æ‰€æœ‰æ®µè½
                infobox_paragraphs = infobox.find_all('p')
                for p in infobox_paragraphs:
                    p_text = clean_text(p.get_text())
                    if p_text:
                        infobox_data.setdefault('paragraphs', []).append(p_text)
                
                # æå–åˆ«å
                alias_row = infobox.find('th', text=re.compile('å­—|è™Ÿ|åˆ¥å'))
                if alias_row and alias_row.find_next_sibling('td'):
                    alias_text = alias_row.find_next_sibling('td').get_text(strip=True)
                    alias_list = [a.strip() for a in re.split('[ï¼Œã€\n]', alias_text) if a.strip()]
                    infobox_data['alias'] = alias_text
                
                # æå–å‡ºç”Ÿæ—¥æœŸ
                birth_row = infobox.find('th', text=re.compile('å‡ºç”Ÿ'))
                if birth_row and birth_row.find_next_sibling('td'):
                    birth_text = birth_row.find_next_sibling('td').get_text(strip=True)
                    birth_date = self.date_parser.parse_chinese_date(birth_text)
                    infobox_data['birth'] = birth_text
                
                # æå–å»ä¸–æ—¥æœŸ
                death_row = infobox.find('th', text=re.compile('é€ä¸–'))
                if death_row and death_row.find_next_sibling('td'):
                    death_text = death_row.find_next_sibling('td').get_text(strip=True)
                    death_date = self.date_parser.parse_chinese_date(death_text)
                    infobox_data['death'] = death_text
                
                # æå–èŒä½
                position_row = infobox.find('th', text=re.compile('è·æ¥­|å®˜è·'))
                if position_row and position_row.find_next_sibling('td'):
                    position = clean_text(position_row.find_next_sibling('td').get_text())
                    person_type = self._determine_person_type(position)
                    infobox_data['position'] = position
            
            # æå–ç”Ÿå¹³
            content = soup.find('div', class_='mw-parser-output')
            if content:
                first_para = content.find('p', recursive=False)
                if first_para:
                    for sup in first_para.find_all('sup'):
                        sup.decompose()
                    biography = clean_text(first_para.get_text())
                
                # æå–ç”Ÿå¹³HTMLå†…å®¹
                biography_html = self._extract_biography_section(soup)
            
            # åˆ›å»ºPersonå®ä½“
            person_id = generate_id("ming_person", name)
            
            person = Person(
                person_id=person_id,
                dynasty_id=MING_DYNASTY['dynasty_id'],
                name=name,
                person_type=person_type,
                alias=alias_list,
                birth_date=birth_date,
                death_date=death_date,
                position=position,
                biography=biography,
                related_emperors=[emperor_id] if emperor_id else [],
                html_content=biography_html,
                source_url=f"https://zh.wikipedia.org/wiki/{name}",
                data_source='wikipedia'
            )
            
            return person
        
        except Exception as e:
            self.logger.warning(f"[Wiki] æå–äººç‰©æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return None
    
    def _determine_person_type(self, position: str) -> PersonType:
        """æ ¹æ®èŒä½åˆ¤æ–­äººç‰©ç±»å‹"""
        if not position:
            return PersonType.OTHER
        
        if any(keyword in position for keyword in ['å°‡è»', 'å°‡é¢†', 'æ­¦', 'å†›']):
            return PersonType.GENERAL
        elif any(keyword in position for keyword in ['è©©äºº', 'æ–‡å­¦', 'ä½œå®¶', 'è©äºº']):
            return PersonType.WRITER
        elif any(keyword in position for keyword in ['ç•«å®¶', 'æ›¸æ³•', 'è—è¡“']):
            return PersonType.ARTIST
        elif any(keyword in position for keyword in ['å¤§è‡£', 'å°šæ›¸', 'ä¾éƒ', 'å­¸å£«', 'é–£']):
            return PersonType.OFFICIAL
        elif any(keyword in position for keyword in ['çš‡å­', 'çš‡å', 'å¦ƒ']):
            return PersonType.ROYAL
        elif any(keyword in position for keyword in ['åƒ§', 'é“']):
            return PersonType.MONK
        elif any(keyword in position for keyword in ['æ€æƒ³', 'å“²å­¸']):
            return PersonType.THINKER
        elif any(keyword in position for keyword in ['ç§‘å­¸', 'å¤©æ–‡', 'æ•¸å­¸', 'é†«']):
            return PersonType.SCIENTIST
        else:
            return PersonType.OTHER
