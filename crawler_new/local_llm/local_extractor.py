"""
æœ¬åœ°å¤§æ¨¡å‹æå–å™¨
ä½¿ç”¨ Ollama æœ¬åœ°éƒ¨ç½²çš„å¤§æ¨¡å‹è¿›è¡Œç»“æ„åŒ–æ•°æ®æå–
"""

import json
import os
from datetime import datetime
import requests
from typing import Dict, Any, List
from bs4 import BeautifulSoup
from .html_cleaner import HTMLCleanerFactory, CleanedContent


class LocalLLMExtractor:
    """æœ¬åœ°å¤§æ¨¡å‹æå–å™¨ï¼ˆåŸºäº Ollamaï¼‰"""
    
    def __init__(self, model_name: str = 'qwen2.5:7b', base_url: str = 'http://localhost:11434'):
        """
        åˆå§‹åŒ–æœ¬åœ°å¤§æ¨¡å‹æå–å™¨
        
        Args:
            model_name: Ollama æ¨¡å‹åç§°ï¼Œé»˜è®¤ qwen2.5:7b
            base_url: Ollama API åœ°å€ï¼Œé»˜è®¤æœ¬åœ°
        """
        self.model_name = model_name
        self.base_url = base_url
        self.api_url = f'{base_url}/api/generate'
    
    def extract_emperor_all_data(self, html_content: str, page_name: str) -> Dict[str, Any]:
        """
        ä¸€æ¬¡æ€§æå–çš‡å¸æ‰€æœ‰ä¿¡æ¯ï¼ˆåŸºæœ¬ä¿¡æ¯ + ç”Ÿå¹³äº‹è¿¹ï¼‰
        
        Args:
            html_content: Wikipedia HTML å†…å®¹
            page_name: é¡µé¢åç§°ï¼ˆçš‡å¸å§“åï¼‰
        
        Returns:
            åŒ…å« emperor_info å’Œ events çš„å­—å…¸
        """
        # æ¸…ç† HTMLï¼Œåªä¿ç•™ä¸»è¦å†…å®¹
        cleaned_html = self._clean_html(html_content, 'wikipedia', page_name)
        
        print(f'Cleaning HTML content for {page_name}...')
        # æ„å»ºä¸€æ¬¡æ€§æå–çš„æç¤ºè¯
        prompt = self._build_emperor_all_data_prompt(cleaned_html, page_name)
        print(f'Building prompt for {page_name}...')

        # è°ƒç”¨æœ¬åœ°å¤§æ¨¡å‹ API
        response_text = self._call_local_llm(prompt)
        print(f'Calling local LLM for {page_name}...')

        # å­˜å‚¨ JSON å“åº”
        self._save_response_to_file(response_text)
        print(f'Saving response to file for {page_name}...')
        # è§£æè¿”å›ç»“æœ
        result = self._parse_emperor_all_data_response(response_text)
        print(f'Parsing response for {page_name}...')
        
        return result
    
    def extract_emperor_info(self, html_content: str, page_name: str) -> Dict[str, Any]:
        """
        ä»çš‡å¸é¡µé¢ HTML ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯
        
        Args:
            html_content: Wikipedia HTML å†…å®¹
            page_name: é¡µé¢åç§°ï¼ˆçš‡å¸å§“åï¼‰
        
        Returns:
            ç»“æ„åŒ–çš„çš‡å¸ä¿¡æ¯å­—å…¸
        """
        # æ¸…ç† HTML
        cleaned_html = self._clean_html(html_content, 'wikipedia', page_name)
        
        # æ„å»ºæç¤ºè¯
        prompt = self._build_emperor_prompt(cleaned_html, page_name)
        
        # è°ƒç”¨æœ¬åœ°å¤§æ¨¡å‹ API
        response_text = self._call_local_llm(prompt)
        
        # è§£æè¿”å›ç»“æœ
        emperor_info = self._parse_emperor_response(response_text)
        
        return emperor_info
    
    def extract_emperor_events(self, html_content: str, page_name: str) -> List[Dict[str, Any]]:
        """
        ä»çš‡å¸é¡µé¢ HTML ä¸­æå–ç”Ÿå¹³äº‹è¿¹
        
        Args:
            html_content: Wikipedia HTML å†…å®¹
            page_name: é¡µé¢åç§°ï¼ˆçš‡å¸å§“åï¼‰
        
        Returns:
            ç”Ÿå¹³äº‹è¿¹åˆ—è¡¨
        """
        # æ¸…ç† HTML
        cleaned_html = self._clean_html(html_content, 'wikipedia', page_name)
        
        # æ„å»ºæç¤ºè¯
        prompt = self._build_events_prompt(cleaned_html, page_name)
        
        # è°ƒç”¨æœ¬åœ°å¤§æ¨¡å‹ API
        response_text = self._call_local_llm(prompt)
        
        # å­˜å‚¨ JSON å“åº”
        self._save_response_to_file(response_text)

        # è§£æè¿”å›ç»“æœ
        events = self._parse_events_response(response_text)
        
        return events
    
    def _clean_html(self, html_content: str, data_source: str = 'wikipedia', page_name: str = None) -> str:
        """
        æ¸…ç† HTMLï¼Œç§»é™¤è„šæœ¬ã€æ ·å¼ç­‰æ— å…³å†…å®¹
        æå– infobox vcard åˆ° id="è¯„ä»·" ä¹‹é—´çš„ä¸»è¦å†…å®¹
        åŒæ—¶æå–ç›®å½•ç»“æ„å’Œäººç‰©/äº‹ä»¶é“¾æ¥
        
        Args:
            html_content: åŸå§‹ HTML
            data_source: æ•°æ®æºï¼ˆå›ºå®šä¸º 'wikipedia'ï¼‰
            page_name: é¡µé¢åç§°ï¼ˆå¯é€‰ï¼Œç”¨äºä¿å­˜æ–‡ä»¶å‘½åï¼‰
        
        Returns:
            æ¸…ç†åçš„ HTML æ–‡æœ¬
        """
        # ä½¿ç”¨ç‹¬ç«‹çš„HTMLæ¸…ç†å™¨
        cleaner = HTMLCleanerFactory.create_cleaner(data_source)
        cleaned_content = cleaner.clean(html_content)
        
        # ä¿å­˜æ¸…ç†åçš„æ–‡æœ¬
        self._save_cleaned_text(cleaned_content.text, page_name)
        
        # ä¿å­˜ç›®å½•ç»“æ„
        self._save_toc(cleaned_content.toc, page_name)
        
        # ä¿å­˜é“¾æ¥æ•°æ®
        self._save_links(cleaned_content.links, page_name)
        
        return cleaned_content.text
    
    def _build_emperor_all_data_prompt(self, cleaned_html: str, page_name: str) -> str:
        """æ„å»ºä¸€æ¬¡æ€§æå–çš‡å¸æ‰€æœ‰ä¿¡æ¯çš„æç¤ºè¯(åŸºæœ¬ä¿¡æ¯ + ç”Ÿå¹³äº‹è¿¹)"""
        # å¯¹äºæœ¬åœ°å°æ¨¡å‹,é€‚å½“é™åˆ¶è¾“å…¥é•¿åº¦ä»¥æå‡æå–è´¨é‡
        html_content = cleaned_html
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªå†å²æ•°æ®æå–ä¸“å®¶ã€‚ä»ä»¥ä¸‹ Wikipedia å†…å®¹ä¸­æå–å…³äºçš‡å¸â€œ{page_name}â€çš„ç»“æ„åŒ–ä¿¡æ¯ã€‚

é‡è¦è¦æ±‚:
1. å¿…é¡»æå– 20+ æ¡ç”Ÿå¹³äº‹è¿¹ï¼ŒæŒ‰æ—¶é—´é¡ºåºæ’åˆ—
2. æ¯ä¸ªäº‹ä»¶å¿…é¡»åŒ…å«:æ—¶é—´ã€äº‹ä»¶ã€äº‹ä»¶å½±å“ã€äººç‰©ã€åœ°ç‚¹
3. ä»å‡ºç”Ÿåˆ°å»ä¸–ï¼Œå…¨é¢è¦†ç›–é‡è¦äº‹ä»¶
4. åªè¿”å› JSON æ ¼å¼ï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹

=== Wikipedia å†…å®¹ ===
{html_content}

è¯·æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼Œåªè¿”å› JSONï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ï¼š

{{
  "emperor_info": {{
    "çš‡å¸": "æœ±å…ƒç’‹",
    "åº™å·": "æ˜å¤ªç¥–",
    "å¹´å·": "æ´ªæ­¦",
    "ç”»åƒurl": "https://...",
    "å‡ºç”Ÿ": "1328å¹´10æœˆ21æ—¥ï¼ˆå…ƒå¤©å†å…ƒå¹´ä¹æœˆåå…«æ—¥ï¼‰",
    "å»ä¸–": "1398å¹´6æœˆ24æ—¥ï¼ˆæ´ªæ­¦ä¸‰åä¸€å¹´é—°äº”æœˆåˆåï¼‰",
    "ç®€ä»‹": "æ˜æœå¼€å›½çš‡å¸..."
  }},
  "events": [
    {{
      "æ—¶é—´": "1328å¹´10æœˆ29æ—¥ï¼ˆå…ƒå¤©å†å…ƒå¹´ä¹æœˆåå…«æ—¥ï¼‰",
      "äº‹ä»¶": "å‡ºç”Ÿäºè´«å†œå®¶åº­ï¼ŒåŸåæœ±é‡å…«ï¼Œåæ”¹åæœ±å…´å®—ã€‚å‡ºèº«å¯’å¾®ä¸ºå…¶æ—¥åé‡å†œã€ä¸¥æƒ©è´ªè…åŸ‹ä¸‹æ€æƒ³åŸºç¡€ã€‚",
      "äº‹ä»¶å½±å“": "å¡‘é€ äº†æœ±å…ƒç’‹çš„å¹³æ°‘æ„è¯†å’Œåè…å†³å¿ƒ",
      "äººç‰©": [
        {{"å§“å": "æœ±äº”å››", "å…³ç³»": "çˆ¶", "é“¾æ¥": "https://..."}},
        {{"å§“å": "é™ˆæ°", "å…³ç³»": "æ¯", "é“¾æ¥": "https://..."}}
      ],
      "åœ°ç‚¹": "æ¿ å·é’Ÿç¦»å¿ä¸œä¹¡ï¼ˆä»Šå®‰å¾½çœå‡¤é˜³å¿å°æºªæ²³é•‡ç‡ƒç¯å¯ºæ‘ï¼‰"
    }},
    {{
      "æ—¶é—´": "1344å¹´ï¼ˆè‡³æ­£å››å¹´ï¼‰",
      "äº‹ä»¶": "æ·®åŒ—å¤§æ—±ï¼Œçˆ¶æ¯å…„é•¿ç›¸ç»§å»ä¸–ï¼›å…¥çš‡è§‰å¯ºä¸ºåƒ§ï¼Œä¸ä¹…è¢«é£æ•£ï¼Œå¼€å§‹ä¸‰å¹´æ¸¸æ–¹åƒ§ç”Ÿæ¶¯ï¼Œäº²å†æ°‘é—´ç–¾è‹¦ï¼Œæ·±åˆ»å½±å“å…¶æ²»å›½ç†å¿µã€‚",
      "äº‹ä»¶å½±å“": "äº²å†åº•å±‚è‹¦éš¾ï¼Œå½¢æˆé‡å†œæŠ‘å•†æ”¿ç­–åŸºç¡€",
      "äººç‰©": [],
      "åœ°ç‚¹": "çš‡è§‰å¯ºï¼ˆæ¿ å·ï¼Œä»Šå®‰å¾½å‡¤é˜³ï¼‰"
    }}
  ]
}}

æ³¨æ„ï¼š
**åŸºæœ¬ä¿¡æ¯éƒ¨åˆ†**ï¼š
1. **æ—¥æœŸæ ¼å¼**ï¼š"YYYYå¹´MMæœˆDDæ—¥ï¼ˆå¤ä»£å¹´å·çºªå¹´ï¼‰"ï¼Œå¦‚"1328å¹´10æœˆ21æ—¥ï¼ˆå…ƒå¤©å†å…ƒå¹´ä¹æœˆåå…«æ—¥ï¼‰"
2. **ç”»åƒurl**ï¼šä½¿ç”¨ Wikipedia çš„é«˜æ¸…å›¾ç‰‡é“¾æ¥
3. **ç®€ä»‹**ï¼šæ§åˆ¶åœ¨250å­—ä»¥å†…ï¼Œçªå‡ºå…³é”®æˆå°±
4. **ç¼ºå¤±å­—æ®µ**ï¼šå¦‚æœæŸä¸ªå­—æ®µæ‰¾ä¸åˆ°ï¼Œå¡«å†™ null

**ç”Ÿå¹³äº‹è¿¹éƒ¨åˆ†ï¼ˆæœ€é‡è¦ï¼‰**ï¼š
1. **æ•°é‡è¦æ±‚**ï¼šå¿…é¡»æå–è¶³å¤Ÿå¤šçš„äº‹ä»¶ï¼Œè¦†ç›–ä»å‡ºç”Ÿåˆ°å»ä¸–çš„å®Œæ•´ç”Ÿæ¶¯
2. **æ—¶é—´æ ¼å¼**ï¼šç²¾ç¡®åˆ°å¹´æœˆæ—¥ï¼Œå¹¶æ ‡æ³¨å¤ä»£å¹´å·ï¼Œå¦‚"1328å¹´10æœˆ29æ—¥ï¼ˆå…ƒå¤©å†å…ƒå¹´ä¹æœˆåå…«æ—¥ï¼‰"
3. **äº‹ä»¶æè¿°**ï¼šè¯¦ç»†è®°å½•äº‹ä»¶ç»è¿‡å’ŒèƒŒæ™¯ï¼Œ150-200å­—
4. **äº‹ä»¶å½±å“**ï¼šå¿…å¡«ï¼Œç®€è¿°è¯¥äº‹ä»¶å¯¹åç»­å†å²çš„å½±å“
5. **äººç‰©ç»“æ„**ï¼šæ¯ä¸ªäººç‰©åŒ…å«â€œå§“åâ€ã€â€œå…³ç³»â€ï¼ˆå¦‚çˆ¶ã€æ¯ã€å¥½å‹ã€å¤§è‡£ç­‰ï¼‰ã€â€œé“¾æ¥â€
6. **åœ°ç‚¹æ ¼å¼**ï¼šâ€œå¤ä»£åœ°åï¼ˆä»Šåœ°åï¼‰â€ï¼Œå¦‚â€œåº”å¤©åºœï¼ˆä»Šå—äº¬å¸‚ï¼‰â€
7. **æå–é‡ç‚¹**ï¼šæ”¿æ²»ã€å†›äº‹ã€æ–‡åŒ–ã€å¤–äº¤ç­‰é‡å¤§äº‹ä»¶ï¼ŒæŒ‰æ—¶é—´é¡ºåºæ’åˆ—
8. **é“¾æ¥æå–**ï¼šä»åŸç½‘é¡µä¸­æå–å®é™…é“¾æ¥ï¼Œå¦‚æœæ²¡æœ‰åˆ™å¡«å†™ null

ç‰¹åˆ«æé†’ï¼šç”Ÿå¹³äº‹è¿¹å¿…é¡»è¯¦ç»†ä¸”å…¨é¢ï¼ŒåŒ…æ‹¬ï¼š
- å‡ºç”Ÿå’Œæ—©å¹´ç»å†
- é‡è¦å†›äº‹è¡ŒåŠ¨å’Œæˆ˜å½¹
- ç™»åŸºå’Œæ”¿æ²»æ”¹é©
- æ–‡åŒ–å’Œåˆ¶åº¦å»ºè®¾
- é‡å¤§å†å²äº‹ä»¶å‚ä¸
- æ™šå¹´æ”¿ç­–å’Œå»ä¸–
ç¡®ä¿æå– 15-20 æ¡äº‹è¿¹ï¼
"""
        return prompt
    
    def _build_emperor_prompt(self, cleaned_html: str, page_name: str) -> str:
        """æ„å»ºçš‡å¸ä¿¡æ¯æå–çš„æç¤ºè¯"""
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªå†å²æ•°æ®æå–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹ Wikipedia çš„ç½‘é¡µå†…å®¹ä¸­æå–å…³äºçš‡å¸â€œ{page_name}â€çš„ç»“æ„åŒ–ä¿¡æ¯ã€‚

=== Wikipedia å†…å®¹ ===
{cleaned_html}

è¯·æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼Œåªè¿”å› JSONï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ï¼š

{{
  "çš‡å¸": "æœ±å…ƒç’‹",
  "åº™å·": "æ˜å¤ªç¥–",
  "å¹´å·": "æ´ªæ­¦",
  "ç”»åƒurl": "https://...",
  "å‡ºç”Ÿ": "1328å¹´10æœˆ21æ—¥ï¼ˆå…ƒå¤©å†å…ƒå¹´ä¹æœˆåå…«æ—¥ï¼‰",
  "å»ä¸–": "1398å¹´6æœˆ24æ—¥ï¼ˆæ´ªæ­¦ä¸‰åä¸€å¹´é—°äº”æœˆåˆåï¼‰",
  "ç®€ä»‹": "æ˜æœå¼€å›½çš‡å¸..."
}}

æ³¨æ„ï¼š
1. **æ—¥æœŸæ ¼å¼**ï¼š"YYYYå¹´MMæœˆDDæ—¥ï¼ˆå¤ä»£å¹´å·çºªå¹´ï¼‰"ï¼Œå¦‚"1328å¹´10æœˆ21æ—¥ï¼ˆå…ƒå¤©å†å…ƒå¹´ä¹æœˆåå…«æ—¥ï¼‰"
2. **ç”»åƒurl**ï¼šä½¿ç”¨ Wikipedia çš„é«˜æ¸…å›¾ç‰‡é“¾æ¥
3. **ç®€ä»‹**ï¼šæ§åˆ¶åœ¨250å­—ä»¥å†…ï¼Œçªå‡ºå…³é”®æˆå°±
4. **ç¼ºå¤±å­—æ®µ**ï¼šå¦‚æœæŸä¸ªå­—æ®µæ‰¾ä¸åˆ°ï¼Œå¡«å†™ null
"""
        return prompt
    
    def _build_events_prompt(self, cleaned_html: str, page_name: str) -> str:
        """æ„å»ºç”Ÿå¹³äº‹è¿¹æå–çš„æç¤ºè¯"""
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªå†å²æ•°æ®æå–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹ Wikipedia çš„ç½‘é¡µå†…å®¹ä¸­æå–å…³äºçš‡å¸â€œ{page_name}â€çš„ç”Ÿå¹³äº‹è¿¹æ—¶é—´çº¿ã€‚

=== Wikipedia å†…å®¹ ===
{cleaned_html}

è¯·æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºäº‹è¿¹åˆ—è¡¨ï¼Œåªè¿”å› JSONï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ï¼š

[
  {{
    "æ—¶é—´": "1328å¹´10æœˆ29æ—¥ï¼ˆå…ƒå¤©å†å…ƒå¹´ä¹æœˆåå…«æ—¥ï¼‰",
    "äº‹ä»¶": "å‡ºç”Ÿäºè´«å†œå®¶åº­ï¼ŒåŸåæœ±é‡å…«ï¼Œåæ”¹åæœ±å…´å®—ã€‚å‡ºèº«å¯’å¾®ä¸ºå…¶æ—¥åé‡å†œã€ä¸¥æƒ©è´ªè…åŸ‹ä¸‹æ€æƒ³åŸºç¡€ã€‚",
    "äº‹ä»¶å½±å“": "å¡‘é€ äº†æœ±å…ƒç’‹çš„å¹³æ°‘æ„è¯†å’Œåè…å†³å¿ƒ",
    "äººç‰©": [
      {{"å§“å": "æœ±äº”å››", "å…³ç³»": "çˆ¶", "é“¾æ¥": "https://..."}},
      {{"å§“å": "é™ˆæ°", "å…³ç³»": "æ¯", "é“¾æ¥": "https://..."}},
      {{"å§“å": "å¥å®¹æœ±æ°", "å…³ç³»": "ç¥–çˆ¶", "é“¾æ¥": null}}
    ],
    "åœ°ç‚¹": "æ¿ å·é’Ÿç¦»å¿ä¸œä¹¡ï¼ˆä»Šå®‰å¾½çœå‡¤é˜³å¿å°æºªæ²³é•‡ç‡ƒç¯å¯ºæ‘ï¼‰"
  }},
  {{
    "æ—¶é—´": "1344å¹´ï¼ˆè‡³æ­£å››å¹´ï¼‰",
    "äº‹ä»¶": "æ·®åŒ—å¤§æ—±ï¼Œçˆ¶æ¯å…„é•¿ç›¸ç»§å»ä¸–ï¼›å…¥çš‡è§‰å¯ºä¸ºåƒ§ï¼Œä¸ä¹…è¢«é£æ•£ï¼Œå¼€å§‹ä¸‰å¹´æ¸¸æ–¹åƒ§ç”Ÿæ¶¯ï¼Œäº²å†æ°‘é—´ç–¾è‹¦ï¼Œæ·±åˆ»å½±å“å…¶æ²»å›½ç†å¿µã€‚",
    "äº‹ä»¶å½±å“": "äº²å†åº•å±‚è‹¦éš¾ï¼Œå½¢æˆé‡å†œæŠ‘å•†æ”¿ç­–åŸºç¡€",
    "äººç‰©": [],
    "åœ°ç‚¹": "çš‡è§‰å¯ºï¼ˆæ¿ å·ï¼Œä»Šå®‰å¾½å‡¤é˜³ï¼‰"
  }}
]

æ³¨æ„ï¼š
1. **æ—¶é—´æ ¼å¼**ï¼šç²¾ç¡®åˆ°å¹´æœˆæ—¥ï¼Œå¹¶æ ‡æ³¨å¤ä»£å¹´å·ï¼Œå¦‚"1328å¹´10æœˆ29æ—¥ï¼ˆå…ƒå¤©å†å…ƒå¹´ä¹æœˆåå…«æ—¥ï¼‰"
2. **äº‹ä»¶æè¿°**ï¼šè¯¦ç»†è®°å½•äº‹ä»¶ç»è¿‡å’ŒèƒŒæ™¯ï¼Œ200å­—ä»¥å†…
3. **äº‹ä»¶å½±å“**ï¼šç®€è¿°è¯¥äº‹ä»¶å¯¹åç»­å†å²çš„å½±å“ï¼Œå¯é€‰å­—æ®µ
4. **äººç‰©ç»“æ„**ï¼šæ¯ä¸ªäººç‰©åŒ…å«â€œå§“åâ€ã€â€œå…³ç³»â€ï¼ˆå¦‚çˆ¶ã€æ¯ã€å¥½å‹ã€å¤§è‡£ç­‰ï¼‰ã€â€œé“¾æ¥â€ï¼ˆä» Wikipedia æå–ï¼‰
5. **åœ°ç‚¹æ ¼å¼**ï¼šâ€œå¤ä»£åœ°åï¼ˆä»Šåœ°åï¼‰â€ï¼Œå¦‚â€œåº”å¤©åºœï¼ˆä»Šå—äº¬å¸‚ï¼‰â€
6. **æå–é‡ç‚¹**ï¼šæ”¿æ²»ã€å†›äº‹ã€æ–‡åŒ–ã€å¤–äº¤ç­‰é‡å¤§äº‹ä»¶ï¼ŒæŒ‰æ—¶é—´é¡ºåºæ’åˆ—
7. **æ•°é‡æ§åˆ¶**ï¼š15-20ä¸ªå…³é”®äº‹ä»¶
8. **é“¾æ¥æå–**ï¼šä»åŸç½‘é¡µä¸­æå–å®é™…é“¾æ¥ï¼Œå¦‚æœæ²¡æœ‰åˆ™å¡«å†™ null
"""
        return prompt

    
    def _call_local_llm(self, prompt: str, max_retries: int = 3) -> str:
        """
        è°ƒç”¨æœ¬åœ°å¤§æ¨¡å‹ API (Ollama)
        
        Args:
            prompt: æç¤ºè¯
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
        Returns:
            API è¿”å›çš„æ–‡æœ¬
        """
        headers = {
            'Content-Type': 'application/json'
        }
        
        data = {
            'model': self.model_name,
            'prompt': prompt,
            'stream': False,
            'options': {
                'temperature': 0.2,  # é™ä½éšæœºæ€§ï¼Œæå‡ç»“æ„åŒ–è¾“å‡ºç¨³å®šæ€§
                'top_p': 0.8,
                'top_k': 40,
                'num_predict': 4096,  # å¢åŠ æœ€å¤§è¾“å‡ºé•¿åº¦ï¼Œç¡®ä¿èƒ½è¾“å‡º 15-20 æ¡äº‹è¿¹
                'repeat_penalty': 1.1  # é˜²æ­¢é‡å¤å†…å®¹
            }
        }
        
        for attempt in range(max_retries):
            try:
                response = requests.post(
                    self.api_url,
                    headers=headers,
                    json=data,
                    timeout=300  # æœ¬åœ°æ¨ç†å¯èƒ½è¾ƒæ…¢ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´
                )
                
                if response.status_code == 200:
                    result = response.json()
                    # æå–è¿”å›æ–‡æœ¬
                    content = result.get('response', '')
                    return content
                else:
                    raise Exception(f"APIè¯·æ±‚å¤±è´¥: {response.status_code}, {response.text}")
            
            except Exception as e:
                if attempt == max_retries - 1:
                    raise Exception(f"è°ƒç”¨æœ¬åœ°å¤§æ¨¡å‹å¤±è´¥ï¼ˆå·²é‡è¯•{max_retries}æ¬¡ï¼‰: {str(e)}")
                continue
        
        return ""
    
    def _parse_emperor_all_data_response(self, response_text: str) -> Dict[str, Any]:
        """
        è§£æä¸€æ¬¡æ€§æå–çš„å®Œæ•´æ•°æ®ï¼ˆåŸºæœ¬ä¿¡æ¯ + ç”Ÿå¹³äº‹è¿¹ï¼‰
        
        Args:
            response_text: API è¿”å›æ–‡æœ¬
        
        Returns:
            åŒ…å« emperor_info å’Œ events çš„å­—å…¸
        """
        try:
            # å°è¯•æå– JSON éƒ¨åˆ†
            json_str = self._extract_json(response_text)
            result = json.loads(json_str)
            
            # éªŒè¯è¿”å›æ•°æ®ç»“æ„
            if not isinstance(result, dict):
                raise Exception("è¿”å›ç»“æœä¸æ˜¯å­—å…¸æ ¼å¼")
            
            if 'emperor_info' not in result or 'events' not in result:
                raise Exception("è¿”å›ç»“æœç¼ºå°‘ emperor_info æˆ– events å­—æ®µ")
            
            return result
        except Exception as e:
            raise Exception(f"è§£æçš‡å¸å®Œæ•´æ•°æ®å¤±è´¥: {str(e)}, è¿”å›æ–‡æœ¬: {response_text[:200]}")
    
    def _parse_emperor_response(self, response_text: str) -> Dict[str, Any]:
        """
        è§£æçš‡å¸ä¿¡æ¯è¿”å›ç»“æœ
        
        Args:
            response_text: API è¿”å›æ–‡æœ¬
        
        Returns:
            è§£æåçš„å­—å…¸
        """
        try:
            # å°è¯•æå– JSON éƒ¨åˆ†
            json_str = self._extract_json(response_text)
            emperor_info = json.loads(json_str)
            return emperor_info
        except Exception as e:
            raise Exception(f"è§£æçš‡å¸ä¿¡æ¯å¤±è´¥: {str(e)}, è¿”å›æ–‡æœ¬: {response_text[:200]}")
    
    def _parse_events_response(self, response_text: str) -> List[Dict[str, Any]]:
        """
        è§£æç”Ÿå¹³äº‹è¿¹è¿”å›ç»“æœ
        
        Args:
            response_text: API è¿”å›æ–‡æœ¬
        
        Returns:
            äº‹è¿¹åˆ—è¡¨
        """
        try:
            # å°è¯•æå– JSON éƒ¨åˆ†
            json_str = self._extract_json(response_text)
            events = json.loads(json_str)
            return events if isinstance(events, list) else []
        except Exception as e:
            raise Exception(f"è§£æç”Ÿå¹³äº‹è¿¹å¤±è´¥: {str(e)}, è¿”å›æ–‡æœ¬: {response_text[:200]}")
    
    def _extract_json(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå– JSON éƒ¨åˆ†"""
        # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
        text = text.strip()
        if text.startswith('```json'):
            text = text[7:]
        if text.startswith('```'):
            text = text[3:]
        if text.endswith('```'):
            text = text[:-3]
        
        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª { æˆ– [
        start_idx = -1
        for i, char in enumerate(text):
            if char in ['{', '[']:
                start_idx = i
                break
        
        if start_idx == -1:
            raise Exception("æœªæ‰¾åˆ° JSON èµ·å§‹æ ‡è®°")
        
        return text[start_idx:].strip()


    def _save_cleaned_text(self, content: str, page_name: str = None) -> None:
        """
        ä¿å­˜æ¸…ç†åçš„HTMLæ–‡æœ¬åˆ°data/html/cleaned_textæ–‡ä»¶å¤¹
        
        Args:
            content: æ¸…ç†åçš„æ–‡æœ¬å†…å®¹
            page_name: é¡µé¢åç§°ï¼ˆå¯é€‰ï¼‰
        """
        try:
            # ç¡®å®šä¿å­˜ç›®å½•
            output_dir = os.path.join(
                os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
                'data', 'html', 'cleaned_text'
            )
            os.makedirs(output_dir, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆæ—¶é—´æˆ³ + é¡µé¢åç§°ï¼‰
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            if page_name:
                filename = f'cleaned_{page_name}_{timestamp}.txt'
            else:
                filename = f'cleaned_text_{timestamp}.txt'
            filepath = os.path.join(output_dir, filename)
            
            # ä¿å­˜æ–‡ä»¶
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            
            print(f"ğŸ’¾ å·²ä¿å­˜æ¸…ç†åçš„æ–‡æœ¬: {filepath}")
            print(f"   æ–‡æœ¬å¤§å°: {len(content)} å­—ç¬¦")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜æ¸…ç†æ–‡æœ¬å¤±è´¥: {str(e)}")
    
    def _save_response_to_file(self, content: str) -> None:
        """
        ä¿å­˜APIå“åº”åˆ°data/htmlæ–‡ä»¶å¤¹
        
        Args:
            content: APIè¿”å›çš„JSONå†…å®¹
        """
        try:
            # ç¡®å®šä¿å­˜ç›®å½•
            output_dir = os.path.join(
                os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
                'data', 'html', 'qwen_responses'
            )
            os.makedirs(output_dir, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆæ—¶é—´æˆ³ï¼‰
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:17]
            filename = f'qwen_response_{timestamp}.json'
            filepath = os.path.join(output_dir, filename)
            
            # ä¿å­˜æ–‡ä»¶
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            
            print(f"ğŸ’¾ å·²ä¿å­˜APIå“åº”: {filepath}")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜APIå“åº”å¤±è´¥: {str(e)}")
    
    def _save_toc(self, toc: List[Dict[str, str]], page_name: str = None) -> None:
        """
        ä¿å­˜ç›®å½•ç»“æ„åˆ°data/html/tocæ–‡ä»¶å¤¹
        
        Args:
            toc: ç›®å½•åˆ—è¡¨
            page_name: é¡µé¢åç§°ï¼ˆå¯é€‰ï¼‰
        """
        try:
            # ç¡®å®šä¿å­˜ç›®å½•
            output_dir = os.path.join(
                os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
                'data', 'html', 'toc'
            )
            os.makedirs(output_dir, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            if page_name:
                filename = f'toc_{page_name}_{timestamp}.json'
            else:
                filename = f'toc_{timestamp}.json'
            filepath = os.path.join(output_dir, filename)
            
            # ä¿å­˜ä¸ºJSONæ ¼å¼
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(toc, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“‘ å·²ä¿å­˜ç›®å½•ç»“æ„: {filepath}")
            print(f"   ç›®å½•æ¡ç›®æ•°: {len(toc)}")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜ç›®å½•ç»“æ„å¤±è´¥: {str(e)}")
    
    def _save_links(self, links: List[Dict[str, str]], page_name: str = None) -> None:
        """
        ä¿å­˜é“¾æ¥æ•°æ®åˆ°data/html/linksæ–‡ä»¶å¤¹
        
        Args:
            links: é“¾æ¥åˆ—è¡¨
            page_name: é¡µé¢åç§°ï¼ˆå¯é€‰ï¼‰
        """
        try:
            # ç¡®å®šä¿å­˜ç›®å½•
            output_dir = os.path.join(
                os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
                'data', 'html', 'links'
            )
            os.makedirs(output_dir, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            if page_name:
                filename = f'links_{page_name}_{timestamp}.json'
            else:
                filename = f'links_{timestamp}.json'
            filepath = os.path.join(output_dir, filename)
            
            # ç»Ÿè®¡é“¾æ¥ç±»å‹
            link_stats = {}
            for link in links:
                link_type = link.get('type', 'unknown')
                link_stats[link_type] = link_stats.get(link_type, 0) + 1
            
            # ä¿å­˜ä¸ºJSONæ ¼å¼
            data = {
                'total': len(links),
                'statistics': link_stats,
                'links': links
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ”— å·²ä¿å­˜é“¾æ¥æ•°æ®: {filepath}")
            print(f"   é“¾æ¥æ€»æ•°: {len(links)}")
            print(f"   é“¾æ¥åˆ†ç±»: {link_stats}")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜é“¾æ¥æ•°æ®å¤±è´¥: {str(e)}")