"""
æµ‹è¯•HTMLæ¸…ç†å™¨åŠŸèƒ½
"""
import os
import sys

# æ·»åŠ è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'local_llm'))

from html_cleaner import HTMLCleanerFactory


def test_html_cleaner():
    """æµ‹è¯•HTMLæ¸…ç†å™¨"""
    
    # æŸ¥æ‰¾ä¸€ä¸ªHTMLæ–‡ä»¶è¿›è¡Œæµ‹è¯•
    html_dir = os.path.join(
        os.path.dirname(os.path.abspath(__file__)),
        'data', 'html'
    )
    
    # æŸ¥æ‰¾æœ±å…ƒç’‹çš„HTMLæ–‡ä»¶
    html_file = None
    for root, dirs, files in os.walk(html_dir):
        for file in files:
            if 'æœ±å…ƒç’‹' in file and file.endswith('.html'):
                html_file = os.path.join(root, file)
                break
        if html_file:
            break
    
    if not html_file:
        print("âŒ æœªæ‰¾åˆ°æµ‹è¯•ç”¨çš„HTMLæ–‡ä»¶")
        return
    
    print(f"ğŸ“„ ä½¿ç”¨æµ‹è¯•æ–‡ä»¶: {html_file}")
    
    # è¯»å–HTMLå†…å®¹
    with open(html_file, 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    print(f"ğŸ“Š åŸå§‹HTMLå¤§å°: {len(html_content)} å­—ç¬¦")
    
    # åˆ›å»ºæ¸…ç†å™¨
    cleaner = HTMLCleanerFactory.create_cleaner('wikipedia')
    
    # æ¸…ç†HTML
    print("\nğŸ”§ å¼€å§‹æ¸…ç†HTML...")
    cleaned_content = cleaner.clean(html_content)
    
    # è¾“å‡ºç»“æœ
    print("\n" + "="*60)
    print("ğŸ“ æ¸…ç†åçš„æ–‡æœ¬")
    print("="*60)
    print(f"æ–‡æœ¬å¤§å°: {len(cleaned_content.text)} å­—ç¬¦")
    print(f"\nå‰500å­—ç¬¦:\n{cleaned_content.text[:500]}")
    
    print("\n" + "="*60)
    print("ğŸ“‘ ç›®å½•ç»“æ„")
    print("="*60)
    print(f"ç›®å½•æ¡ç›®æ•°: {len(cleaned_content.toc)}")
    for i, item in enumerate(cleaned_content.toc[:10], 1):
        print(f"{i}. [H{item['level']}] {item['title']} (id: {item['id']})")
    if len(cleaned_content.toc) > 10:
        print(f"... è¿˜æœ‰ {len(cleaned_content.toc) - 10} ä¸ªæ¡ç›®")
    
    print("\n" + "="*60)
    print("ğŸ”— é“¾æ¥æ•°æ®")
    print("="*60)
    print(f"é“¾æ¥æ€»æ•°: {len(cleaned_content.links)}")
    
    # ç»Ÿè®¡é“¾æ¥ç±»å‹
    link_stats = {}
    for link in cleaned_content.links:
        link_type = link['type']
        link_stats[link_type] = link_stats.get(link_type, 0) + 1
    
    print(f"\né“¾æ¥åˆ†ç±»ç»Ÿè®¡:")
    for link_type, count in sorted(link_stats.items(), key=lambda x: x[1], reverse=True):
        print(f"  {link_type}: {count}")
    
    print(f"\nå‰10ä¸ªé“¾æ¥ç¤ºä¾‹:")
    for i, link in enumerate(cleaned_content.links[:10], 1):
        print(f"{i}. [{link['type']}] {link['text']} -> {link['href']}")
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")


if __name__ == '__main__':
    test_html_cleaner()
