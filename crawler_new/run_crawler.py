#!/usr/bin/env python
"""
è¿è¡Œçˆ¬è™«çš„ä¾¿æ·è„šæœ¬
"""

import sys
import os
from pathlib import Path

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings


def run_crawler(spider_name='ming_emperor', mode='test'):
    """
    è¿è¡Œçˆ¬è™«
    
    Args:
        spider_name: çˆ¬è™«åç§°ï¼Œé»˜è®¤ 'ming_emperor'
        mode: çˆ¬å–æ¨¡å¼ï¼Œå¯é€‰ 'test', 'full'
    """
    # è®¾ç½®å·¥ä½œç›®å½•
    os.chdir(project_root)
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„
    required_dirs = [
        'crawler_new/data/logs',
        'crawler_new/data/html',
        'crawler_new/data/httpcache',
    ]
    for dir_path in required_dirs:
        Path(dir_path).mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½é…ç½®
    settings = get_project_settings()
    settings.setmodule('crawler_new.config.settings')
    
    # è¦†ç›–éƒ¨åˆ†é…ç½®
    settings.set('CRAWL_MODE', mode)
    
    # åˆ›å»ºçˆ¬è™«è¿›ç¨‹
    process = CrawlerProcess(settings)
    
    # å¯åŠ¨çˆ¬è™«
    print(f"ğŸš€ å¯åŠ¨çˆ¬è™«: {spider_name}")
    print(f"   æ•°æ®æº: Wikipediaï¼ˆåªçˆ¬å–ç»´åŸºç™¾ç§‘ï¼‰")
    print(f"   æ¨¡å¼: {mode}")
    print(f"{'='*80}\n")
    
    process.crawl(spider_name)
    process.start()


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='è¿è¡Œ crawler_new çˆ¬è™«ï¼ˆåªçˆ¬å– Wikipediaï¼‰')
    parser.add_argument('--spider', default='ming_emperor', help='çˆ¬è™«åç§°')
    parser.add_argument('--mode', default='test', choices=['test', 'full'], help='çˆ¬å–æ¨¡å¼')
    
    args = parser.parse_args()
    
    run_crawler(
        spider_name=args.spider,
        mode=args.mode
    )
