#!/usr/bin/env python
"""
è¿è¡Œçˆ¬è™«çš„ä¾¿æ·è„šæœ¬
"""

import sys
import os
from pathlib import Path

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings


def run_crawler(spider_name='ming_emperor', source='wikipedia', mode='test'):
    """
    è¿è¡Œçˆ¬è™«
    
    Args:
        spider_name: çˆ¬è™«åç§°ï¼Œé»˜è®¤ 'ming_emperor'
        source: æ•°æ®æºï¼Œå¯é€‰ 'wikipedia', 'baidu', 'both'
        mode: çˆ¬å–æ¨¡å¼ï¼Œå¯é€‰ 'test', 'full'
    """
    # è®¾ç½®å·¥ä½œç›®å½•
    os.chdir(project_root)
    
    # åŠ è½½é…ç½®
    settings = get_project_settings()
    settings.setmodule('crawler_new.config.settings')
    
    # è¦†ç›–éƒ¨åˆ†é…ç½®
    settings.set('CRAWL_MODE', mode)
    
    # åˆ›å»ºçˆ¬è™«è¿›ç¨‹
    process = CrawlerProcess(settings)
    
    # å¯åŠ¨çˆ¬è™«
    print(f"ğŸš€ å¯åŠ¨çˆ¬è™«: {spider_name}")
    print(f"   æ•°æ®æº: {source}")
    print(f"   æ¨¡å¼: {mode}")
    print(f"{'='*80}\n")
    
    process.crawl(spider_name, source=source)
    process.start()


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='è¿è¡Œ crawler_new çˆ¬è™«')
    parser.add_argument('--spider', default='ming_emperor', help='çˆ¬è™«åç§°')
    parser.add_argument('--source', default='wikipedia', choices=['wikipedia', 'baidu', 'both'], help='æ•°æ®æº')
    parser.add_argument('--mode', default='test', choices=['test', 'full'], help='çˆ¬å–æ¨¡å¼')
    
    args = parser.parse_args()
    
    run_crawler(
        spider_name=args.spider,
        source=args.source,
        mode=args.mode
    )
