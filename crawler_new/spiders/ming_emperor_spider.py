"""
æ˜æœçš‡å¸çˆ¬è™« - åŸºäºåƒé—®å¤§æ¨¡å‹çš„æ™ºèƒ½åŒ–çˆ¬è™«
çˆ¬å– Wikipedia å’Œç™¾åº¦ç™¾ç§‘çš„ HTML é¡µé¢ï¼Œç„¶åç”±åƒé—®å¤§æ¨¡å‹è¿›è¡Œç»“æ„åŒ–æå–
"""

import scrapy
from datetime import datetime
from typing import Optional
from urllib.parse import urljoin

from crawler_new.models.items import HtmlPageItem, LinkItem
from crawler_new.config.ming_data import MING_EMPERORS, MING_DYNASTY


class MingEmperorSpider(scrapy.Spider):
    """æ˜æœçš‡å¸çˆ¬è™«"""
    
    name = 'ming_emperor'
    
    # å…è®¸çš„åŸŸå
    allowed_domains = ['zh.wikipedia.org', 'baike.baidu.com']
    
    custom_settings = {
        'DOWNLOAD_DELAY': 3,
        'RANDOMIZE_DOWNLOAD_DELAY': True,
        'CONCURRENT_REQUESTS': 4,
    }
    
    def __init__(self, source='both', *args, **kwargs):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            source: æ•°æ®æºé€‰æ‹©ï¼Œå¯é€‰å€¼ï¼š'wikipedia', 'baidu', 'both'ï¼ˆé»˜è®¤ï¼‰
        """
        super().__init__(*args, **kwargs)
        self.data_source = source
        self.crawled_urls = set()  # é˜²æ­¢é‡å¤çˆ¬å–
        
    def start_requests(self):
        """ç”Ÿæˆèµ·å§‹è¯·æ±‚"""
        # ä» settings ä¸­è·å–çˆ¬å–æ¨¡å¼é…ç½®
        crawl_mode = self.settings.get('CRAWL_MODE', 'test')
        test_emperor_count = self.settings.get('TEST_EMPEROR_COUNT', 3)
        
        self.logger.info(f"\n{'='*100}")
        self.logger.info(f"ğŸš€ [çˆ¬è™«å¯åŠ¨] Spider: {self.name}")
        self.logger.info(f"   æ•°æ®æº: {self.data_source}")
        self.logger.info(f"   çˆ¬å–æ¨¡å¼: {crawl_mode}")
        self.logger.info(f"{'='*100}\n")
        
        # æ ¹æ®çˆ¬å–æ¨¡å¼å†³å®šçˆ¬å–å¤šå°‘ä½çš‡å¸
        emperors_to_crawl = MING_EMPERORS
        if crawl_mode == 'test':
            emperors_to_crawl = MING_EMPERORS[:test_emperor_count]
            self.logger.info(f"ğŸ“‹ [çˆ¬å–èŒƒå›´] æµ‹è¯•æ¨¡å¼ï¼šåªçˆ¬å–å‰ {test_emperor_count} ä½çš‡å¸")
        else:
            self.logger.info(f"ğŸ“‹ [çˆ¬å–èŒƒå›´] å…¨é‡æ¨¡å¼ï¼šçˆ¬å–æ‰€æœ‰ {len(MING_EMPERORS)} ä½çš‡å¸")
        
        self.logger.info(f"ğŸ“Š [ç»Ÿè®¡] å¾…çˆ¬å–çš‡å¸: {len(emperors_to_crawl)} ä½")
        for idx, emp in enumerate(emperors_to_crawl, 1):
            self.logger.info(f"   {idx}. {emp['name']} ({emp['temple_name']}) - {emp['reign_title']}")
        self.logger.info("")
        
        # çˆ¬å–çš‡å¸ä¿¡æ¯
        request_count = 0
        for emperor_info in emperors_to_crawl:
            # æ ¹æ® source å‚æ•°å†³å®šçˆ¬å–å“ªä¸ªæ•°æ®æº
            if self.data_source in ['wikipedia', 'both']:
                request_count += 1
                yield self._create_request(
                    url=emperor_info['wikipedia_url'],
                    emperor_info=emperor_info,
                    data_source='wikipedia'
                )
            
            if self.data_source in ['baidu', 'both']:
                request_count += 1
                yield self._create_request(
                    url=emperor_info['baidu_url'],
                    emperor_info=emperor_info,
                    data_source='baidu'
                )
        
        self.logger.info(f"âœ… [è¯·æ±‚ç”Ÿæˆ] å…±ç”Ÿæˆ {request_count} ä¸ªçˆ¬å–è¯·æ±‚\n")
    
    def _create_request(self, url: str, emperor_info: dict, data_source: str):
        """åˆ›å»ºè¯·æ±‚"""
        self.logger.info(f"\n{'='*80}")
        self.logger.info(f"ğŸ‘‘ [è¯·æ±‚åˆ›å»º] çš‡å¸: {emperor_info['name']} ({data_source})")
        self.logger.info(f"   URL: {url}")
        self.logger.info(f"   æœä»£é¡ºåº: {emperor_info.get('dynasty_order')}")
        self.logger.info(f"   åº™å·: {emperor_info.get('temple_name')}")
        self.logger.info(f"   å¹´å·: {emperor_info.get('reign_title')}")
        self.logger.info(f"{'='*80}")
        
        return scrapy.Request(
            url=url,
            callback=self.parse_emperor,
            meta={
                'emperor_info': emperor_info,
                'data_source': data_source,
                'page_type': 'emperor',
                'depth': 0  # é€’å½’æ·±åº¦
            },
            dont_filter=True
        )
    
    def parse_emperor(self, response):
        """è§£æçš‡å¸é¡µé¢ - åªä¿å­˜ HTMLï¼Œä¸åšè§£æ"""
        emperor_info = response.meta['emperor_info']
        data_source = response.meta['data_source']
        page_name = emperor_info['name']
        
        self.logger.info(f"\n{'='*80}")
        self.logger.info(f"âœ… [HTTPå“åº”] æˆåŠŸè·å–HTML")
        self.logger.info(f"   çš‡å¸: {page_name}")
        self.logger.info(f"   æ•°æ®æº: {data_source}")
        self.logger.info(f"   çŠ¶æ€ç : {response.status}")
        self.logger.info(f"   HTMLå¤§å°: {len(response.text)} å­—ç¬¦")
        self.logger.info(f"{'='*80}")
        
        # æ ‡è®°å·²çˆ¬å–
        self.crawled_urls.add(response.url)
        
        # ç”Ÿæˆé¡µé¢ID
        page_id = f"ming_emperor_{emperor_info['dynasty_order']:03d}_{data_source}"
        
        self.logger.info(f"ğŸ“¦ [Itemåˆ›å»º] ç”Ÿæˆ HtmlPageItem")
        self.logger.info(f"   page_id: {page_id}")
        self.logger.info(f"   page_type: emperor")
        self.logger.info(f"   page_name: {page_name}")
        
        # åˆ›å»º HtmlPageItem
        html_item = HtmlPageItem(
            page_type='emperor',
            page_id=page_id,
            page_name=page_name,
            data_source=data_source,
            source_url=response.url,
            html_content=response.text,
            metadata={
                'temple_name': emperor_info.get('temple_name'),
                'reign_title': emperor_info.get('reign_title'),
                'dynasty_order': emperor_info['dynasty_order'],
                'reign_years': emperor_info.get('reign_years'),
                'dynasty_id': MING_DYNASTY['dynasty_id']
            },
            crawl_time=datetime.now().isoformat()
        )
        
        self.logger.info(f"â¡ï¸  [Pipeline] æäº¤ HtmlPageItem åˆ° Pipeline å¤„ç†é“¾\n")
        
        # æäº¤ Item åˆ° Pipeline
        yield html_item
    
    def parse_event(self, response):
        """è§£æäº‹ä»¶é¡µé¢"""
        event_name = response.meta['event_name']
        data_source = response.meta['data_source']
        depth = response.meta.get('depth', 1)
        
        # é˜²æ­¢é‡å¤çˆ¬å–
        if response.url in self.crawled_urls:
            self.logger.info(f"âš ï¸  [å»é‡] äº‹ä»¶é¡µé¢å·²çˆ¬å–ï¼Œè·³è¿‡: {event_name}")
            return
        
        self.logger.info(f"\n{'='*80}")
        self.logger.info(f"ğŸ“° [äº‹ä»¶çˆ¬å–] æˆåŠŸè·å–äº‹ä»¶HTML")
        self.logger.info(f"   äº‹ä»¶: {event_name}")
        self.logger.info(f"   æ•°æ®æº: {data_source}")
        self.logger.info(f"   é€’å½’æ·±åº¦: {depth}")
        self.logger.info(f"   çŠ¶æ€ç : {response.status}")
        self.logger.info(f"   HTMLå¤§å°: {len(response.text)} å­—ç¬¦")
        self.logger.info(f"{'='*80}")
        
        self.crawled_urls.add(response.url)
        
        # ç”Ÿæˆé¡µé¢ID
        page_id = f"ming_event_{event_name}_{data_source}"
        
        # åˆ›å»º HtmlPageItem
        html_item = HtmlPageItem(
            page_type='event',
            page_id=page_id,
            page_name=event_name,
            data_source=data_source,
            source_url=response.url,
            html_content=response.text,
            metadata={
                'dynasty_id': MING_DYNASTY['dynasty_id'],
                'depth': depth,
                'source_page': response.meta.get('source_page', '')
            },
            crawl_time=datetime.now().isoformat()
        )
        
        yield html_item
    
    def parse_person(self, response):
        """è§£æäººç‰©é¡µé¢"""
        person_name = response.meta['person_name']
        data_source = response.meta['data_source']
        depth = response.meta.get('depth', 1)
        
        # é˜²æ­¢é‡å¤çˆ¬å–
        if response.url in self.crawled_urls:
            self.logger.info(f"âš ï¸  [å»é‡] äººç‰©é¡µé¢å·²çˆ¬å–ï¼Œè·³è¿‡: {person_name}")
            return
        
        self.logger.info(f"\n{'='*80}")
        self.logger.info(f"ğŸ‘¤ [äººç‰©çˆ¬å–] æˆåŠŸè·å–äººç‰©HTML")
        self.logger.info(f"   äººç‰©: {person_name}")
        self.logger.info(f"   æ•°æ®æº: {data_source}")
        self.logger.info(f"   é€’å½’æ·±åº¦: {depth}")
        self.logger.info(f"   çŠ¶æ€ç : {response.status}")
        self.logger.info(f"   HTMLå¤§å°: {len(response.text)} å­—ç¬¦")
        self.logger.info(f"{'='*80}")
        
        self.crawled_urls.add(response.url)
        
        # ç”Ÿæˆé¡µé¢ID
        page_id = f"ming_person_{person_name}_{data_source}"
        
        # åˆ›å»º HtmlPageItem
        html_item = HtmlPageItem(
            page_type='person',
            page_id=page_id,
            page_name=person_name,
            data_source=data_source,
            source_url=response.url,
            html_content=response.text,
            metadata={
                'dynasty_id': MING_DYNASTY['dynasty_id'],
                'depth': depth,
                'source_page': response.meta.get('source_page', '')
            },
            crawl_time=datetime.now().isoformat()
        )
        
        yield html_item
