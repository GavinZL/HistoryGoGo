# Crawler_new 日志指南

## 📋 日志概述

crawler_new 在整个数据获取流程中添加了完整的关键日志，方便追踪每个环节的执行状态和调试问题。

## 🔍 日志级别

- **INFO**: 正常流程日志（默认显示）
- **WARNING**: 警告信息（跳过、缺失配置等）
- **ERROR**: 错误信息（失败、异常等）
- **DEBUG**: 调试详细信息（需手动启用）

## 📊 完整数据流程日志

### 1. **爬虫启动** (Spider)

```
================================================================================
🚀 [爬虫启动] Spider: ming_emperor
   数据源: both
   爬取模式: test
================================================================================

📋 [爬取范围] 测试模式：只爬取前 3 位皇帝
📊 [统计] 待爬取皇帝: 3 位
   1. 朱元璋 (明太祖) - 洪武
   2. 朱允炆 (明惠帝) - 建文
   3. 朱棣 (明成祖) - 永乐

✅ [请求生成] 共生成 6 个爬取请求
```

---

### 2. **HTTP 请求** (Spider)

#### 创建请求
```
================================================================================
👑 [请求创建] 皇帝: 朱元璋 (wikipedia)
   URL: https://zh.wikipedia.org/wiki/朱元璋
   朝代顺序: 1
   庙号: 明太祖
   年号: 洪武
================================================================================
```

#### HTTP 响应
```
================================================================================
✅ [HTTP响应] 成功获取HTML
   皇帝: 朱元璋
   数据源: wikipedia
   状态码: 200
   HTML大小: 156234 字符
================================================================================

📦 [Item创建] 生成 HtmlPageItem
   page_id: ming_emperor_001_wikipedia
   page_type: emperor
   page_name: 朱元璋

➡️  [Pipeline] 提交 HtmlPageItem 到 Pipeline 处理链
```

---

### 3. **Pipeline-1: HTML 存储**

#### Pipeline 启动
```
====================================================================================================
📁 [Pipeline-1] HtmlStoragePipeline 启动
   存储路径: crawler_new/data/html
   ✅ 目录已就绪: crawler_new/data/html/emperor
   ✅ 目录已就绪: crawler_new/data/html/event
   ✅ 目录已就绪: crawler_new/data/html/person
====================================================================================================
```

#### 处理 Item
```
================================================================================
💾 [Pipeline-1] HTML存储开始
   page_id: ming_emperor_001_wikipedia
   page_name: 朱元璋
   data_source: wikipedia
   HTML大小: 156234 字符
   ✅ HTML文件: crawler_new/data/html/emperor/ming_emperor_001_wikipedia.html
   ✅ 元数据文件: crawler_new/data/html/emperor/ming_emperor_001_wikipedia_metadata.json
✅ [Pipeline-1] HTML存储完成
================================================================================
```

---

### 4. **Pipeline-2: 千问大模型提取**

#### Pipeline 启动
```
====================================================================================================
🤖 [Pipeline-2] QwenExtractionPipeline 启动
   ✅ 千问大模型已初始化
   模型: qwen-max
   API Key: sk-1234567...
====================================================================================================
```

#### 缓存 HTML（等待双源）
```
================================================================================
🤖 [Pipeline-2] 千问提取开始
   page_id: ming_emperor_001_wikipedia
   page_name: 朱元璋
   data_source: wikipedia
   💾 缓存HTML: 朱元璋 (wikipedia)
   📋 数据源状态: Wikipedia=✅, Baidu=❌
   ⏳ 等待另一个数据源完成...
   已有: wikipedia
================================================================================
```

#### 双源完成，开始提取
```
================================================================================
🤖 [Pipeline-2] 千问提取开始
   page_id: ming_emperor_001_baidu
   page_name: 朱元璋
   data_source: baidu
   💾 缓存HTML: 朱元璋 (baidu)
   📋 数据源状态: Wikipedia=✅, Baidu=✅
   ✅ 双源已完成，开始融合提取
================================================================================
```

#### 大模型提取过程
```
================================================================================
🤖 [大模型提取] 开始提取皇帝信息
   皇帝: 朱元璋
   Wikipedia HTML: 156234 字符
   Baidu HTML: 142567 字符
================================================================================

📑 [Step 1] 调用大模型提取皇帝基本信息...
   ✅ 皇帝信息提取完成
   皇帝: 朱元璋
   庙号: 明太祖
   年号: 洪武
   出生: 1328年10月21日（元天历元年九月十八日）
   去世: 1398年6月24日（洪武三十一年闰五月初十）

📜 [Step 2] 调用大模型提取生平事迹...
   ✅ 生平事迹提取完成: 18 条
      1. 1328年10月29日（元天历元年九月十八日） - 出生于贫农家庭，原名朱重八...
      2. 1344年（至正四年） - 淮北大旱，父母兄长相继去世...
      3. 1352年 - 受儿时好友汤和邀请投奔郭子兴...
      ... 还有 15 条事迹

🔗 [Step 3] 提取链接信息...
   ✅ 链接提取完成
   事件链接: 5 个
   人物链接: 12 个

📦 [Step 4] 创建 ExtractedDataItem
   ✅ ExtractedDataItem 创建完成
================================================================================

================================================================================
✅ [Pipeline-2] 千问提取完成: 朱元璋
================================================================================
```

---

### 5. **Pipeline-3: 数据验证**
```
================================================================================
✅ [Pipeline-3] 数据验证通过: ming_emperor_001
   皇帝: 朱元璋, 事迹数: 18
================================================================================
```

---

### 6. **Pipeline-4/5: 数据库存储**
```
💾 [Pipeline-4] SQLite存储: ming_emperor_001（待实现）
🔗 [Pipeline-5] Neo4j存储: ming_emperor_001（待实现）
```

---

### 7. **Pipeline-6: 递归爬取**
```
================================================================================
🔄 [Pipeline-6] 递归爬取开始
   page_id: ming_emperor_001
   🔗 发现 17 个链接，准备递归爬取（深度: 1）
   📥 添加递归请求: person - 汤和（深度: 1）
   📥 添加递归请求: event - 胡惟庸案（深度: 1）
   ...
================================================================================
```

---

## 🛠️ 错误日志示例

### HTTP 请求失败
```
❌ [HTTP错误] 请求失败
   URL: https://zh.wikipedia.org/wiki/...
   状态码: 404
   错误: Not Found
```

### HTML 存储失败
```
================================================================================
❌ [Pipeline-1] HTML存储失败
   page_id: ming_emperor_001_wikipedia
   错误: Permission denied
================================================================================
```

### 千问 API 调用失败
```
================================================================================
❌ [Pipeline-2] 千问提取失败
   page_id: ming_emperor_001
   错误: API rate limit exceeded
================================================================================
```

---

## 📝 日志配置

### 修改日志级别

编辑 `config/settings.py`:

```python
# 显示所有日志（包括DEBUG）
LOG_LEVEL = 'DEBUG'

# 只显示重要日志
LOG_LEVEL = 'INFO'

# 只显示错误
LOG_LEVEL = 'ERROR'
```

### 日志文件位置

```
crawler_new/data/logs/crawler.log
```

### 查看实时日志

```bash
# 方式1：运行时直接查看
python run_crawler.py --source both --mode test

# 方式2：查看日志文件
tail -f crawler_new/data/logs/crawler.log

# 方式3：查看最近100行
tail -n 100 crawler_new/data/logs/crawler.log
```

---

## 🔍 日志过滤技巧

### 只查看错误日志
```bash
grep "❌" crawler_new/data/logs/crawler.log
```

### 只查看某个皇帝的日志
```bash
grep "朱元璋" crawler_new/data/logs/crawler.log
```

### 只查看 Pipeline 日志
```bash
grep "Pipeline" crawler_new/data/logs/crawler.log
```

### 只查看大模型提取日志
```bash
grep "大模型提取" crawler_new/data/logs/crawler.log
```

---

## 📊 日志符号说明

| 符号 | 含义 | 使用场景 |
|------|------|----------|
| 🚀 | 启动 | 爬虫启动 |
| 👑 | 皇帝 | 皇帝相关操作 |
| 📰 | 事件 | 事件相关操作 |
| 👤 | 人物 | 人物相关操作 |
| 📁 | 文件 | 文件存储 |
| 💾 | 缓存 | HTML 缓存 |
| 🤖 | AI | 大模型提取 |
| 🔗 | 链接 | 链接提取/递归爬取 |
| ✅ | 成功 | 操作成功 |
| ❌ | 失败 | 操作失败 |
| ⚠️ | 警告 | 警告信息 |
| ⏳ | 等待 | 等待其他资源 |
| 📋 | 统计 | 统计信息 |
| 📊 | 数据 | 数据统计 |
| 📦 | 打包 | Item 创建 |
| ➡️ | 流转 | 数据流转 |

---

## 💡 调试技巧

### 1. 追踪单个皇帝的完整流程

```bash
python run_crawler.py --source both --mode test 2>&1 | grep "朱元璋"
```

### 2. 检查是否有错误

```bash
python run_crawler.py --source both --mode test 2>&1 | grep "❌"
```

### 3. 查看 Pipeline 执行顺序

```bash
python run_crawler.py --source both --mode test 2>&1 | grep "Pipeline"
```

### 4. 监控双源融合状态

```bash
python run_crawler.py --source both --mode test 2>&1 | grep "数据源状态"
```

---

## 📈 性能监控

通过日志可以监控：

1. **HTML 下载速度**: 查看 HTTP 响应时间
2. **大模型调用次数**: 查看 Pipeline-2 日志
3. **缓存命中率**: 查看缓存日志
4. **递归深度**: 查看递归爬取日志

---

## 🎯 最佳实践

1. **运行前检查日志配置**: 确保 `LOG_LEVEL` 合适
2. **定期清理日志文件**: 避免日志文件过大
3. **保存重要日志**: 出现问题时保存完整日志
4. **使用日志过滤**: 快速定位问题

---

**更新时间**: 2025-12-14  
**版本**: v1.2  
**新增内容**: 完整数据流程关键日志
