"""
åƒé—®å¤§æ¨¡å‹æå– Pipeline
ä½¿ç”¨é€šä¹‰åƒé—®å¤„ç† HTML å¹¶æå–ç»“æ„åŒ–æ•°æ®
"""

from datetime import datetime
from typing import Dict, Any, List

from crawler_new.models.items import HtmlPageItem, ExtractedDataItem
from crawler_new.utils.qwen_extractor import QwenExtractor
from crawler_new.local_llm.local_extractor import LocalLLMExtractor


class QwenExtractionPipeline:
    """åƒé—®å¤§æ¨¡å‹æå–Pipeline"""
    
    def __init__(self, api_key: str, model: str, use_local_llm: bool = False, local_llm_model: str = '', local_llm_base_url: str = ''):
        self.api_key = api_key
        self.model = model
        self.use_local_llm = use_local_llm
        self.local_llm_model = local_llm_model
        self.local_llm_base_url = local_llm_base_url
        self.extractor = None
        self.html_cache = {}  # ç¼“å­˜å·²çˆ¬å–çš„HTMLï¼Œç”¨äºåŒæºèåˆ
    
    @classmethod
    def from_crawler(cls, crawler):
        # è¯»å–é…ç½®
        use_local_llm = crawler.settings.get('USE_LOCAL_LLM', False)
        api_key = crawler.settings.get('QWEN_API_KEY', '')
        model = crawler.settings.get('QWEN_MODEL', 'qwen-max')
        local_llm_model = crawler.settings.get('LOCAL_LLM_MODEL', 'qwen2.5:7b')
        local_llm_base_url = crawler.settings.get('LOCAL_LLM_BASE_URL', 'http://localhost:11434')
        return cls(api_key, model, use_local_llm, local_llm_model, local_llm_base_url)
    
    def open_spider(self, spider):
        """Spider å¼€å¯æ—¶åˆå§‹åŒ–æå–å™¨"""
        spider.logger.info(f"\n{'='*100}")
        spider.logger.info(f"ğŸ¤– [Pipeline-2] QwenExtractionPipeline å¯åŠ¨")
        
        # åˆ¤æ–­ä½¿ç”¨å“ªç§å¤§æ¨¡å‹
        if self.use_local_llm:
            # ä½¿ç”¨æœ¬åœ°å¤§æ¨¡å‹
            try:
                self.extractor = LocalLLMExtractor(self.local_llm_model, self.local_llm_base_url)
                spider.logger.info(f"   âœ… æœ¬åœ°å¤§æ¨¡å‹å·²åˆå§‹åŒ–")
                spider.logger.info(f"   æ¨¡å‹: {self.local_llm_model}")
                spider.logger.info(f"   APIåœ°å€: {self.local_llm_base_url}")
                spider.logger.info(f"   ä¼˜åŠ¿: æ— å­—ç¬¦é™åˆ¶ï¼Œå®Œæ•´HTMLå¤„ç†")
            except Exception as e:
                spider.logger.error(f"   âŒ æœ¬åœ°å¤§æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                spider.logger.warning(f"   æç¤ºï¼šè¯·ç¡®ä¿ Ollama æœåŠ¡å·²å¯åŠ¨ (ollama serve)")
                self.extractor = None
        else:
            # ä½¿ç”¨ API
            if not self.api_key:
                spider.logger.warning(f"   âš ï¸  QWEN_API_KEY æœªé…ç½®ï¼Œå°†è·³è¿‡å¤§æ¨¡å‹æå–")
                spider.logger.warning(f"   æç¤ºï¼šè¯·åœ¨ config/settings.py ä¸­é…ç½® QWEN_API_KEY")
                self.extractor = None
            else:
                self.extractor = QwenExtractor(self.api_key, self.model)
                spider.logger.info(f"   âœ… åƒé—® API å·²åˆå§‹åŒ–")
                spider.logger.info(f"   æ¨¡å‹: {self.model}")
                spider.logger.info(f"   API Key: {self.api_key[:10]}...")
                spider.logger.info(f"   æ³¨æ„: å­˜åœ¨å­—ç¬¦é™åˆ¶")
        
        spider.logger.info(f"{'='*100}\n")
    
    def process_item(self, item, spider):
        """å¤„ç† Item"""
        # åªå¤„ç† HtmlPageItem
        if not isinstance(item, HtmlPageItem):
            return item
        
        # å¦‚æœ API Key æœªé…ç½®ï¼Œè·³è¿‡æå–
        if not self.extractor:
            spider.logger.warning(f"âš ï¸  [è·³è¿‡] åƒé—®æå–: {item['page_id']}ï¼ˆAPI Key æœªé…ç½®ï¼‰")
            return item
        
        try:
            # æ ¹æ®é¡µé¢ç±»å‹å¤„ç†
            page_type = item['page_type']
            page_name = item['page_name']
            data_source = item['data_source']
            
            spider.logger.info(f"\n{'='*80}")
            spider.logger.info(f"ğŸ¤– [Pipeline-2] åƒé—®æå–å¼€å§‹")
            spider.logger.info(f"   page_id: {item['page_id']}")
            spider.logger.info(f"   page_name: {page_name}")
            spider.logger.info(f"   data_source: {data_source}")
            
            # å…ˆç¼“å­˜HTMLï¼Œç­‰å¾…åŒæºéƒ½çˆ¬å–å®Œæ¯•åå†å¤„ç†
            cache_key = f"{page_type}_{page_name}"
            
            if cache_key not in self.html_cache:
                self.html_cache[cache_key] = {}
            
            # å­˜å‚¨å½“å‰æ•°æ®æºçš„HTML
            self.html_cache[cache_key][data_source] = item['html_content']
            
            spider.logger.info(f"   ğŸ’¾ ç¼“å­˜HTML: {page_name} ({data_source})")
            
            # æ£€æŸ¥æ˜¯å¦åŒæºéƒ½å·²å®Œæˆ
            has_wikipedia = 'wikipedia' in self.html_cache[cache_key]
            has_baidu = 'baidu' in self.html_cache[cache_key]
            
            spider.logger.info(f"   ğŸ“‹ æ•°æ®æºçŠ¶æ€: Wikipedia={'âœ…' if has_wikipedia else 'âŒ'}, Baidu={'âœ…' if has_baidu else 'âŒ'}")
            
            # å¦‚æœåŒæºéƒ½å­˜åœ¨ï¼Œæ‰§è¡Œæå–
            if has_wikipedia and has_baidu:
                spider.logger.info(f"   âœ… åŒæºå·²å®Œæˆï¼Œå¼€å§‹èåˆæå–")
                spider.logger.info(f"{'='*80}\n")
                
                html_wiki = self.html_cache[cache_key].get('wikipedia', '')
                html_baidu = self.html_cache[cache_key].get('baidu', '')
                
                if page_type == 'emperor':
                    extracted_item = self._extract_emperor_dual_source(item, html_wiki, html_baidu, spider)
                elif page_type == 'event':
                    extracted_item = self._extract_event(item, spider)
                elif page_type == 'person':
                    extracted_item = self._extract_person(item, spider)
                else:
                    spider.logger.warning(f"âš ï¸  æœªçŸ¥é¡µé¢ç±»å‹: {page_type}")
                    return item
                
                spider.logger.info(f"\n{'='*80}")
                spider.logger.info(f"âœ… [Pipeline-2] åƒé—®æå–å®Œæˆ: {page_name}")
                spider.logger.info(f"{'='*80}\n")
                
                # æ¸…ç†ç¼“å­˜
                del self.html_cache[cache_key]
                
                return extracted_item
            else:
                # åªæœ‰ä¸€ä¸ªæ•°æ®æºï¼Œç­‰å¾…å¦ä¸€ä¸ª
                spider.logger.info(f"   â³ ç­‰å¾…å¦ä¸€ä¸ªæ•°æ®æºå®Œæˆ...")
                spider.logger.info(f"   å·²æœ‰: {', '.join(self.html_cache[cache_key].keys())}")
                spider.logger.info(f"{'='*80}\n")
                return item
                
        except Exception as e:
            spider.logger.error(f"\n{'='*80}")
            spider.logger.error(f"âŒ [Pipeline-2] åƒé—®æå–å¤±è´¥")
            spider.logger.error(f"   page_id: {item['page_id']}")
            spider.logger.error(f"   é”™è¯¯: {str(e)}")
            spider.logger.error(f"{'='*80}\n")
            import traceback
            spider.logger.debug(traceback.format_exc())
            return item
    
    def _extract_emperor_dual_source(self, html_item: HtmlPageItem, html_wiki: str, html_baidu: str, spider) -> ExtractedDataItem:
        """æå–çš‡å¸ä¿¡æ¯ï¼ˆåŒæºèåˆï¼‰"""
        page_name = html_item['page_name']
        
        spider.logger.info(f"\n{'='*80}")
        spider.logger.info(f"ğŸ¤– [å¤§æ¨¡å‹æå–] å¼€å§‹æå–çš‡å¸ä¿¡æ¯")
        spider.logger.info(f"   çš‡å¸: {page_name}")
        spider.logger.info(f"   Wikipedia HTML: {len(html_wiki)} å­—ç¬¦")
        spider.logger.info(f"   Baidu HTML: {len(html_baidu)} å­—ç¬¦")
        spider.logger.info(f"   æå–æ¨¡å¼: ä¸€æ¬¡æ€§æå–ï¼ˆåŸºæœ¬ä¿¡æ¯ + ç”Ÿå¹³äº‹è¿¹ï¼‰")
        spider.logger.info(f"   ä¼ è¾“é™åˆ¶: Wiki 10000å­—ç¬¦ + Baidu 10000å­—ç¬¦")
        spider.logger.info(f"{'='*80}")
        
        # ä½¿ç”¨æ–°çš„ä¸€æ¬¡æ€§æå–æ–¹æ³•
        spider.logger.info(f"\nğŸš€ [å¤§æ¨¡å‹è°ƒç”¨] ä¸€æ¬¡æ€§æå–æ‰€æœ‰æ•°æ®...")
        
        try:
            # 1. ä¸€æ¬¡æ€§æå–æ‰€æœ‰æ•°æ®
            result = self.extractor.extract_emperor_all_data(
                html_content_wiki=html_wiki,
                html_content_baidu=html_baidu,
                page_name=page_name
            )
            
            emperor_info = result.get('emperor_info', {})
            events = result.get('events', [])
            
            spider.logger.info(f"   âœ… æ•°æ®æå–å®Œæˆ")
            spider.logger.info(f"\nğŸ“‘ [åŸºæœ¬ä¿¡æ¯]")
            spider.logger.info(f"   çš‡å¸: {emperor_info.get('çš‡å¸')}")
            spider.logger.info(f"   åº™å·: {emperor_info.get('åº™å·')}")
            spider.logger.info(f"   å¹´å·: {emperor_info.get('å¹´å·')}")
            spider.logger.info(f"   å‡ºç”Ÿ: {emperor_info.get('å‡ºç”Ÿ')}")
            spider.logger.info(f"   å»ä¸–: {emperor_info.get('å»ä¸–')}")
            
            spider.logger.info(f"\nğŸ“œ [ç”Ÿå¹³äº‹è¿¹] æå–å®Œæˆ: {len(events)} æ¡")
            for idx, event in enumerate(events[:3], 1):
                spider.logger.info(f"      {idx}. {event.get('æ—¶é—´')} - {event.get('äº‹ä»¶', '')[:30]}...")
            if len(events) > 3:
                spider.logger.info(f"      ... è¿˜æœ‰ {len(events) - 3} æ¡äº‹è¿¹")
            
        except Exception as e:
            # å¦‚æœä¸€æ¬¡æ€§æå–å¤±è´¥ï¼Œé™çº§ä¸ºåˆ†æ¬¡æå–
            spider.logger.warning(f"   âš ï¸  ä¸€æ¬¡æ€§æå–å¤±è´¥: {str(e)}")
            spider.logger.warning(f"   ğŸ”„ é™çº§ä¸ºåˆ†æ¬¡æå–æ¨¡å¼...")
            
            # 1. æå–çš‡å¸åŸºæœ¬ä¿¡æ¯
            spider.logger.info(f"\nğŸ“‘ [Step 1] è°ƒç”¨å¤§æ¨¡å‹æå–çš‡å¸åŸºæœ¬ä¿¡æ¯...")
            emperor_info = self.extractor.extract_emperor_info(
                html_content_wiki=html_wiki,
                html_content_baidu=html_baidu,
                page_name=page_name
            )
            
            spider.logger.info(f"   âœ… çš‡å¸ä¿¡æ¯æå–å®Œæˆ")
            spider.logger.info(f"   çš‡å¸: {emperor_info.get('çš‡å¸')}")
            spider.logger.info(f"   åº™å·: {emperor_info.get('åº™å·')}")
            spider.logger.info(f"   å¹´å·: {emperor_info.get('å¹´å·')}")
            spider.logger.info(f"   å‡ºç”Ÿ: {emperor_info.get('å‡ºç”Ÿ')}")
            spider.logger.info(f"   å»ä¸–: {emperor_info.get('å»ä¸–')}")
            
            # 2. æå–ç”Ÿå¹³äº‹è¿¹ï¼ˆåŒæºèåˆï¼‰
            spider.logger.info(f"\nğŸ“œ [Step 2] è°ƒç”¨å¤§æ¨¡å‹æå–ç”Ÿå¹³äº‹è¿¹...")
            events = self.extractor.extract_emperor_events(
                html_content_wiki=html_wiki,
                html_content_baidu=html_baidu,
                page_name=page_name
            )
            
            spider.logger.info(f"   âœ… ç”Ÿå¹³äº‹è¿¹æå–å®Œæˆ: {len(events)} æ¡")
            for idx, event in enumerate(events[:3], 1):
                spider.logger.info(f"      {idx}. {event.get('æ—¶é—´')} - {event.get('äº‹ä»¶', '')[:30]}...")
            if len(events) > 3:
                spider.logger.info(f"      ... è¿˜æœ‰ {len(events) - 3} æ¡äº‹è¿¹")
        
        # 3. æå–é“¾æ¥ï¼ˆç”¨äºé€’å½’çˆ¬å–ï¼‰
        spider.logger.info(f"\nğŸ”— [Step 3] æå–é“¾æ¥ä¿¡æ¯...")
        extracted_links = self._extract_links_from_events(events)
        
        event_links = [l for l in extracted_links if l['type'] == 'event']
        person_links = [l for l in extracted_links if l['type'] == 'person']
        
        spider.logger.info(f"   âœ… é“¾æ¥æå–å®Œæˆ")
        spider.logger.info(f"   äº‹ä»¶é“¾æ¥: {len(event_links)} ä¸ª")
        spider.logger.info(f"   äººç‰©é“¾æ¥: {len(person_links)} ä¸ª")
        
        # 4. åˆ›å»º ExtractedDataItem
        spider.logger.info(f"\nğŸ“¦ [Step 4] åˆ›å»º ExtractedDataItem")
        extracted_item = ExtractedDataItem(
            data_type='emperor',
            html_item=html_item,
            extracted_data={
                'emperor_info': emperor_info,
                'events': events
            },
            extracted_links=extracted_links,
            extraction_time=datetime.now().isoformat()
        )
        
        spider.logger.info(f"   âœ… ExtractedDataItem åˆ›å»ºå®Œæˆ")
        spider.logger.info(f"{'='*80}\n")
        
        return extracted_item
    
    def _extract_event(self, html_item: HtmlPageItem, spider) -> ExtractedDataItem:
        """æå–äº‹ä»¶ä¿¡æ¯ï¼ˆå¾…å®ç°ï¼‰"""
        spider.logger.info(f"ğŸ¤– æå–äº‹ä»¶ä¿¡æ¯: {html_item['page_name']}ï¼ˆåŠŸèƒ½å¾…å®ç°ï¼‰")
        
        # TODO: å®ç°äº‹ä»¶ä¿¡æ¯æå–
        event_info = {}
        
        extracted_item = ExtractedDataItem(
            data_type='event',
            html_item=html_item,
            extracted_data={'event_info': event_info},
            extracted_links=[],
            extraction_time=datetime.now().isoformat()
        )
        
        return extracted_item
    
    def _extract_person(self, html_item: HtmlPageItem, spider) -> ExtractedDataItem:
        """æå–äººç‰©ä¿¡æ¯ï¼ˆå¾…å®ç°ï¼‰"""
        spider.logger.info(f"ğŸ¤– æå–äººç‰©ä¿¡æ¯: {html_item['page_name']}ï¼ˆåŠŸèƒ½å¾…å®ç°ï¼‰")
        
        # TODO: å®ç°äººç‰©ä¿¡æ¯æå–
        person_info = {}
        
        extracted_item = ExtractedDataItem(
            data_type='person',
            html_item=html_item,
            extracted_data={'person_info': person_info},
            extracted_links=[],
            extraction_time=datetime.now().isoformat()
        )
        
        return extracted_item
    
    def _extract_links_from_events(self, events: List[Dict]) -> List[Dict]:
        """ä»äº‹è¿¹ä¸­æå–äººç‰©å’Œäº‹ä»¶é“¾æ¥ï¼ˆé€‚é…æ–°æ•°æ®æ ¼å¼ï¼‰"""
        links = []
        
        for event in events:
            # æå–äº‹ä»¶é“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼‰
            event_link = event.get('äº‹ä»¶é“¾æ¥')
            if event_link and event_link != 'null' and event_link:
                links.append({
                    'type': 'event',
                    'name': event.get('äº‹ä»¶', ''),
                    'url': event_link,
                    'source': self._detect_source_from_url(event_link)
                })
            
            # æå–äººç‰©é“¾æ¥ï¼ˆæ–°æ ¼å¼ï¼šå¯¹è±¡æ•°ç»„ï¼‰
            persons = event.get('äººç‰©', [])
            
            if persons and isinstance(persons, list):
                for person_obj in persons:
                    if isinstance(person_obj, dict):
                        person_link = person_obj.get('é“¾æ¥')
                        person_name = person_obj.get('å§“å', '')
                        
                        if person_link and person_link != 'null' and person_link:
                            links.append({
                                'type': 'person',
                                'name': person_name,
                                'url': person_link,
                                'source': self._detect_source_from_url(person_link)
                            })
        
        return links
    
    def _detect_source_from_url(self, url: str) -> str:
        """æ ¹æ®URLæ£€æµ‹æ•°æ®æº"""
        if 'wikipedia' in url:
            return 'wikipedia'
        elif 'baidu' in url:
            return 'baidu'
        else:
            return 'wikipedia'  # é»˜è®¤
