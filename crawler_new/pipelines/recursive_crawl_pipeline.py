"""
é€’å½’çˆ¬å– Pipeline
æ ¹æ®æå–çš„é“¾æ¥è‡ªåŠ¨è§¦å‘æ–°çš„çˆ¬å–ä»»åŠ¡
"""

import scrapy
from crawler_new.models.items import ExtractedDataItem


class RecursiveCrawlPipeline:
    """é€’å½’çˆ¬å–Pipeline"""
    
    def __init__(self, enable_recursive: bool, max_depth: int):
        self.enable_recursive = enable_recursive
        self.max_depth = max_depth
        self.crawled_urls = set()
    
    @classmethod
    def from_crawler(cls, crawler):
        enable_recursive = crawler.settings.get('ENABLE_RECURSIVE_CRAWL', True)
        max_depth = crawler.settings.get('MAX_RECURSIVE_DEPTH', 2)
        return cls(enable_recursive, max_depth)
    
    def open_spider(self, spider):
        """Spider å¼€å¯æ—¶åˆå§‹åŒ–"""
        if self.enable_recursive:
            spider.logger.info(f"ğŸ”„ é€’å½’çˆ¬å–å·²å¯ç”¨ï¼Œæœ€å¤§æ·±åº¦: {self.max_depth}")
        else:
            spider.logger.info("ğŸ”„ é€’å½’çˆ¬å–å·²ç¦ç”¨")
    
    def process_item(self, item, spider):
        """å¤„ç† Item"""
        # åªå¤„ç† ExtractedDataItem
        if not isinstance(item, ExtractedDataItem):
            return item
        
        # å¦‚æœæœªå¯ç”¨é€’å½’çˆ¬å–ï¼Œç›´æ¥è¿”å›
        if not self.enable_recursive:
            return item
        
        try:
            # è·å–å½“å‰æ·±åº¦
            current_depth = item['html_item']['metadata'].get('depth', 0)
            
            # æ£€æŸ¥æ·±åº¦é™åˆ¶
            if current_depth >= self.max_depth:
                spider.logger.info(f"âš ï¸  å·²è¾¾æœ€å¤§é€’å½’æ·±åº¦ {self.max_depth}ï¼Œåœæ­¢é€’å½’")
                return item
            
            # æå–é“¾æ¥
            extracted_links = item.get('extracted_links', [])
            
            if extracted_links:
                spider.logger.info(f"ğŸ”— å‘ç° {len(extracted_links)} ä¸ªé“¾æ¥ï¼Œå‡†å¤‡é€’å½’çˆ¬å–ï¼ˆæ·±åº¦: {current_depth + 1}ï¼‰")
                
                # ä¸ºæ¯ä¸ªé“¾æ¥ç”Ÿæˆæ–°çš„è¯·æ±‚
                for link in extracted_links:
                    self._create_recursive_request(link, spider, current_depth + 1)
            
        except Exception as e:
            spider.logger.error(f"âŒ é€’å½’çˆ¬å–å¤„ç†å¤±è´¥: {str(e)}")
            import traceback
            spider.logger.debug(traceback.format_exc())
        
        return item
    
    def _create_recursive_request(self, link: dict, spider, depth: int):
        """åˆ›å»ºé€’å½’è¯·æ±‚"""
        link_url = link.get('url')
        link_type = link.get('type')  # event æˆ– person
        link_name = link.get('name')
        
        # é˜²æ­¢é‡å¤çˆ¬å–
        if link_url in self.crawled_urls:
            spider.logger.debug(f"   âš ï¸  é“¾æ¥å·²çˆ¬å–ï¼Œè·³è¿‡: {link_name}")
            return
        
        # æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ
        if not link_url or link_url == 'null':
            return
        
        # æ ‡è®°å·²çˆ¬å–
        self.crawled_urls.add(link_url)
        
        spider.logger.info(f"   ğŸ“¥ æ·»åŠ é€’å½’è¯·æ±‚: {link_type} - {link_name}ï¼ˆæ·±åº¦: {depth}ï¼‰")
        
        # æ„å»ºè¯·æ±‚
        if link_type == 'event':
            callback = spider.parse_event
            meta = {
                'event_name': link_name,
                'data_source': link.get('source', 'wikipedia'),
                'depth': depth,
                'source_page': spider.name
            }
        elif link_type == 'person':
            callback = spider.parse_person
            meta = {
                'person_name': link_name,
                'data_source': link.get('source', 'wikipedia'),
                'depth': depth,
                'source_page': spider.name
            }
        else:
            spider.logger.warning(f"   âš ï¸  æœªçŸ¥é“¾æ¥ç±»å‹: {link_type}")
            return
        
        # åˆ›å»ºè¯·æ±‚å¹¶æäº¤åˆ°è°ƒåº¦å™¨
        request = scrapy.Request(
            url=link_url,
            callback=callback,
            meta=meta,
            dont_filter=False
        )
        
        # å°†è¯·æ±‚æ·»åŠ åˆ°çˆ¬è™«çš„è¯·æ±‚é˜Ÿåˆ—
        spider.crawler.engine.crawl(request, spider)
