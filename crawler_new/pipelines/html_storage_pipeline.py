"""
HTML å­˜å‚¨ Pipeline
å°†çˆ¬å–çš„åŸå§‹ HTML ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ
"""

import os
import json
from pathlib import Path
from datetime import datetime

from crawler_new.models.items import HtmlPageItem


class HtmlStoragePipeline:
    """HTMLå­˜å‚¨Pipeline"""
    
    def __init__(self, storage_path: str):
        self.storage_path = storage_path
        
    @classmethod
    def from_crawler(cls, crawler):
        storage_path = crawler.settings.get('HTML_STORAGE_PATH', 'crawler_new/data/html')
        return cls(storage_path)
    
    def open_spider(self, spider):
        """Spider å¼€å¯æ—¶åˆ›å»ºå­˜å‚¨ç›®å½•"""
        spider.logger.info(f"\n{'='*100}")
        spider.logger.info(f"ğŸ“ [Pipeline-1] HtmlStoragePipeline å¯åŠ¨")
        spider.logger.info(f"   å­˜å‚¨è·¯å¾„: {self.storage_path}")
        
        # åˆ›å»ºå­˜å‚¨ç›®å½•ç»“æ„
        for subdir in ['emperor', 'event', 'person']:
            dir_path = Path(self.storage_path) / subdir
            dir_path.mkdir(parents=True, exist_ok=True)
            spider.logger.info(f"   âœ… ç›®å½•å·²å°±ç»ª: {dir_path}")
        
        spider.logger.info(f"{'='*100}\n")
    
    def process_item(self, item, spider):
        """å¤„ç† Item"""
        # åªå¤„ç† HtmlPageItem
        if not isinstance(item, HtmlPageItem):
            return item
        
        spider.logger.info(f"\n{'='*80}")
        spider.logger.info(f"ğŸ’¾ [Pipeline-1] HTMLå­˜å‚¨å¼€å§‹")
        spider.logger.info(f"   page_id: {item['page_id']}")
        spider.logger.info(f"   page_name: {item['page_name']}")
        spider.logger.info(f"   data_source: {item['data_source']}")
        spider.logger.info(f"   HTMLå¤§å°: {len(item['html_content'])} å­—ç¬¦")
        
        try:
            # ä¿å­˜ HTML æ–‡ä»¶
            html_file = self._save_html(item, spider)
            spider.logger.info(f"   âœ… HTMLæ–‡ä»¶: {html_file}")
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata_file = self._save_metadata(item, spider)
            spider.logger.info(f"   âœ… å…ƒæ•°æ®æ–‡ä»¶: {metadata_file}")
            
            spider.logger.info(f"âœ… [Pipeline-1] HTMLå­˜å‚¨å®Œæˆ")
            spider.logger.info(f"{'='*80}\n")
            
        except Exception as e:
            spider.logger.error(f"\n{'='*80}")
            spider.logger.error(f"âŒ [Pipeline-1] HTMLå­˜å‚¨å¤±è´¥")
            spider.logger.error(f"   page_id: {item['page_id']}")
            spider.logger.error(f"   é”™è¯¯: {str(e)}")
            spider.logger.error(f"{'='*80}\n")
            import traceback
            spider.logger.debug(traceback.format_exc())
        
        return item
    
    def _save_html(self, item: HtmlPageItem, spider):
        """ä¿å­˜ HTML æ–‡ä»¶"""
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        page_type = item['page_type']
        page_id = item['page_id']
        
        file_path = Path(self.storage_path) / page_type / f"{page_id}.html"
        
        # å†™å…¥ HTML å†…å®¹
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(item['html_content'])
        
        return file_path
    
    def _save_metadata(self, item: HtmlPageItem, spider):
        """ä¿å­˜å…ƒæ•°æ® JSON æ–‡ä»¶"""
        page_type = item['page_type']
        page_id = item['page_id']
        
        file_path = Path(self.storage_path) / page_type / f"{page_id}_metadata.json"
        
        # æ„å»ºå…ƒæ•°æ®
        metadata = {
            'page_type': item['page_type'],
            'page_id': item['page_id'],
            'page_name': item['page_name'],
            'data_source': item['data_source'],
            'source_url': item['source_url'],
            'crawl_time': item['crawl_time'],
            'metadata': item['metadata']
        }
        
        # å†™å…¥ JSON æ–‡ä»¶
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        return file_path
