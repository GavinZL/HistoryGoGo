#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
çˆ¬è™«è¿è¡Œè„šæœ¬
æ”¯æŒæµ‹è¯•æ¨¡å¼å’Œå…¨é‡çˆ¬å–æ¨¡å¼
"""
import os
import sys
import argparse
from pathlib import Path
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))


def run_crawler(mode='test', spider_name='baidu_baike'):
    """
    è¿è¡Œçˆ¬è™«
    
    Args:
        mode: 'test' æˆ– 'full'ï¼Œæµ‹è¯•æ¨¡å¼åªçˆ¬å–å‰3ä½çš‡å¸
        spider_name: çˆ¬è™«åç§°ï¼Œ'baidu_baike' æˆ– 'wikipedia'
    """
    print("=" * 80)
    print(f"ğŸš€ å¯åŠ¨çˆ¬è™«ï¼š{spider_name}")
    print(f"   æ¨¡å¼ï¼š{'æµ‹è¯•æ¨¡å¼ï¼ˆå‰3ä½çš‡å¸ï¼‰' if mode == 'test' else 'å…¨é‡çˆ¬å–'}")
    print("=" * 80)
    
    # è·å–Scrapyé…ç½®
    settings = get_project_settings()
    
    # è¦†ç›–çˆ¬å–æ¨¡å¼é…ç½®
    settings.set('CRAWL_MODE', mode)
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_dir = project_root / 'crawler' / 'data' / 'logs'
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºæŠ¥å‘Šç›®å½•
    report_dir = project_root / 'crawler' / 'data' / 'reports'
    report_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—æ–‡ä»¶
    log_file = log_dir / f'{spider_name}_{mode}.log'
    settings.set('LOG_FILE', str(log_file))
    
    # åˆ›å»ºçˆ¬è™«è¿›ç¨‹
    process = CrawlerProcess(settings)
    
    # æ·»åŠ çˆ¬è™«
    process.crawl(spider_name)
    
    # å¼€å§‹çˆ¬å–
    process.start()
    
    print("\n" + "=" * 80)
    print("âœ… çˆ¬è™«è¿è¡Œå®Œæˆ")
    print(f"   æ—¥å¿—æ–‡ä»¶ï¼š{log_file}")
    print("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='HistoryGogo æ•°æ®çˆ¬å–å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹ï¼š
  # æµ‹è¯•æ¨¡å¼ï¼ˆåªçˆ¬å–å‰3ä½çš‡å¸ï¼‰
  python run_crawler.py --mode test --spider baidu_baike
  
  # å…¨é‡çˆ¬å–ç™¾åº¦ç™¾ç§‘
  python run_crawler.py --mode full --spider baidu_baike
  
  # å…¨é‡çˆ¬å–ç»´åŸºç™¾ç§‘
  python run_crawler.py --mode full --spider wikipedia
  
  # åŒæ—¶çˆ¬å–ä¸¤ä¸ªæºï¼ˆå…ˆç™¾åº¦åç»´åŸºï¼‰
  python run_crawler.py --mode full --spider all
        """
    )
    
    parser.add_argument(
        '--mode',
        choices=['test', 'full'],
        default='test',
        help='çˆ¬å–æ¨¡å¼ï¼štest=æµ‹è¯•æ¨¡å¼ï¼ˆå‰3ä½çš‡å¸ï¼‰ï¼Œfull=å…¨é‡çˆ¬å–'
    )
    
    parser.add_argument(
        '--spider',
        choices=['baidu_baike', 'wikipedia', 'all'],
        default='baidu_baike',
        help='é€‰æ‹©çˆ¬è™«ï¼šbaidu_baike, wikipedia, æˆ– allï¼ˆä¸¤ä¸ªéƒ½çˆ¬ï¼‰'
    )
    
    args = parser.parse_args()
    
    print("\n" + "ğŸš€ HistoryGogo æ•°æ®çˆ¬å–å·¥å…·".center(80, "="))
    print()
    
    # æ£€æŸ¥æ˜¯å¦å·²åˆå§‹åŒ–æ•°æ®åº“
    db_path = project_root / 'server' / 'database' / 'historygogo.db'
    if not db_path.exists():
        print("âš  è­¦å‘Šï¼šæ•°æ®åº“æœªåˆå§‹åŒ–")
        print("è¯·å…ˆè¿è¡Œï¼špython init_database.py")
        print()
        response = input("æ˜¯å¦ç»§ç»­ï¼Ÿï¼ˆy/nï¼‰ï¼š")
        if response.lower() != 'y':
            return 1
    
    # è¿è¡Œçˆ¬è™«
    if args.spider == 'all':
        # å…ˆçˆ¬ç™¾åº¦ç™¾ç§‘
        run_crawler(args.mode, 'baidu_baike')
        print("\nâ³ ç­‰å¾…5ç§’åå¼€å§‹çˆ¬å–ç»´åŸºç™¾ç§‘...\n")
        import time
        time.sleep(5)
        # å†çˆ¬ç»´åŸºç™¾ç§‘
        run_crawler(args.mode, 'wikipedia')
    else:
        run_crawler(args.mode, args.spider)
    
    print("\n" + "=" * 80)
    print("âœ… æ‰€æœ‰çˆ¬å–ä»»åŠ¡å®Œæˆï¼")
    print()
    print("ä¸‹ä¸€æ­¥ï¼š")
    print("  1. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶ï¼šcrawler/data/logs/")
    print("  2. æŸ¥çœ‹éªŒè¯æŠ¥å‘Šï¼šcrawler/data/reports/validation_report.json")
    print("  3. æŸ¥çœ‹æ•°æ®åº“ï¼šserver/database/historygogo.db")
    print("  4. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Šï¼špython generate_statistics.py")
    print("=" * 80)
    print()
    
    return 0


if __name__ == '__main__':
    sys.exit(main())
